<!DOCTYPE HTML>
<html lang="zh" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>MobileCLIP å®Œæ•´ä½¿ç”¨æŒ‡å— - Jason Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Jason Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/shihyu/jason_note" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="mobileclip-å®Œæ•´ä½¿ç”¨æŒ‡å—"><a class="header" href="#mobileclip-å®Œæ•´ä½¿ç”¨æŒ‡å—">MobileCLIP å®Œæ•´ä½¿ç”¨æŒ‡å—</a></h1>
<blockquote>
<p>Apple MobileCLIP æ¨¡å‹ä»‹ç´¹ + å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹</p>
</blockquote>
<hr />
<h2 id="-ç›®éŒ„"><a class="header" href="#-ç›®éŒ„">ğŸ“‘ ç›®éŒ„</a></h2>
<ul>
<li><a href="#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%B4%B9%E8%88%87%E9%81%B8%E6%93%87">æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡</a></li>
<li><a href="#%E5%AE%89%E8%A3%9D%E8%88%87%E7%92%B0%E5%A2%83%E8%A8%AD%E7%BD%AE">å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®</a></li>
<li><a href="#%E5%9F%BA%E7%A4%8E%E4%BD%BF%E7%94%A8%E7%AF%84%E4%BE%8B">åŸºç¤ä½¿ç”¨ç¯„ä¾‹</a></li>
<li><a href="#%E9%80%B2%E9%9A%8E%E6%87%89%E7%94%A8%E7%AF%84%E4%BE%8B">é€²éšæ‡‰ç”¨ç¯„ä¾‹</a></li>
<li><a href="#%E6%95%88%E8%83%BD%E5%84%AA%E5%8C%96%E6%8A%80%E5%B7%A7">æ•ˆèƒ½å„ªåŒ–æŠ€å·§</a></li>
<li><a href="#%E5%B8%B8%E8%A6%8B%E5%95%8F%E9%A1%8C%E8%99%95%E7%90%86">å¸¸è¦‹å•é¡Œè™•ç†</a></li>
</ul>
<hr />
<h2 id="æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡"><a class="header" href="#æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡">æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡</a></h2>
<h3 id="-äº”å€‹æ¨¡å‹å®Œæ•´å°æ¯”"><a class="header" href="#-äº”å€‹æ¨¡å‹å®Œæ•´å°æ¯”">ğŸ“Š äº”å€‹æ¨¡å‹å®Œæ•´å°æ¯”</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹</th><th>åƒæ•¸é‡</th><th>æ¨è«–é€Ÿåº¦</th><th>ImageNet æº–ç¢ºåº¦</th><th>38 å€‹æ•¸æ“šé›†å¹³å‡</th><th>æª”æ¡ˆå¤§å°</th><th>é©ç”¨å ´æ™¯</th></tr></thead><tbody>
<tr><td><strong>S0</strong></td><td>54M</td><td>3.1ms</td><td>67.8%</td><td>58.1%</td><td>~45 MB</td><td>æ¥µè‡´è¼•é‡ï¼Œä½éšæ‰‹æ©Ÿ</td></tr>
<tr><td><strong>S1</strong></td><td>63M</td><td>3.3ms</td><td>72.6%</td><td>61.3%</td><td>~55 MB</td><td>å¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºåº¦ â­</td></tr>
<tr><td><strong>S2</strong></td><td>82M</td><td>4.2ms</td><td>75.7%</td><td>63.7%</td><td>~70 MB</td><td>è¼ƒé«˜æº–ç¢ºåº¦éœ€æ±‚ â­</td></tr>
<tr><td><strong>B</strong></td><td>86M</td><td>5.4ms</td><td>76.8%</td><td>65.2%</td><td>~75 MB</td><td>é«˜æº–ç¢ºåº¦ï¼Œä¸­éšæ‰‹æ©Ÿ</td></tr>
<tr><td><strong>B (LT)</strong></td><td>86M</td><td>5.4ms</td><td>77.2%</td><td>65.8%</td><td>~75 MB</td><td>æœ€é«˜æº–ç¢ºåº¦</td></tr>
</tbody></table>
</div>
<blockquote>
<p>â­ æ¨è–¦ï¼šä»¥åœ–æ‰¾åœ–æ‡‰ç”¨å„ªå…ˆé¸æ“‡ <strong>S1</strong> æˆ– <strong>S2</strong></p>
</blockquote>
<hr />
<h3 id="-æ¨¡å‹è©³ç´°èªªæ˜"><a class="header" href="#-æ¨¡å‹è©³ç´°èªªæ˜">ğŸ” æ¨¡å‹è©³ç´°èªªæ˜</a></h3>
<h4 id="mobileclip_s0pt---æ¥µè‡´è¼•é‡ç‰ˆ"><a class="header" href="#mobileclip_s0pt---æ¥µè‡´è¼•é‡ç‰ˆ"><strong>mobileclip_s0.pt</strong> - æ¥µè‡´è¼•é‡ç‰ˆ</a></h4>
<pre><code>âœ“ ç‰¹é»ï¼šé«”ç©æœ€å°ã€é€Ÿåº¦æœ€å¿«
âœ“ åƒæ•¸ï¼š11.4M (åœ–åƒ) + 42.4M (æ–‡å­—) = 53.8M
âœ“ é€Ÿåº¦ï¼š1.5ms (åœ–åƒ) + 1.6ms (æ–‡å­—) = 3.1ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 67.8%

é©åˆï¼š
- å…¥é–€ç´šæ‰‹æ©Ÿã€IoT è£ç½®
- å³æ™‚è™•ç†éœ€æ±‚ï¼ˆç›¸æ©Ÿ Appï¼‰
- éœ€è¦æ¥µä½å»¶é²çš„æ‡‰ç”¨

æ¯”è¼ƒï¼š
èˆ‡ OpenAI ViT-B/16 æº–ç¢ºåº¦ç›¸ç•¶ï¼Œä½†å¿« 4.8 å€ã€å° 2.8 å€
</code></pre>
<h4 id="mobileclip_s1pt---è¼•é‡å¹³è¡¡ç‰ˆ-"><a class="header" href="#mobileclip_s1pt---è¼•é‡å¹³è¡¡ç‰ˆ-"><strong>mobileclip_s1.pt</strong> - è¼•é‡å¹³è¡¡ç‰ˆ â­</a></h4>
<pre><code>âœ“ ç‰¹é»ï¼šè¼•é‡èˆ‡æº–ç¢ºåº¦çš„æœ€ä½³å¹³è¡¡é»
âœ“ åƒæ•¸ï¼šç´„ 63M
âœ“ é€Ÿåº¦ï¼šç´„ 3.3ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 72.6%

é©åˆï¼š
- ä¸€èˆ¬æ‰‹æ©Ÿæ‡‰ç”¨
- å¤§å¤šæ•¸ä»¥åœ–æ‰¾åœ–å ´æ™¯
- å¹³è¡¡æ•ˆèƒ½èˆ‡å“è³ª

æ¨è–¦ç†ç”±ï¼š
- å¿«é€Ÿé©—è­‰ POC çš„æœ€ä½³é¸æ“‡
- Android ç§»æ¤æœ€å®¹æ˜“
- æº–ç¢ºåº¦å·²è¶³å¤ å¤§å¤šæ•¸å ´æ™¯
</code></pre>
<h4 id="mobileclip_s2pt---ä¸­ç­‰è¦æ¨¡ç‰ˆ-"><a class="header" href="#mobileclip_s2pt---ä¸­ç­‰è¦æ¨¡ç‰ˆ-"><strong>mobileclip_s2.pt</strong> - ä¸­ç­‰è¦æ¨¡ç‰ˆ â­</a></h4>
<pre><code>âœ“ ç‰¹é»ï¼šæ›´é«˜æº–ç¢ºåº¦ï¼Œä»ä¿æŒè¼•é‡
âœ“ åƒæ•¸ï¼šç´„ 82M
âœ“ é€Ÿåº¦ï¼šç´„ 4.2ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 75.7%

é©åˆï¼š
- ä¸­é«˜éšæ‰‹æ©Ÿ
- éœ€è¦è¼ƒé«˜è¾¨è­˜æº–ç¢ºåº¦çš„æ‡‰ç”¨
- é›»å•†ã€åœ–ç‰‡æœå°‹ç­‰å ´æ™¯

æ¯”è¼ƒï¼š
æ¯” SigLIP ViT-B/16 å¿« 2.3 å€ã€å° 2.1 å€ï¼Œä½†æº–ç¢ºåº¦æ›´é«˜
</code></pre>
<h4 id="mobileclip_bpt---æ¨™æº–å¤§æ¨¡å‹"><a class="header" href="#mobileclip_bpt---æ¨™æº–å¤§æ¨¡å‹"><strong>mobileclip_b.pt</strong> - æ¨™æº–å¤§æ¨¡å‹</a></h4>
<pre><code>âœ“ ç‰¹é»ï¼šé«˜æº–ç¢ºåº¦ç‰ˆæœ¬
âœ“ åƒæ•¸ï¼šç´„ 86M
âœ“ é€Ÿåº¦ï¼šç´„ 5.4ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 76.8%

é©åˆï¼š
- æ——è‰¦æ‰‹æ©Ÿã€å¹³æ¿
- å°ˆæ¥­æ‡‰ç”¨ï¼ˆè¨­è¨ˆã€å‰µæ„å·¥å…·ï¼‰
- å°æº–ç¢ºåº¦è¦æ±‚é«˜çš„å ´æ™¯
</code></pre>
<h4 id="mobileclip_bltpt---é•·è¨“ç·´ç‰ˆæœ¬"><a class="header" href="#mobileclip_bltpt---é•·è¨“ç·´ç‰ˆæœ¬"><strong>mobileclip_blt.pt</strong> - é•·è¨“ç·´ç‰ˆæœ¬</a></h4>
<pre><code>âœ“ ç‰¹é»ï¼šB ç‰ˆæœ¬çš„å¢å¼·è¨“ç·´ç‰ˆï¼Œæº–ç¢ºåº¦æœ€é«˜
âœ“ åƒæ•¸ï¼š86Mï¼ˆèˆ‡ B ç›¸åŒï¼‰
âœ“ é€Ÿåº¦ï¼š5.4msï¼ˆèˆ‡ B ç›¸åŒï¼‰
âœ“ æº–ç¢ºåº¦ï¼šImageNet 77.2%ï¼ˆæœ€é«˜ï¼‰
âœ“ è¨“ç·´ï¼šä½¿ç”¨æ›´é•·çš„è¨“ç·´æ™‚é–“ï¼ˆ600k iterationsï¼‰

é©åˆï¼š
- éœ€è¦æœ€ä½³æº–ç¢ºåº¦çš„æ‡‰ç”¨
- æœå‹™ç«¯éƒ¨ç½²ï¼ˆè¨˜æ†¶é«”ä¸å—é™ï¼‰
- å“è³ªå„ªå…ˆçš„å ´æ™¯

æ¯”è¼ƒï¼š
æº–ç¢ºåº¦è¶…è¶Š OpenAI ViT-L/14@336
</code></pre>
<hr />
<h3 id="-æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹"><a class="header" href="#-æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹">ğŸ¯ æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹</a></h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  éœ€è¦æœ€å¿«é€Ÿåº¦ï¼Ÿ              â”‚
â”‚  â””â”€ Yes â†’ S0                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ No
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  éœ€è¦æœ€é«˜æº–ç¢ºåº¦ï¼Ÿ            â”‚
â”‚  â””â”€ Yes â†’ B (LT)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ No
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä»‹æ–¼å…©è€…ä¹‹é–“ï¼Ÿ              â”‚
â”‚  â”œâ”€ åå‘é€Ÿåº¦ â†’ S1 â­        â”‚
â”‚  â”œâ”€ å¹³è¡¡ â†’ S2 â­            â”‚
â”‚  â””â”€ åå‘æº–ç¢ºåº¦ â†’ B          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3 id="-å¯¦éš›æ‡‰ç”¨å ´æ™¯æ¨è–¦"><a class="header" href="#-å¯¦éš›æ‡‰ç”¨å ´æ™¯æ¨è–¦">ğŸ“± å¯¦éš›æ‡‰ç”¨å ´æ™¯æ¨è–¦</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ‡‰ç”¨å ´æ™¯</th><th>æ¨è–¦æ¨¡å‹</th><th>ç†ç”±</th></tr></thead><tbody>
<tr><td>ç›¸æ©Ÿå³æ™‚è¾¨è­˜</td><td><strong>S0</strong></td><td>é€Ÿåº¦å„ªå…ˆï¼Œä½å»¶é²</td></tr>
<tr><td>æ‰‹æ©Ÿç›¸ç°¿æœå°‹</td><td><strong>S1</strong> æˆ– <strong>S2</strong></td><td>å¹³è¡¡é«”é©—</td></tr>
<tr><td>é›»å•†ä»¥åœ–æ‰¾åœ–</td><td><strong>S2</strong> æˆ– <strong>B</strong></td><td>æº–ç¢ºåº¦é‡è¦</td></tr>
<tr><td>å°ˆæ¥­åœ–ç‰‡ç®¡ç†</td><td><strong>B (LT)</strong></td><td>å“è³ªå„ªå…ˆ</td></tr>
<tr><td>IoT/é‚Šç·£è¨­å‚™</td><td><strong>S0</strong></td><td>è³‡æºå—é™</td></tr>
<tr><td>æœå‹™ç«¯ API</td><td><strong>B (LT)</strong></td><td>ç„¡è³‡æºé™åˆ¶</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®"><a class="header" href="#å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®">å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®</a></h2>
<h3 id="-å®‰è£ä¾è³´"><a class="header" href="#-å®‰è£ä¾è³´">ğŸ“¦ å®‰è£ä¾è³´</a></h3>
<pre><code class="language-bash"># å®‰è£å¿…è¦å¥—ä»¶
pip install torch torchvision pillow numpy tqdm matplotlib

# å®‰è£ MobileCLIP
pip install git+https://github.com/apple/ml-mobileclip.git
</code></pre>
<h3 id="-ä¸‹è¼‰é è¨“ç·´æ¨¡å‹"><a class="header" href="#-ä¸‹è¼‰é è¨“ç·´æ¨¡å‹">ğŸ“¥ ä¸‹è¼‰é è¨“ç·´æ¨¡å‹</a></h3>
<pre><code class="language-bash"># å»ºç«‹æ¨¡å‹è³‡æ–™å¤¾
mkdir -p checkpoints
cd checkpoints

# ä¸‹è¼‰æ¨¡å‹ï¼ˆé¸ä¸€å€‹æˆ–å¤šå€‹ï¼‰
# S0 - æœ€è¼•é‡ï¼ˆå»ºè­°å…ˆä¸‹è¼‰é€™å€‹æ¸¬è©¦ï¼‰
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt

# S1 - å¹³è¡¡ç‰ˆï¼ˆæ¨è–¦ï¼‰
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s1.pt

# S2 - ä¸­ç­‰è¦æ¨¡ï¼ˆæ¨è–¦ï¼‰
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s2.pt

# B - å¤§æ¨¡å‹
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_b.pt

# B (LT) - æœ€ä½³æº–ç¢ºåº¦
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_blt.pt
</code></pre>
<p>æˆ–ä½¿ç”¨å®˜æ–¹è…³æœ¬ï¼š</p>
<pre><code class="language-bash"># ä¸‹è¼‰æ‰€æœ‰æ¨¡å‹
source get_pretrained_models.sh
</code></pre>
<hr />
<h2 id="åŸºç¤ä½¿ç”¨ç¯„ä¾‹"><a class="header" href="#åŸºç¤ä½¿ç”¨ç¯„ä¾‹">åŸºç¤ä½¿ç”¨ç¯„ä¾‹</a></h2>
<h3 id="-ç¯„ä¾‹-1å–®å¼µåœ–ç‰‡ç‰¹å¾µæå–æœ€åŸºæœ¬"><a class="header" href="#-ç¯„ä¾‹-1å–®å¼µåœ–ç‰‡ç‰¹å¾µæå–æœ€åŸºæœ¬">ğŸ¯ ç¯„ä¾‹ 1ï¼šå–®å¼µåœ–ç‰‡ç‰¹å¾µæå–ï¼ˆæœ€åŸºæœ¬ï¼‰</a></h3>
<pre><code class="language-python">import torch
import mobileclip
from PIL import Image

# ========== 1. è¼‰å…¥æ¨¡å‹ ==========
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s1',  # æ¨¡å‹åç¨±ï¼šs0, s1, s2, b
    pretrained='checkpoints/mobileclip_s1.pt'  # æ¬Šé‡æª”æ¡ˆè·¯å¾‘
)

# è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
model.eval()

# é¸æ“‡è£ç½®ï¼ˆGPU æˆ– CPUï¼‰
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥ï¼Œä½¿ç”¨è£ç½®: {device}")


# ========== 2. å¾åœ–ç‰‡æª”æ¡ˆè½‰æ›æˆ tensor ==========

# è®€å–åœ–ç‰‡ï¼ˆæ”¯æ´ jpg, png ç­‰æ ¼å¼ï¼‰
image_path = "my_cat.jpg"
image = Image.open(image_path).convert('RGB')  # ç¢ºä¿æ˜¯ RGB æ ¼å¼

print(f"âœ“ åŸå§‹åœ–ç‰‡å¤§å°: {image.size}")

# ä½¿ç”¨ preprocess é€²è¡Œé è™•ç†ï¼ˆresize, normalize ç­‰ï¼‰
image_tensor = preprocess(image)  # è¼¸å‡º shape: (3, H, W)

print(f"âœ“ é è™•ç†å¾Œ tensor shape: {image_tensor.shape}")

# å¢åŠ  batch ç¶­åº¦ (1, 3, H, W)
image_tensor = image_tensor.unsqueeze(0)

print(f"âœ“ åŠ å…¥ batch ç¶­åº¦å¾Œ: {image_tensor.shape}")

# ç§»å‹•åˆ°å°æ‡‰è£ç½®
image_tensor = image_tensor.to(device)


# ========== 3. æå–ç‰¹å¾µ ==========

with torch.no_grad():  # ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦
    image_features = model.encode_image(image_tensor)
    
    # L2 æ­¸ä¸€åŒ–ï¼ˆé‡è¦ï¼ç”¨æ–¼è¨ˆç®—ç›¸ä¼¼åº¦ï¼‰
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)

print(f"âœ“ ç‰¹å¾µå‘é‡ shape: {image_features.shape}")  # (1, 512)
print(f"âœ“ ç‰¹å¾µå‘é‡ç¯„ä¾‹ï¼ˆå‰ 10 ç¶­ï¼‰: {image_features[0, :10]}")

# è½‰æ›æˆ numpyï¼ˆå¦‚æœéœ€è¦å„²å­˜æˆ–é€²ä¸€æ­¥è™•ç†ï¼‰
features_numpy = image_features.cpu().numpy()
print(f"âœ“ Numpy æ ¼å¼ shape: {features_numpy.shape}")
</code></pre>
<p><strong>è¼¸å‡ºç¯„ä¾‹ï¼š</strong></p>
<pre><code>âœ“ æ¨¡å‹å·²è¼‰å…¥ï¼Œä½¿ç”¨è£ç½®: cpu
âœ“ åŸå§‹åœ–ç‰‡å¤§å°: (1920, 1080)
âœ“ é è™•ç†å¾Œ tensor shape: torch.Size([3, 256, 256])
âœ“ åŠ å…¥ batch ç¶­åº¦å¾Œ: torch.Size([1, 3, 256, 256])
âœ“ ç‰¹å¾µå‘é‡ shape: torch.Size([1, 512])
âœ“ ç‰¹å¾µå‘é‡ç¯„ä¾‹ï¼ˆå‰ 10 ç¶­ï¼‰: tensor([ 0.0234, -0.1234,  0.0567, ...])
âœ“ Numpy æ ¼å¼ shape: (1, 512)
</code></pre>
<hr />
<h3 id="-é—œéµæ¦‚å¿µèªªæ˜"><a class="header" href="#-é—œéµæ¦‚å¿µèªªæ˜">ğŸ“ é—œéµæ¦‚å¿µèªªæ˜</a></h3>
<h4 id="image_tensor-çš„å®Œæ•´è½‰æ›æµç¨‹"><a class="header" href="#image_tensor-çš„å®Œæ•´è½‰æ›æµç¨‹"><strong>image_tensor çš„å®Œæ•´è½‰æ›æµç¨‹</strong></a></h4>
<pre><code class="language-python"># æ­¥é©Ÿ 1: è®€å–åœ–ç‰‡æª”æ¡ˆ
image = Image.open("cat.jpg").convert('RGB')
# â†’ PIL.Image ç‰©ä»¶ï¼Œä¾‹å¦‚ (1920, 1080, 3)

# æ­¥é©Ÿ 2: é è™•ç†ï¼ˆresize, normalizeï¼‰
image_tensor = preprocess(image)
# â†’ torch.Tensor, shape: (3, H, W)ï¼Œä¾‹å¦‚ (3, 256, 256)
# â†’ å€¼ç¯„åœå·²è¢«æ¨™æº–åŒ–ï¼ˆé€šå¸¸æ˜¯ [-1, 1] æˆ– [0, 1]ï¼‰

# æ­¥é©Ÿ 3: å¢åŠ  batch ç¶­åº¦
image_tensor = image_tensor.unsqueeze(0)
# â†’ shape: (1, 3, H, W)ï¼Œä¾‹å¦‚ (1, 3, 256, 256)

# æ­¥é©Ÿ 4: ç§»åˆ°å°æ‡‰è£ç½®
image_tensor = image_tensor.to(device)
# â†’ å¦‚æœæœ‰ GPU å°±ç§»åˆ° GPUï¼Œå¦å‰‡ç•™åœ¨ CPU

# æ­¥é©Ÿ 5: æå–ç‰¹å¾µ
image_features = model.encode_image(image_tensor)
# â†’ shape: (1, 512)ï¼Œå°±æ˜¯ä½ è¦çš„ç‰¹å¾µå‘é‡ï¼
</code></pre>
<h4 id="preprocess-åšäº†ä»€éº¼"><a class="header" href="#preprocess-åšäº†ä»€éº¼"><strong>preprocess åšäº†ä»€éº¼ï¼Ÿ</strong></a></h4>
<p><code>preprocess</code> æ˜¯ MobileCLIP æä¾›çš„é è™•ç†å‡½æ•¸ï¼Œç­‰åŒæ–¼ï¼š</p>
<pre><code class="language-python">from torchvision import transforms

preprocess = transforms.Compose([
    transforms.Resize(256),              # èª¿æ•´å¤§å°
    transforms.CenterCrop(256),          # ä¸­å¿ƒè£åˆ‡
    transforms.ToTensor(),               # è½‰æˆ Tensor
    transforms.Normalize(                # æ¨™æº–åŒ–
        mean=[0.485, 0.456, 0.406],      # ImageNet mean
        std=[0.229, 0.224, 0.225]        # ImageNet std
    )
])
</code></pre>
<hr />
<h2 id="é€²éšæ‡‰ç”¨ç¯„ä¾‹"><a class="header" href="#é€²éšæ‡‰ç”¨ç¯„ä¾‹">é€²éšæ‡‰ç”¨ç¯„ä¾‹</a></h2>
<h3 id="-ç¯„ä¾‹-2æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡æ›´å¿«"><a class="header" href="#-ç¯„ä¾‹-2æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡æ›´å¿«">ğŸš€ ç¯„ä¾‹ 2ï¼šæ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡ï¼ˆæ›´å¿«ï¼‰</a></h3>
<pre><code class="language-python">import torch
import mobileclip
from PIL import Image
from pathlib import Path
from tqdm import tqdm

# è¼‰å…¥æ¨¡å‹
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s1',
    pretrained='checkpoints/mobileclip_s1.pt'
)
model.eval()
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)


# ========== æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡ ==========

def extract_features_batch(image_paths, batch_size=32):
    """
    æ‰¹æ¬¡æå–å¤šå¼µåœ–ç‰‡çš„ç‰¹å¾µ
    
    Args:
        image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
    
    Returns:
        features: (N, 512) çš„ç‰¹å¾µçŸ©é™£
        valid_paths: æˆåŠŸè™•ç†çš„åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
    """
    all_features = []
    valid_paths = []
    
    # åˆ†æ‰¹è™•ç†
    for i in tqdm(range(0, len(image_paths), batch_size), desc="æå–ç‰¹å¾µ"):
        batch_paths = image_paths[i:i+batch_size]
        
        # è¼‰å…¥ä¸¦é è™•ç†é€™ä¸€æ‰¹åœ–ç‰‡
        batch_images = []
        batch_valid_paths = []
        
        for path in batch_paths:
            try:
                img = Image.open(path).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid_paths.append(path)
            except Exception as e:
                print(f"âš  ç„¡æ³•è®€å– {path}: {e}")
                continue
        
        if not batch_images:
            continue
        
        # å †ç–Šæˆ batch (B, 3, H, W)
        batch_tensor = torch.stack(batch_images).to(device)
        
        # æå–ç‰¹å¾µ
        with torch.no_grad():
            features = model.encode_image(batch_tensor)
            # L2 æ­¸ä¸€åŒ–
            features = features / features.norm(dim=-1, keepdim=True)
        
        all_features.append(features.cpu())
        valid_paths.extend(batch_valid_paths)
    
    # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
    if all_features:
        all_features = torch.cat(all_features, dim=0)
    else:
        all_features = torch.empty(0, 512)
    
    return all_features, valid_paths


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========

# æƒæåœ–ç‰‡è³‡æ–™å¤¾
image_folder = "./my_photos"
image_paths = []

for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp']:
    image_paths.extend(list(Path(image_folder).glob(ext)))

image_paths = [str(p) for p in image_paths]
print(f"æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")

# æ‰¹æ¬¡æå–ç‰¹å¾µ
features, valid_paths = extract_features_batch(image_paths, batch_size=32)

print(f"âœ“ ç‰¹å¾µçŸ©é™£ shape: {features.shape}")  # (N, 512)
print(f"âœ“ æˆåŠŸè™•ç† {len(valid_paths)} å¼µåœ–ç‰‡")

# å„²å­˜ç‰¹å¾µ
import numpy as np
np.savez('image_features.npz', 
         features=features.numpy(),
         paths=valid_paths)
print("âœ“ ç‰¹å¾µå·²å„²å­˜åˆ° image_features.npz")
</code></pre>
<hr />
<h3 id="-ç¯„ä¾‹-3ä»¥åœ–æ‰¾åœ–å®Œæ•´æµç¨‹"><a class="header" href="#-ç¯„ä¾‹-3ä»¥åœ–æ‰¾åœ–å®Œæ•´æµç¨‹">ğŸ” ç¯„ä¾‹ 3ï¼šä»¥åœ–æ‰¾åœ–ï¼ˆå®Œæ•´æµç¨‹ï¼‰</a></h3>
<h4 id="æ­¥é©Ÿ-1-å»ºç«‹åœ–ç‰‡ç´¢å¼•"><a class="header" href="#æ­¥é©Ÿ-1-å»ºç«‹åœ–ç‰‡ç´¢å¼•">æ­¥é©Ÿ 1: å»ºç«‹åœ–ç‰‡ç´¢å¼•</a></h4>
<pre><code class="language-python">import torch
import mobileclip
from PIL import Image
import numpy as np
from pathlib import Path
from tqdm import tqdm

def build_image_index(image_folder, model_name='mobileclip_s1', output_file='index.npz'):
    """
    å»ºç«‹åœ–ç‰‡ç´¢å¼•
    
    Args:
        image_folder: åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±
        output_file: ç´¢å¼•è¼¸å‡ºæª”æ¡ˆ
    
    Returns:
        features_matrix: (N, 512) ç‰¹å¾µçŸ©é™£
        image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
    """
    
    # è¼‰å…¥æ¨¡å‹
    print("ğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name,
        pretrained=f'checkpoints/{model_name}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")
    
    # æƒææ‰€æœ‰åœ–ç‰‡
    print("\nğŸ“‚ æƒæåœ–ç‰‡...")
    image_paths = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.bmp']:
        image_paths.extend(Path(image_folder).glob(ext))
    
    image_paths = [str(p) for p in image_paths]
    print(f"âœ“ æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")
    
    # æ‰¹æ¬¡æå–ç‰¹å¾µ
    print("\nğŸ¨ æå–ç‰¹å¾µ...")
    all_features = []
    valid_paths = []
    batch_size = 32
    
    for i in tqdm(range(0, len(image_paths), batch_size)):
        batch_paths = image_paths[i:i+batch_size]
        batch_images = []
        batch_valid = []
        
        for img_path in batch_paths:
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid.append(img_path)
            except Exception as e:
                print(f"âš  è·³é {img_path}: {e}")
        
        if not batch_images:
            continue
        
        # æ‰¹æ¬¡æ¨è«–
        batch_tensor = torch.stack(batch_images).to(device)
        
        with torch.no_grad():
            features = model.encode_image(batch_tensor)
            features = features / features.norm(dim=-1, keepdim=True)
        
        all_features.append(features.cpu().numpy())
        valid_paths.extend(batch_valid)
    
    # åˆä½µç‰¹å¾µ
    features_matrix = np.vstack(all_features)
    
    # å„²å­˜ç´¢å¼•
    print(f"\nğŸ’¾ å„²å­˜ç´¢å¼•...")
    np.savez(output_file, 
             features=features_matrix,
             paths=valid_paths,
             model_name=model_name)
    
    print(f"âœ“ ç´¢å¼•å·²å»ºç«‹: {output_file}")
    print(f"âœ“ ç‰¹å¾µçŸ©é™£ shape: {features_matrix.shape}")
    print(f"âœ“ åœ–ç‰‡æ•¸é‡: {len(valid_paths)}")
    
    return features_matrix, valid_paths


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========
if __name__ == '__main__':
    features, paths = build_image_index(
        image_folder='./my_photos',
        model_name='mobileclip_s1',
        output_file='photo_index.npz'
    )
</code></pre>
<h4 id="æ­¥é©Ÿ-2-æœå°‹ç›¸ä¼¼åœ–ç‰‡"><a class="header" href="#æ­¥é©Ÿ-2-æœå°‹ç›¸ä¼¼åœ–ç‰‡">æ­¥é©Ÿ 2: æœå°‹ç›¸ä¼¼åœ–ç‰‡</a></h4>
<pre><code class="language-python">import torch
import mobileclip
from PIL import Image
import numpy as np

def search_similar_images(query_image_path, 
                         index_file='index.npz', 
                         top_k=5,
                         model_name='mobileclip_s1'):
    """
    æœå°‹ç›¸ä¼¼åœ–ç‰‡
    
    Args:
        query_image_path: æŸ¥è©¢åœ–ç‰‡è·¯å¾‘
        index_file: ç´¢å¼•æª”æ¡ˆè·¯å¾‘
        top_k: è¿”å›çµæœæ•¸é‡
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±
    
    Returns:
        results: [(image_path, similarity_score), ...]
    """
    
    # è¼‰å…¥ç´¢å¼•
    print(f"ğŸ“‚ è¼‰å…¥ç´¢å¼•: {index_file}")
    data = np.load(index_file, allow_pickle=True)
    index_features = data['features']  # (N, 512)
    image_paths = data['paths'].tolist()
    
    print(f"âœ“ è¼‰å…¥ {len(image_paths)} å¼µåœ–ç‰‡çš„ç´¢å¼•")
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name,
        pretrained=f'checkpoints/{model_name}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # æå–æŸ¥è©¢åœ–ç‰‡ç‰¹å¾µ
    print(f"\nğŸ” æå–æŸ¥è©¢åœ–ç‰‡ç‰¹å¾µ...")
    query_img = Image.open(query_image_path).convert('RGB')
    query_tensor = preprocess(query_img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        query_feat = model.encode_image(query_tensor)
        query_feat = query_feat / query_feat.norm(dim=-1, keepdim=True)
    
    query_feat = query_feat.cpu().numpy()  # (1, 512)
    
    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆçŸ©é™£ä¹˜æ³•ï¼‰
    print(f"\nğŸ“Š è¨ˆç®—ç›¸ä¼¼åº¦...")
    similarities = np.dot(index_features, query_feat.T).squeeze()  # (N,)
    
    # å– Top-K
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    # é¡¯ç¤ºçµæœ
    print(f"\n{'='*60}")
    print(f"æŸ¥è©¢åœ–ç‰‡: {query_image_path}")
    print(f"{'='*60}")
    print(f"\nTop-{top_k} æœ€ç›¸ä¼¼åœ–ç‰‡:\n")
    
    results = []
    for i, idx in enumerate(top_indices):
        path = image_paths[idx]
        score = similarities[idx]
        print(f"{i+1}. {path}")
        print(f"   ç›¸ä¼¼åº¦: {score:.4f} ({score*100:.2f}%)\n")
        results.append((path, float(score)))
    
    return results


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========
if __name__ == '__main__':
    results = search_similar_images(
        query_image_path='./query_cat.jpg',
        index_file='photo_index.npz',
        top_k=5,
        model_name='mobileclip_s1'
    )
</code></pre>
<hr />
<h3 id="-ç¯„ä¾‹-4è¦–è¦ºåŒ–æœå°‹çµæœ"><a class="header" href="#-ç¯„ä¾‹-4è¦–è¦ºåŒ–æœå°‹çµæœ">ğŸ“Š ç¯„ä¾‹ 4ï¼šè¦–è¦ºåŒ–æœå°‹çµæœ</a></h3>
<pre><code class="language-python">import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

def visualize_search_results(query_path, results, top_k=5, save_path='search_results.png'):
    """
    è¦–è¦ºåŒ–æœå°‹çµæœ
    
    Args:
        query_path: æŸ¥è©¢åœ–ç‰‡è·¯å¾‘
        results: [(image_path, score), ...] æœå°‹çµæœ
        top_k: é¡¯ç¤ºæ•¸é‡
        save_path: å„²å­˜è·¯å¾‘
    """
    
    # è¨­å®šåœ–è¡¨
    fig, axes = plt.subplots(1, top_k+1, figsize=(3*(top_k+1), 3))
    
    # é¡¯ç¤ºæŸ¥è©¢åœ–ç‰‡
    query_img = Image.open(query_path)
    axes[0].imshow(query_img)
    axes[0].set_title('Query Image', fontsize=12, fontweight='bold', color='red')
    axes[0].axis('off')
    axes[0].set_facecolor('#ffe6e6')
    
    # é¡¯ç¤ºæœå°‹çµæœ
    for i, (img_path, score) in enumerate(results[:top_k]):
        try:
            img = Image.open(img_path)
            axes[i+1].imshow(img)
            axes[i+1].set_title(
                f'#{i+1}\nScore: {score:.3f}', 
                fontsize=10,
                color='green' if score &gt; 0.8 else 'blue'
            )
            axes[i+1].axis('off')
        except Exception as e:
            print(f"ç„¡æ³•è¼‰å…¥åœ–ç‰‡ {img_path}: {e}")
            axes[i+1].text(0.5, 0.5, 'Error', ha='center', va='center')
            axes[i+1].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"âœ“ æœå°‹çµæœå·²å„²å­˜åˆ° {save_path}")


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========
if __name__ == '__main__':
    # å…ˆæœå°‹
    results = search_similar_images(
        query_image_path='./query_cat.jpg',
        index_file='photo_index.npz',
        top_k=5
    )
    
    # è¦–è¦ºåŒ–
    visualize_search_results(
        query_path='./query_cat.jpg',
        results=results,
        top_k=5
    )
</code></pre>
<hr />
<h3 id="-ç¯„ä¾‹-5å®Œæ•´çš„-cli-å·¥å…·"><a class="header" href="#-ç¯„ä¾‹-5å®Œæ•´çš„-cli-å·¥å…·">ğŸ¯ ç¯„ä¾‹ 5ï¼šå®Œæ•´çš„ CLI å·¥å…·</a></h3>
<h4 id="build_indexpy---å»ºç«‹ç´¢å¼•"><a class="header" href="#build_indexpy---å»ºç«‹ç´¢å¼•">build_index.py - å»ºç«‹ç´¢å¼•</a></h4>
<pre><code class="language-python">#!/usr/bin/env python3
"""
å»ºç«‹åœ–ç‰‡ç´¢å¼•
ç”¨æ³•: python build_index.py --images ./photos --output index.npz --model mobileclip_s1
"""

import argparse
import torch
import mobileclip
from PIL import Image
import numpy as np
from pathlib import Path
from tqdm import tqdm

def main():
    parser = argparse.ArgumentParser(description='å»ºç«‹åœ–ç‰‡ç´¢å¼•')
    parser.add_argument('--images', required=True, help='åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘')
    parser.add_argument('--output', default='index.npz', help='è¼¸å‡ºç´¢å¼•æª”æ¡ˆ')
    parser.add_argument('--model', default='mobileclip_s1', 
                       choices=['mobileclip_s0', 'mobileclip_s1', 'mobileclip_s2', 
                               'mobileclip_b', 'mobileclip_blt'],
                       help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    args = parser.parse_args()
    
    print("="*60)
    print("MobileCLIP åœ–ç‰‡ç´¢å¼•å»ºç«‹å·¥å…·")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥æ¨¡å‹: {args.model}")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        args.model.replace('mobileclip_', ''),
        pretrained=f'checkpoints/{args.model}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    print(f"âœ“ ä½¿ç”¨è£ç½®: {device}")
    
    # æƒæåœ–ç‰‡
    print(f"\nğŸ“‚ æƒæåœ–ç‰‡è³‡æ–™å¤¾: {args.images}")
    image_paths = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.bmp']:
        image_paths.extend(Path(args.images).glob(ext))
    image_paths = [str(p) for p in image_paths]
    print(f"âœ“ æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")
    
    if len(image_paths) == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œè«‹æª¢æŸ¥è·¯å¾‘")
        return
    
    # æå–ç‰¹å¾µ
    print(f"\nğŸ¨ æå–ç‰¹å¾µï¼ˆbatch_size={args.batch_size}ï¼‰...")
    all_features = []
    valid_paths = []
    
    for i in tqdm(range(0, len(image_paths), args.batch_size)):
        batch_paths = image_paths[i:i+args.batch_size]
        batch_images = []
        batch_valid = []
        
        for img_path in batch_paths:
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid.append(img_path)
            except:
                continue
        
        if batch_images:
            batch_tensor = torch.stack(batch_images).to(device)
            with torch.no_grad():
                features = model.encode_image(batch_tensor)
                features = features / features.norm(dim=-1, keepdim=True)
            all_features.append(features.cpu().numpy())
            valid_paths.extend(batch_valid)
    
    # åˆä½µä¸¦å„²å­˜
    features_matrix = np.vstack(all_features)
    
    print(f"\nğŸ’¾ å„²å­˜ç´¢å¼•...")
    np.savez(args.output,
             features=features_matrix,
             paths=valid_paths,
             model_name=args.model)
    
    print(f"\n{'='*60}")
    print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"è¼¸å‡ºæª”æ¡ˆ: {args.output}")
    print(f"ç‰¹å¾µçŸ©é™£: {features_matrix.shape}")
    print(f"æˆåŠŸè™•ç†: {len(valid_paths)} å¼µåœ–ç‰‡")
    print(f"å¤±æ•—: {len(image_paths) - len(valid_paths)} å¼µåœ–ç‰‡")

if __name__ == '__main__':
    main()
</code></pre>
<h4 id="searchpy---æœå°‹ç›¸ä¼¼åœ–ç‰‡"><a class="header" href="#searchpy---æœå°‹ç›¸ä¼¼åœ–ç‰‡">search.py - æœå°‹ç›¸ä¼¼åœ–ç‰‡</a></h4>
<pre><code class="language-python">#!/usr/bin/env python3
"""
æœå°‹ç›¸ä¼¼åœ–ç‰‡
ç”¨æ³•: python search.py --query cat.jpg --index index.npz --top 5
"""

import argparse
import torch
import mobileclip
from PIL import Image
import numpy as np

def main():
    parser = argparse.ArgumentParser(description='æœå°‹ç›¸ä¼¼åœ–ç‰‡')
    parser.add_argument('--query', required=True, help='æŸ¥è©¢åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--index', required=True, help='ç´¢å¼•æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--top', type=int, default=5, help='è¿”å›çµæœæ•¸é‡')
    parser.add_argument('--visualize', action='store_true', help='è¦–è¦ºåŒ–çµæœ')
    args = parser.parse_args()
    
    print("="*60)
    print("MobileCLIP ä»¥åœ–æ‰¾åœ–å·¥å…·")
    print("="*60)
    
    # è¼‰å…¥ç´¢å¼•
    print(f"\nğŸ“‚ è¼‰å…¥ç´¢å¼•: {args.index}")
    data = np.load(args.index, allow_pickle=True)
    index_features = data['features']
    image_paths = data['paths'].tolist()
    model_name = str(data['model_name'])
    
    print(f"âœ“ è¼‰å…¥ {len(image_paths)} å¼µåœ–ç‰‡")
    print(f"âœ“ ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name.replace('mobileclip_', ''),
        pretrained=f'checkpoints/{model_name}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # æå–æŸ¥è©¢ç‰¹å¾µ
    print(f"\nğŸ” åˆ†ææŸ¥è©¢åœ–ç‰‡: {args.query}")
    query_img = Image.open(args.query).convert('RGB')
    query_tensor = preprocess(query_img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        query_feat = model.encode_image(query_tensor)
        query_feat = query_feat / query_feat.norm(dim=-1, keepdim=True)
    
    query_feat = query_feat.cpu().numpy()
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    print(f"\nğŸ“Š è¨ˆç®—ç›¸ä¼¼åº¦...")
    similarities = np.dot(index_features, query_feat.T).squeeze()
    top_indices = np.argsort(similarities)[::-1][:args.top]
    
    # é¡¯ç¤ºçµæœ
    print(f"\n{'='*60}")
    print(f"Top-{args.top} æœ€ç›¸ä¼¼åœ–ç‰‡:")
    print(f"{'='*60}\n")
    
    results = []
    for i, idx in enumerate(top_indices):
        path = image_paths[idx]
        score = similarities[idx]
        print(f"{i+1}. {path}")
        print(f"   ç›¸ä¼¼åº¦: {score:.4f} ({score*100:.2f}%)\n")
        results.append((path, float(score)))
    
    # è¦–è¦ºåŒ–ï¼ˆå¯é¸ï¼‰
    if args.visualize:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, args.top+1, figsize=(3*(args.top+1), 3))
        
        # æŸ¥è©¢åœ–ç‰‡
        axes[0].imshow(query_img)
        axes[0].set_title('Query', fontweight='bold')
        axes[0].axis('off')
        
        # çµæœ
        for i, (path, score) in enumerate(results):
            img = Image.open(path)
            axes[i+1].imshow(img)
            axes[i+1].set_title(f'#{i+1}: {score:.3f}')
            axes[i+1].axis('off')
        
        plt.tight_layout()
        plt.savefig('search_results.png', dpi=150, bbox_inches='tight')
        print(f"âœ“ è¦–è¦ºåŒ–çµæœå·²å„²å­˜: search_results.png")
        plt.show()

if __name__ == '__main__':
    main()
</code></pre>
<h4 id="ä½¿ç”¨ç¯„ä¾‹"><a class="header" href="#ä½¿ç”¨ç¯„ä¾‹">ä½¿ç”¨ç¯„ä¾‹</a></h4>
<pre><code class="language-bash"># å»ºç«‹ç´¢å¼•
python build_index.py --images ./my_photos --output photos.npz --model mobileclip_s1

# æœå°‹ç›¸ä¼¼åœ–ç‰‡
python search.py --query ./cat.jpg --index photos.npz --top 5

# æœå°‹ä¸¦è¦–è¦ºåŒ–
python search.py --query ./cat.jpg --index photos.npz --top 5 --visualize
</code></pre>
<hr />
<h2 id="æ•ˆèƒ½å„ªåŒ–æŠ€å·§"><a class="header" href="#æ•ˆèƒ½å„ªåŒ–æŠ€å·§">æ•ˆèƒ½å„ªåŒ–æŠ€å·§</a></h2>
<h3 id="-1-ä½¿ç”¨-gpu-åŠ é€Ÿ"><a class="header" href="#-1-ä½¿ç”¨-gpu-åŠ é€Ÿ">âš¡ 1. ä½¿ç”¨ GPU åŠ é€Ÿ</a></h3>
<pre><code class="language-python"># æª¢æŸ¥ä¸¦ä½¿ç”¨ GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

# ç¢ºèªæ˜¯å¦ä½¿ç”¨ GPU
print(f"ä½¿ç”¨è£ç½®: {device}")
print(f"GPU åç¨±: {torch.cuda.get_device_name(0)}" if torch.cuda.is_available() else "")
</code></pre>
<h3 id="-2-æ‰¹æ¬¡è™•ç†é€Ÿåº¦æå‡-5-10-å€"><a class="header" href="#-2-æ‰¹æ¬¡è™•ç†é€Ÿåº¦æå‡-5-10-å€">âš¡ 2. æ‰¹æ¬¡è™•ç†ï¼ˆé€Ÿåº¦æå‡ 5-10 å€ï¼‰</a></h3>
<pre><code class="language-python"># âŒ ä¸å¥½ï¼šä¸€å¼µå¼µè™•ç†
for img_path in image_paths:
    tensor = preprocess(Image.open(img_path)).unsqueeze(0)
    features = model.encode_image(tensor)

# âœ… å¥½ï¼šæ‰¹æ¬¡è™•ç†
batch_size = 32
for i in range(0, len(image_paths), batch_size):
    batch = [preprocess(Image.open(p)) for p in image_paths[i:i+batch_size]]
    batch_tensor = torch.stack(batch)
    batch_features = model.encode_image(batch_tensor)  # å¿«å¾ˆå¤šï¼
</code></pre>
<h3 id="-3-ä½¿ç”¨æ··åˆç²¾åº¦fp16"><a class="header" href="#-3-ä½¿ç”¨æ··åˆç²¾åº¦fp16">âš¡ 3. ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰</a></h3>
<pre><code class="language-python"># ä½¿ç”¨è‡ªå‹•æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
with torch.cuda.amp.autocast():
    image_features = model.encode_image(image_tensor)

# é€Ÿåº¦æå‡ç´„ 2 å€ï¼Œè¨˜æ†¶é«”æ¸›å°‘ç´„ 50%
</code></pre>
<h3 id="-4-ä¸è¨ˆç®—æ¢¯åº¦"><a class="header" href="#-4-ä¸è¨ˆç®—æ¢¯åº¦">âš¡ 4. ä¸è¨ˆç®—æ¢¯åº¦</a></h3>
<pre><code class="language-python"># æ¨è«–æ™‚å¿…é ˆä½¿ç”¨
with torch.no_grad():
    image_features = model.encode_image(image_tensor)

# ç¯€çœè¨˜æ†¶é«”å’Œè¨ˆç®—æ™‚é–“
</code></pre>
<h3 id="-5-é å…ˆè¨ˆç®—ä¸¦å¿«å–ç‰¹å¾µ"><a class="header" href="#-5-é å…ˆè¨ˆç®—ä¸¦å¿«å–ç‰¹å¾µ">âš¡ 5. é å…ˆè¨ˆç®—ä¸¦å¿«å–ç‰¹å¾µ</a></h3>
<pre><code class="language-python"># ä¸€æ¬¡æ€§å»ºç«‹ç´¢å¼•
features = extract_all_features(image_folder)
np.save('features_cache.npy', features)

# ä¹‹å¾Œç›´æ¥è¼‰å…¥
features = np.load('features_cache.npy')

# é¿å…é‡è¤‡æå–ç‰¹å¾µ
</code></pre>
<h3 id="-6-ä½¿ç”¨-dataloaderå¤§è¦æ¨¡è³‡æ–™"><a class="header" href="#-6-ä½¿ç”¨-dataloaderå¤§è¦æ¨¡è³‡æ–™">âš¡ 6. ä½¿ç”¨ DataLoaderï¼ˆå¤§è¦æ¨¡è³‡æ–™ï¼‰</a></h3>
<pre><code class="language-python">from torch.utils.data import Dataset, DataLoader

class ImageDataset(Dataset):
    def __init__(self, image_paths, transform):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert('RGB')
        return self.transform(img), self.image_paths[idx]

# ä½¿ç”¨ DataLoader
dataset = ImageDataset(image_paths, preprocess)
dataloader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)

for images, paths in dataloader:
    images = images.to(device)
    with torch.no_grad():
        features = model.encode_image(images)
</code></pre>
<hr />
<h2 id="å¸¸è¦‹å•é¡Œè™•ç†"><a class="header" href="#å¸¸è¦‹å•é¡Œè™•ç†">å¸¸è¦‹å•é¡Œè™•ç†</a></h2>
<h3 id="-éŒ¯èª¤-1-å¿˜è¨˜åŠ -batch-ç¶­åº¦"><a class="header" href="#-éŒ¯èª¤-1-å¿˜è¨˜åŠ -batch-ç¶­åº¦">ğŸ› éŒ¯èª¤ 1: å¿˜è¨˜åŠ  batch ç¶­åº¦</a></h3>
<pre><code class="language-python"># âŒ éŒ¯èª¤
image_tensor = preprocess(image)  # shape: (3, H, W)
features = model.encode_image(image_tensor)  # å ±éŒ¯ï¼

# âœ… æ­£ç¢º
image_tensor = preprocess(image).unsqueeze(0)  # shape: (1, 3, H, W)
features = model.encode_image(image_tensor)
</code></pre>
<h3 id="-éŒ¯èª¤-2-å¿˜è¨˜è½‰-rgb"><a class="header" href="#-éŒ¯èª¤-2-å¿˜è¨˜è½‰-rgb">ğŸ› éŒ¯èª¤ 2: å¿˜è¨˜è½‰ RGB</a></h3>
<pre><code class="language-python"># âŒ éŒ¯èª¤ï¼ˆPNG å¯èƒ½æ˜¯ RGBAï¼Œç°éšåœ–æ˜¯ Lï¼‰
image = Image.open('image.png')
features = model.encode_image(preprocess(image).unsqueeze(0))  # å¯èƒ½å ±éŒ¯

# âœ… æ­£ç¢º
image = Image.open('image.png').convert('RGB')  # å¼·åˆ¶è½‰æˆ RGB
features = model.encode_image(preprocess(image).unsqueeze(0))
</code></pre>
<h3 id="-éŒ¯èª¤-3-å¿˜è¨˜-l2-æ­¸ä¸€åŒ–"><a class="header" href="#-éŒ¯èª¤-3-å¿˜è¨˜-l2-æ­¸ä¸€åŒ–">ğŸ› éŒ¯èª¤ 3: å¿˜è¨˜ L2 æ­¸ä¸€åŒ–</a></h3>
<pre><code class="language-python"># âŒ éŒ¯èª¤ï¼ˆç›¸ä¼¼åº¦è¨ˆç®—ä¸æº–ç¢ºï¼‰
features = model.encode_image(image_tensor)
similarity = features @ features.T

# âœ… æ­£ç¢º
features = model.encode_image(image_tensor)
features = features / features.norm(dim=-1, keepdim=True)  # L2 æ­¸ä¸€åŒ–
similarity = features @ features.T  # æ­£ç¢ºçš„é¤˜å¼¦ç›¸ä¼¼åº¦
</code></pre>
<h3 id="-éŒ¯èª¤-4-è£ç½®ä¸åŒ¹é…"><a class="header" href="#-éŒ¯èª¤-4-è£ç½®ä¸åŒ¹é…">ğŸ› éŒ¯èª¤ 4: è£ç½®ä¸åŒ¹é…</a></h3>
<pre><code class="language-python"># âŒ éŒ¯èª¤
model.to('cuda')
image_tensor = preprocess(image).unsqueeze(0)  # åœ¨ CPU
features = model.encode_image(image_tensor)  # å ±éŒ¯ï¼štensor ä¸åœ¨åŒä¸€è£ç½®

# âœ… æ­£ç¢º
model.to(device)
image_tensor = preprocess(image).unsqueeze(0).to(device)  # ç§»åˆ°åŒä¸€è£ç½®
features = model.encode_image(image_tensor)
</code></pre>
<h3 id="-éŒ¯èª¤-5-è¨˜æ†¶é«”ä¸è¶³oom"><a class="header" href="#-éŒ¯èª¤-5-è¨˜æ†¶é«”ä¸è¶³oom">ğŸ› éŒ¯èª¤ 5: è¨˜æ†¶é«”ä¸è¶³ï¼ˆOOMï¼‰</a></h3>
<pre><code class="language-python"># è§£æ±ºæ–¹æ³• 1: æ¸›å°‘ batch size
batch_size = 16  # åŸæœ¬ 32ï¼Œæ”¹æˆ 16

# è§£æ±ºæ–¹æ³• 2: æ¸…ç† GPU è¨˜æ†¶é«”
torch.cuda.empty_cache()

# è§£æ±ºæ–¹æ³• 3: ä½¿ç”¨æ¢¯åº¦ç´¯ç©
with torch.no_grad():  # ä¸è¨ˆç®—æ¢¯åº¦
    features = model.encode_image(image_tensor)

# è§£æ±ºæ–¹æ³• 4: ä½¿ç”¨ CPU
device = 'cpu'  # æ”¹ç”¨ CPUï¼ˆè¼ƒæ…¢ä½†ä¸æœƒ OOMï¼‰
</code></pre>
<h3 id="-å•é¡Œ-6-åœ–ç‰‡è®€å–å¤±æ•—"><a class="header" href="#-å•é¡Œ-6-åœ–ç‰‡è®€å–å¤±æ•—">ğŸ› å•é¡Œ 6: åœ–ç‰‡è®€å–å¤±æ•—</a></h3>
<pre><code class="language-python"># å¥å£¯çš„åœ–ç‰‡è®€å–
def load_image_safely(image_path):
    try:
        img = Image.open(image_path).convert('RGB')
        return img
    except Exception as e:
        print(f"âš  ç„¡æ³•è®€å– {image_path}: {e}")
        return None

# ä½¿ç”¨
img = load_image_safely('image.jpg')
if img is not None:
    features = extract_features(img)
</code></pre>
<h3 id="-é™¤éŒ¯æŠ€å·§"><a class="header" href="#-é™¤éŒ¯æŠ€å·§">ğŸ”§ é™¤éŒ¯æŠ€å·§</a></h3>
<pre><code class="language-python"># 1. æª¢æŸ¥ tensor shape
print(f"Image tensor shape: {image_tensor.shape}")  # æ‡‰è©²æ˜¯ (1, 3, H, W)
print(f"Features shape: {features.shape}")  # æ‡‰è©²æ˜¯ (1, 512)

# 2. æª¢æŸ¥ç‰¹å¾µå‘é‡æ˜¯å¦æ­¸ä¸€åŒ–
print(f"Feature norm: {torch.norm(features)}")  # æ‡‰è©²æ¥è¿‘ 1.0

# 3. æª¢æŸ¥è£ç½®
print(f"Model device: {next(model.parameters()).device}")
print(f"Tensor device: {image_tensor.device}")

# 4. è¦–è¦ºåŒ–ç›¸ä¼¼åº¦çŸ©é™£
import matplotlib.pyplot as plt
similarity_matrix = features @ features.T
plt.imshow(similarity_matrix.cpu().numpy())
plt.colorbar()
plt.title('Similarity Matrix')
plt.show()
</code></pre>
<hr />
<h2 id="-å®Œæ•´æ¸¬è©¦è…³æœ¬"><a class="header" href="#-å®Œæ•´æ¸¬è©¦è…³æœ¬">ğŸ“ å®Œæ•´æ¸¬è©¦è…³æœ¬</a></h2>
<p>å°‡ä»¥ä¸‹ç¨‹å¼ç¢¼å„²å­˜ç‚º <code>test_mobileclip.py</code>ï¼š</p>
<pre><code class="language-python">#!/usr/bin/env python3
"""
MobileCLIP å®Œæ•´æ¸¬è©¦è…³æœ¬
"""

import torch
import mobileclip
from PIL import Image
import numpy as np

def test_single_image():
    """æ¸¬è©¦å–®å¼µåœ–ç‰‡ç‰¹å¾µæå–"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 1: å–®å¼µåœ–ç‰‡ç‰¹å¾µæå–")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        'mobileclip_s1',
        pretrained='checkpoints/mobileclip_s1.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")
    
    # æ¸¬è©¦åœ–ç‰‡
    image_path = "test.jpg"  # æ›¿æ›æˆä½ çš„åœ–ç‰‡
    image = Image.open(image_path).convert('RGB')
    
    print(f"âœ“ åœ–ç‰‡å¤§å°: {image.size}")
    
    # æå–ç‰¹å¾µ
    image_tensor = preprocess(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        features = model.encode_image(image_tensor)
        features = features / features.norm(dim=-1, keepdim=True)
    
    print(f"âœ“ ç‰¹å¾µ shape: {features.shape}")
    print(f"âœ“ ç‰¹å¾µ norm: {torch.norm(features).item():.4f}")
    print(f"âœ“ ç‰¹å¾µå‰ 5 ç¶­: {features[0, :5]}")


def test_similarity():
    """æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 2: ç›¸ä¼¼åº¦è¨ˆç®—")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        'mobileclip_s1',
        pretrained='checkpoints/mobileclip_s1.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # å…©å¼µæ¸¬è©¦åœ–ç‰‡
    image1 = Image.open("test1.jpg").convert('RGB')
    image2 = Image.open("test2.jpg").convert('RGB')
    
    # æå–ç‰¹å¾µ
    tensor1 = preprocess(image1).unsqueeze(0).to(device)
    tensor2 = preprocess(image2).unsqueeze(0).to(device)
    
    with torch.no_grad():
        feat1 = model.encode_image(tensor1)
        feat2 = model.encode_image(tensor2)
        
        feat1 = feat1 / feat1.norm(dim=-1, keepdim=True)
        feat2 = feat2 / feat2.norm(dim=-1, keepdim=True)
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    similarity = (feat1 @ feat2.T).item()
    
    print(f"âœ“ åœ–ç‰‡ 1 ç‰¹å¾µ: {feat1.shape}")
    print(f"âœ“ åœ–ç‰‡ 2 ç‰¹å¾µ: {feat2.shape}")
    print(f"âœ“ é¤˜å¼¦ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.2f}%)")


def test_batch_processing():
    """æ¸¬è©¦æ‰¹æ¬¡è™•ç†"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 3: æ‰¹æ¬¡è™•ç†")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        'mobileclip_s1',
        pretrained='checkpoints/mobileclip_s1.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # æ‰¹æ¬¡åœ–ç‰‡
    image_paths = ["test1.jpg", "test2.jpg", "test3.jpg"]
    images = [Image.open(p).convert('RGB') for p in image_paths]
    
    # æ‰¹æ¬¡è™•ç†
    batch_tensor = torch.stack([preprocess(img) for img in images]).to(device)
    
    print(f"âœ“ Batch shape: {batch_tensor.shape}")
    
    with torch.no_grad():
        batch_features = model.encode_image(batch_tensor)
        batch_features = batch_features / batch_features.norm(dim=-1, keepdim=True)
    
    print(f"âœ“ Batch features shape: {batch_features.shape}")
    print(f"âœ“ æ¯å¼µåœ–ç‰‡ç‰¹å¾µ norm: {torch.norm(batch_features, dim=-1)}")


def main():
    print("\n" + "ğŸš€"*30)
    print("MobileCLIP å®Œæ•´æ¸¬è©¦")
    print("ğŸš€"*30)
    
    try:
        test_single_image()
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ 1 å¤±æ•—: {e}")
    
    try:
        test_similarity()
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ 2 å¤±æ•—: {e}")
    
    try:
        test_batch_processing()
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ 3 å¤±æ•—: {e}")
    
    print("\n" + "âœ…"*30)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("âœ…"*30 + "\n")


if __name__ == '__main__':
    main()
</code></pre>
<p>åŸ·è¡Œæ¸¬è©¦ï¼š</p>
<pre><code class="language-bash">python test_mobileclip.py
</code></pre>
<hr />
<h2 id="-å­¸ç¿’è·¯å¾‘å»ºè­°"><a class="header" href="#-å­¸ç¿’è·¯å¾‘å»ºè­°">ğŸ“ å­¸ç¿’è·¯å¾‘å»ºè­°</a></h2>
<h3 id="éšæ®µ-1-åŸºç¤1-2-å¤©"><a class="header" href="#éšæ®µ-1-åŸºç¤1-2-å¤©">éšæ®µ 1: åŸºç¤ï¼ˆ1-2 å¤©ï¼‰</a></h3>
<ol>
<li>âœ… å®‰è£ç’°å¢ƒå’Œä¸‹è¼‰æ¨¡å‹</li>
<li>âœ… è·‘é€šç¯„ä¾‹ 1ï¼ˆå–®å¼µåœ–ç‰‡ï¼‰</li>
<li>âœ… ç†è§£ <code>preprocess</code> å’Œ <code>unsqueeze</code> çš„ä½œç”¨</li>
<li>âœ… æ¸¬è©¦ä¸åŒæ¨¡å‹ï¼ˆS0, S1, S2ï¼‰</li>
</ol>
<h3 id="éšæ®µ-2-å¯¦æˆ°3-5-å¤©"><a class="header" href="#éšæ®µ-2-å¯¦æˆ°3-5-å¤©">éšæ®µ 2: å¯¦æˆ°ï¼ˆ3-5 å¤©ï¼‰</a></h3>
<ol>
<li>âœ… å¯¦ä½œç¯„ä¾‹ 2ï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰</li>
<li>âœ… å¯¦ä½œç¯„ä¾‹ 3ï¼ˆä»¥åœ–æ‰¾åœ–ï¼‰</li>
<li>âœ… å»ºç«‹è‡ªå·±çš„åœ–ç‰‡ç´¢å¼•</li>
<li>âœ… æ¸¬è©¦æœå°‹åŠŸèƒ½</li>
</ol>
<h3 id="éšæ®µ-3-å„ªåŒ–2-3-å¤©"><a class="header" href="#éšæ®µ-3-å„ªåŒ–2-3-å¤©">éšæ®µ 3: å„ªåŒ–ï¼ˆ2-3 å¤©ï¼‰</a></h3>
<ol>
<li>âœ… å¯¦ä½œ CLI å·¥å…·</li>
<li>âœ… æ•ˆèƒ½å„ªåŒ–ï¼ˆGPUã€æ‰¹æ¬¡ï¼‰</li>
<li>âœ… è¦–è¦ºåŒ–æœå°‹çµæœ</li>
<li>âœ… éŒ¯èª¤è™•ç†å’Œå¥å£¯æ€§</li>
</ol>
<h3 id="éšæ®µ-4-android-æº–å‚™3-5-å¤©"><a class="header" href="#éšæ®µ-4-android-æº–å‚™3-5-å¤©">éšæ®µ 4: Android æº–å‚™ï¼ˆ3-5 å¤©ï¼‰</a></h3>
<ol>
<li>âœ… æ¨¡å‹è½‰æ›ï¼ˆTorchScriptï¼‰</li>
<li>âœ… é‡åŒ–æ¸¬è©¦ï¼ˆINT8ï¼‰</li>
<li>âœ… CPU æ•ˆèƒ½æ¸¬è©¦</li>
<li>âœ… æ’°å¯«ç§»æ¤æ–‡ä»¶</li>
</ol>
<hr />
<h2 id="-åƒè€ƒè³‡æº"><a class="header" href="#-åƒè€ƒè³‡æº">ğŸ“š åƒè€ƒè³‡æº</a></h2>
<ul>
<li><strong>å®˜æ–¹ GitHub</strong>: https://github.com/apple/ml-mobileclip</li>
<li><strong>è«–æ–‡</strong>: <a href="https://arxiv.org/pdf/2311.17049.pdf">MobileCLIP: Fast Image-Text Models</a></li>
<li><strong>HuggingFace æ¨¡å‹</strong>: <a href="https://huggingface.co/collections/apple/mobileclip-models-datacompdr-data-665789776e1aa2b59f35f7c8">MobileCLIP Collection</a></li>
<li><strong>PyTorch å®˜æ–¹æ–‡æª”</strong>: https://pytorch.org/docs/stable/index.html</li>
</ul>
<hr />
<h2 id="-å¿«é€Ÿåƒè€ƒ"><a class="header" href="#-å¿«é€Ÿåƒè€ƒ">ğŸ’¡ å¿«é€Ÿåƒè€ƒ</a></h2>
<h3 id="æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µ"><a class="header" href="#æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µ">æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µ</a></h3>
<pre><code class="language-python"># è¼‰å…¥æ¨¡å‹
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s1', pretrained='checkpoints/mobileclip_s1.pt'
)
model.eval()
model.to('cuda' if torch.cuda.is_available() else 'cpu')

# å–®å¼µåœ–ç‰‡
image = Image.open('cat.jpg').convert('RGB')
tensor = preprocess(image).unsqueeze(0).to(device)
with torch.no_grad():
    features = model.encode_image(tensor)
    features = features / features.norm(dim=-1, keepdim=True)

# ç›¸ä¼¼åº¦è¨ˆç®—
similarity = (features1 @ features2.T).item()
</code></pre>
<hr />
<p><strong>ç¥æ‚¨ä½¿ç”¨é †åˆ©ï¼æœ‰å•é¡Œéš¨æ™‚æŸ¥é–±æœ¬æŒ‡å— ğŸš€</strong></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../cv/MobileCLIP_Android_éƒ¨ç½²æŒ‡å—.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../strategy/bollmaker.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../cv/MobileCLIP_Android_éƒ¨ç½²æŒ‡å—.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../strategy/bollmaker.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
