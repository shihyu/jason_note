<!DOCTYPE HTML>
<html lang="zh" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>MobileCLIP å®Œæ•´å­¸ç¿’æŒ‡å— - Jason Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Jason Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/shihyu/jason_note" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="mobileclip-å®Œæ•´å­¸ç¿’æŒ‡å—"><a class="header" href="#mobileclip-å®Œæ•´å­¸ç¿’æŒ‡å—">MobileCLIP å®Œæ•´å­¸ç¿’æŒ‡å—</a></h1>
<blockquote>
<p><strong>æ–‡ä»¶æ›´æ–°æ—¥æœŸ</strong>ï¼š2025-10-27<br />
<strong>é©ç”¨å°è±¡</strong>ï¼šæƒ³ä½¿ç”¨ MobileCLIP é–‹ç™¼å•†å“æœå°‹æ‡‰ç”¨çš„é–‹ç™¼è€…</p>
</blockquote>
<hr />
<h2 id="ç›®éŒ„"><a class="header" href="#ç›®éŒ„">ç›®éŒ„</a></h2>
<ol>
<li><a href="#1-mobileclip-%E7%B0%A1%E4%BB%8B">MobileCLIP ç°¡ä»‹</a></li>
<li><a href="#2-%E7%82%BA%E4%BB%80%E9%BA%BC-python-%E8%A8%93%E7%B7%B4%E8%83%BD%E5%9C%A8-ios-%E5%9F%B7%E8%A1%8C">ç‚ºä»€éº¼ Python è¨“ç·´èƒ½åœ¨ iOS åŸ·è¡Œ</a></li>
<li><a href="#3-%E5%AF%A6%E9%9A%9B%E6%87%89%E7%94%A8%E5%95%86%E5%93%81%E6%8B%8D%E7%85%A7%E6%90%9C%E5%B0%8B">å¯¦éš›æ‡‰ç”¨ï¼šå•†å“æ‹ç…§æœå°‹</a></li>
<li><a href="#4-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BC%89%E8%88%87%E5%AE%89%E8%A3%9D">æ¨¡å‹ä¸‹è¼‰èˆ‡å®‰è£</a></li>
<li><a href="#5-mobileclip-%E5%8F%83%E6%95%B8%E8%A9%B3%E8%A7%A3">MobileCLIP åƒæ•¸è©³è§£</a></li>
<li><a href="#6-%E5%B8%B8%E7%94%A8%E6%87%89%E7%94%A8%E5%A0%B4%E6%99%AF">å¸¸ç”¨æ‡‰ç”¨å ´æ™¯</a></li>
<li><a href="#7-%E5%AE%8C%E6%95%B4%E7%A8%8B%E5%BC%8F%E7%A2%BC%E7%AF%84%E4%BE%8B">å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹</a></li>
<li><a href="#8-%E6%95%88%E8%83%BD%E5%84%AA%E5%8C%96%E5%BB%BA%E8%AD%B0">æ•ˆèƒ½å„ªåŒ–å»ºè­°</a></li>
</ol>
<hr />
<h2 id="1-mobileclip-ç°¡ä»‹"><a class="header" href="#1-mobileclip-ç°¡ä»‹">1. MobileCLIP ç°¡ä»‹</a></h2>
<h3 id="11-ä»€éº¼æ˜¯-mobileclip"><a class="header" href="#11-ä»€éº¼æ˜¯-mobileclip">1.1 ä»€éº¼æ˜¯ MobileCLIPï¼Ÿ</a></h3>
<p>MobileCLIP æ˜¯ Apple é–‹ç™¼çš„<strong>è¼•é‡ç´šåœ–åƒ-æ–‡å­—é…å°æ¨¡å‹</strong>ï¼Œå°ˆé–€ç‚ºè¡Œå‹•è£ç½®å„ªåŒ–ã€‚å®ƒå¯ä»¥ï¼š</p>
<ul>
<li><strong>é€£æ¥åœ–åƒèˆ‡æ–‡å­—</strong>ï¼šå°‡åœ–ç‰‡å’Œæ–‡å­—æè¿°æ˜ å°„åˆ°åŒä¸€å€‹ 512 ç¶­çš„å‘é‡ç©ºé–“</li>
<li><strong>é›¶æ¨£æœ¬åˆ†é¡</strong>ï¼šç„¡éœ€è¨“ç·´å°±èƒ½è­˜åˆ¥æœªè¦‹éçš„ç‰©é«”</li>
<li><strong>é«˜æ•ˆé‹è¡Œ</strong>ï¼šåœ¨æ‰‹æ©Ÿä¸Šå»¶é²åªæœ‰ 1.5-15ms</li>
</ul>
<h3 id="12-æ ¸å¿ƒç‰¹æ€§"><a class="header" href="#12-æ ¸å¿ƒç‰¹æ€§">1.2 æ ¸å¿ƒç‰¹æ€§</a></h3>
<div class="table-wrapper"><table><thead><tr><th>ç‰¹æ€§</th><th>èªªæ˜</th></tr></thead><tbody>
<tr><td><strong>é€Ÿåº¦</strong></td><td>æ¯” OpenAI CLIP å¿« 2.3-4.8 å€</td></tr>
<tr><td><strong>é«”ç©</strong></td><td>åƒæ•¸é‡ 50-150Mï¼ˆæ¯”æ¨™æº– CLIP å° 2-3 å€ï¼‰</td></tr>
<tr><td><strong>ç²¾æº–åº¦</strong></td><td>ImageNet é›¶æ¨£æœ¬æº–ç¢ºç‡é” 77.2%</td></tr>
<tr><td><strong>å¹³è‡º</strong></td><td>æ”¯æ´ iOSã€Androidã€Python</td></tr>
</tbody></table>
</div>
<h3 id="13-å¯¦éš›æ‡‰ç”¨å ´æ™¯"><a class="header" href="#13-å¯¦éš›æ‡‰ç”¨å ´æ™¯">1.3 å¯¦éš›æ‡‰ç”¨å ´æ™¯</a></h3>
<pre><code>ç”¨é€”ç¯„ä¾‹ï¼š
âœ“ å•†å“æ‹ç…§æœå°‹ï¼ˆæœ¬æ–‡é‡é»ï¼‰
âœ“ ç›¸ç°¿æ™ºæ…§æœå°‹
âœ“ AR è³¼ç‰©åŠ©æ‰‹
âœ“ å³æ™‚ç‰©é«”è­˜åˆ¥
âœ“ åœ–ç‰‡å»é‡
</code></pre>
<hr />
<h2 id="2-ç‚ºä»€éº¼-python-è¨“ç·´èƒ½åœ¨-ios-åŸ·è¡Œ"><a class="header" href="#2-ç‚ºä»€éº¼-python-è¨“ç·´èƒ½åœ¨-ios-åŸ·è¡Œ">2. ç‚ºä»€éº¼ Python è¨“ç·´èƒ½åœ¨ iOS åŸ·è¡Œ</a></h2>
<h3 id="21-æŠ€è¡“æ¶æ§‹"><a class="header" href="#21-æŠ€è¡“æ¶æ§‹">2.1 æŠ€è¡“æ¶æ§‹</a></h3>
<pre><code>ã€é–‹ç™¼éšæ®µã€‘               ã€éƒ¨ç½²éšæ®µã€‘
Python (PyTorch)    â†’     iOS (Core ML)
    â†“                         â†“
è¨“ç·´æ¨¡å‹               â†’     æ¨è«–åŸ·è¡Œ
(éœ€è¦GPUé›†ç¾¤)             (æ‰‹æ©ŸCPU/GPU)
    â†“                         â†“
.pt æª”æ¡ˆ              â†’     .mlmodelc æª”æ¡ˆ
(PyTorchæ ¼å¼)             (Core MLæ ¼å¼)
</code></pre>
<h3 id="22-è½‰æ›æµç¨‹"><a class="header" href="#22-è½‰æ›æµç¨‹">2.2 è½‰æ›æµç¨‹</a></h3>
<h4 id="æ­¥é©Ÿ-1python-è¨“ç·´é›¢ç·š"><a class="header" href="#æ­¥é©Ÿ-1python-è¨“ç·´é›¢ç·š">æ­¥é©Ÿ 1ï¼šPython è¨“ç·´ï¼ˆé›¢ç·šï¼‰</a></h4>
<pre><code class="language-python"># ä½¿ç”¨ PyTorch è¨“ç·´
import mobileclip
model = mobileclip.create_model_and_transforms('mobileclip_s0')
# è¨“ç·´éç¨‹...
torch.save(model.state_dict(), 'mobileclip_s0.pt')
</code></pre>
<h4 id="æ­¥é©Ÿ-2æ¨¡å‹è½‰æ›"><a class="header" href="#æ­¥é©Ÿ-2æ¨¡å‹è½‰æ›">æ­¥é©Ÿ 2ï¼šæ¨¡å‹è½‰æ›</a></h4>
<pre><code class="language-python"># ä½¿ç”¨ coremltools è½‰æ›
import coremltools as ct

# è½‰æ›ç‚º Core ML æ ¼å¼
coreml_model = ct.convert(
    pytorch_model,
    inputs=[ct.ImageType(shape=(1, 3, 224, 224))]
)
coreml_model.save('MobileCLIP.mlmodelc')
</code></pre>
<h4 id="æ­¥é©Ÿ-3ios-éƒ¨ç½²ç·šä¸Š"><a class="header" href="#æ­¥é©Ÿ-3ios-éƒ¨ç½²ç·šä¸Š">æ­¥é©Ÿ 3ï¼šiOS éƒ¨ç½²ï¼ˆç·šä¸Šï¼‰</a></h4>
<pre><code class="language-swift">// Swift ç¨‹å¼ç¢¼
import CoreML

let model = try MobileCLIP()
let prediction = try model.prediction(image: imageBuffer)
</code></pre>
<h3 id="23-ç‚ºä»€éº¼è¦åˆ†é›¢-textencoder-å’Œ-imageencoder"><a class="header" href="#23-ç‚ºä»€éº¼è¦åˆ†é›¢-textencoder-å’Œ-imageencoder">2.3 ç‚ºä»€éº¼è¦åˆ†é›¢ TextEncoder å’Œ ImageEncoderï¼Ÿ</a></h3>
<p><strong>è¨˜æ†¶é«”å„ªåŒ–ç­–ç•¥</strong>ï¼š</p>
<pre><code>æƒ…å¢ƒï¼šå•†å“è³‡æ–™åº«æœ‰ 10,000 å€‹å•†å“

ã€å‚³çµ±åšæ³•ã€‘ï¼ˆæµªè²»ï¼‰
æ¯æ¬¡æœå°‹éƒ½é‡æ–°è¨ˆç®—ï¼š
- 10,000 å¼µåœ–ç‰‡ Ã— ImageEncoder = è€—æ™‚
- ç”¨æˆ¶è¼¸å…¥æ–‡å­— Ã— TextEncoder = è€—æ™‚

ã€MobileCLIP åšæ³•ã€‘ï¼ˆé«˜æ•ˆï¼‰
é å…ˆè¨ˆç®—ï¼š
- 10,000 å¼µåœ–ç‰‡ â†’ è¨ˆç®—ä¸€æ¬¡ â†’ å­˜å…¥è³‡æ–™åº«
æœå°‹æ™‚ï¼š
- ç”¨æˆ¶æ–‡å­— â†’ å³æ™‚è¨ˆç®—ï¼ˆåªéœ€ 1.6msï¼‰
- èˆ‡é å­˜çš„åœ–ç‰‡å‘é‡æ¯”å°ï¼ˆ&lt; 1ç§’ï¼‰
</code></pre>
<p><strong>å¯¦éš›æ•ˆèƒ½</strong>ï¼š</p>
<ul>
<li>æœå°‹ 10,000 å¼µç…§ç‰‡ï¼š&lt; 1 ç§’</li>
<li>å³æ™‚æœå°‹é«”é©—ï¼</li>
</ul>
<hr />
<h2 id="3-å¯¦éš›æ‡‰ç”¨å•†å“æ‹ç…§æœå°‹"><a class="header" href="#3-å¯¦éš›æ‡‰ç”¨å•†å“æ‹ç…§æœå°‹">3. å¯¦éš›æ‡‰ç”¨ï¼šå•†å“æ‹ç…§æœå°‹</a></h2>
<h3 id="31-éœ€æ±‚åˆ†æ"><a class="header" href="#31-éœ€æ±‚åˆ†æ">3.1 éœ€æ±‚åˆ†æ</a></h3>
<p><strong>å ´æ™¯æè¿°</strong>ï¼š</p>
<ol>
<li>åº—å“¡æ‹æ”å•†å“ç…§ç‰‡ï¼Œæ¨™è¨˜åƒ¹æ ¼å’Œèªªæ˜</li>
<li>é¡§å®¢ç”¨æ‰‹æ©Ÿæ‹æ”å•†å“</li>
<li>ç³»çµ±è‡ªå‹•æ‰¾åˆ°å°æ‡‰å•†å“è³‡è¨Š</li>
</ol>
<h3 id="32-æœå°‹çµæœè™•ç†"><a class="header" href="#32-æœå°‹çµæœè™•ç†">3.2 æœå°‹çµæœè™•ç†</a></h3>
<h4 id="æ–¹æ¡ˆ-aå–®ä¸€æœ€ä½³çµæœæ¨è–¦"><a class="header" href="#æ–¹æ¡ˆ-aå–®ä¸€æœ€ä½³çµæœæ¨è–¦">æ–¹æ¡ˆ Aï¼šå–®ä¸€æœ€ä½³çµæœï¼ˆæ¨è–¦ï¼‰</a></h4>
<pre><code class="language-python">def search_single_best(query_image, products, threshold=0.8):
    """
    å›å‚³æœ€ç›¸ä¼¼çš„ä¸€å€‹å•†å“
    
    Args:
        query_image: ç”¨æˆ¶æ‹æ”çš„ç…§ç‰‡
        products: å•†å“è³‡æ–™åº«
        threshold: æœ€ä½ç›¸ä¼¼åº¦é–€æª» (0-1)
    
    Returns:
        æœ€ç›¸ä¼¼å•†å“æˆ– None
    """
    similarities = calculate_similarity(query_image, products)
    best_match = max(similarities, key=lambda x: x.score)
    
    if best_match.score &gt;= threshold:
        return {
            'product_id': best_match.id,
            'name': best_match.name,
            'price': best_match.price,
            'confidence': f"{best_match.score * 100:.1f}%",
            'description': best_match.description
        }
    else:
        return None  # æ‰¾ä¸åˆ°ç›¸ä¼¼å•†å“
</code></pre>
<h4 id="æ–¹æ¡ˆ-btop-k-çµæœ"><a class="header" href="#æ–¹æ¡ˆ-btop-k-çµæœ">æ–¹æ¡ˆ Bï¼šTop-K çµæœ</a></h4>
<pre><code class="language-python">def search_top_k(query_image, products, k=3, threshold=0.75):
    """
    å›å‚³å‰ K å€‹æœ€ç›¸ä¼¼å•†å“
    
    Args:
        k: å›å‚³å¹¾å€‹çµæœ
        threshold: æœ€ä½ç›¸ä¼¼åº¦é–€æª»
    """
    similarities = calculate_similarity(query_image, products)
    
    # æ’åºä¸¦ç¯©é¸
    results = sorted(similarities, key=lambda x: x.score, reverse=True)
    results = [r for r in results if r.score &gt;= threshold]
    results = results[:k]
    
    return results
</code></pre>
<h3 id="33-ç›¸ä¼¼å•†å“å»é‡"><a class="header" href="#33-ç›¸ä¼¼å•†å“å»é‡">3.3 ç›¸ä¼¼å•†å“å»é‡</a></h3>
<p><strong>å•é¡Œ</strong>ï¼šåŒä¸€å•†å“å¤šå¼µç…§ç‰‡æœƒé‡è¤‡å‡ºç¾</p>
<p><strong>è§£æ±ºæ–¹æ¡ˆ</strong>ï¼š</p>
<pre><code class="language-python">def deduplicate_results(results):
    """
    æ ¹æ“š product_id å»é‡
    """
    seen_ids = set()
    unique_results = []
    
    for result in results:
        if result.product_id not in seen_ids:
            unique_results.append(result)
            seen_ids.add(result.product_id)
    
    return unique_results
</code></pre>
<h3 id="34-uiux-è¨­è¨ˆå»ºè­°"><a class="header" href="#34-uiux-è¨­è¨ˆå»ºè­°">3.4 UI/UX è¨­è¨ˆå»ºè­°</a></h3>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [è¿”å›]    æœå°‹çµæœ          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                              â”‚
â”‚     [å•†å“åœ–ç‰‡]               â”‚
â”‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Nike æ°£å¢Šé‹å‹•é‹             â”‚
â”‚  NT$ 3,200                   â”‚
â”‚                              â”‚
â”‚  åŒ¹é…åº¦ï¼šâ˜…â˜…â˜…â˜…â˜… 95%         â”‚
â”‚                              â”‚
â”‚  èªªæ˜ï¼šé»‘è‰²ç¶“å…¸æ¬¾...         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [æŸ¥çœ‹è©³æƒ…]  [æŸ¥çœ‹æ›´å¤šç›¸ä¼¼]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3 id="35-å®Œæ•´æœå°‹ç³»çµ±"><a class="header" href="#35-å®Œæ•´æœå°‹ç³»çµ±">3.5 å®Œæ•´æœå°‹ç³»çµ±</a></h3>
<pre><code class="language-python">class ProductSearchSystem:
    def __init__(self, model_path):
        # è¼‰å…¥æ¨¡å‹
        self.model, _, self.preprocess = mobileclip.create_model_and_transforms(
            'mobileclip_s2',
            pretrained=model_path
        )
        self.tokenizer = mobileclip.get_tokenizer('mobileclip_s2')
        
        # é å…ˆè¨ˆç®—æ‰€æœ‰å•†å“çš„åœ–ç‰‡å‘é‡
        self.product_embeddings = {}
        
    def index_products(self, products):
        """
        é å…ˆè¨ˆç®—å•†å“å‘é‡ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼‰
        """
        for product in products:
            image = self.preprocess(product.image).unsqueeze(0)
            with torch.no_grad():
                embedding = self.model.encode_image(image)
                embedding /= embedding.norm(dim=-1, keepdim=True)
            
            self.product_embeddings[product.id] = {
                'embedding': embedding,
                'product': product
            }
    
    def search(self, query_image=None, query_text=None, 
               max_results=1, min_similarity=0.75):
        """
        æ··åˆæœå°‹ï¼šæ”¯æ´åœ–ç‰‡æˆ–æ–‡å­—
        """
        # è¨ˆç®—æŸ¥è©¢å‘é‡
        if query_image:
            image = self.preprocess(query_image).unsqueeze(0)
            query_embedding = self.model.encode_image(image)
        elif query_text:
            text = self.tokenizer([query_text])
            query_embedding = self.model.encode_text(text)
        else:
            raise ValueError("éœ€è¦æä¾›åœ–ç‰‡æˆ–æ–‡å­—")
        
        query_embedding /= query_embedding.norm(dim=-1, keepdim=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        results = []
        for product_id, data in self.product_embeddings.items():
            similarity = (query_embedding @ data['embedding'].T).item()
            
            if similarity &gt;= min_similarity:
                results.append({
                    'product': data['product'],
                    'similarity': similarity
                })
        
        # æ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # å»é‡
        results = self._deduplicate(results)
        
        return results[:max_results]
    
    def _deduplicate(self, results):
        """å»é™¤é‡è¤‡å•†å“"""
        seen = set()
        unique = []
        for r in results:
            pid = r['product'].id
            if pid not in seen:
                unique.append(r)
                seen.add(pid)
        return unique
</code></pre>
<hr />
<h2 id="4-æ¨¡å‹ä¸‹è¼‰èˆ‡å®‰è£"><a class="header" href="#4-æ¨¡å‹ä¸‹è¼‰èˆ‡å®‰è£">4. æ¨¡å‹ä¸‹è¼‰èˆ‡å®‰è£</a></h2>
<h3 id="41-ç’°å¢ƒå®‰è£"><a class="header" href="#41-ç’°å¢ƒå®‰è£">4.1 ç’°å¢ƒå®‰è£</a></h3>
<pre><code class="language-bash"># å‰µå»ºè™›æ“¬ç’°å¢ƒ
conda create -n clipenv python=3.10
conda activate clipenv

# å®‰è£ MobileCLIP
git clone https://github.com/apple/ml-mobileclip.git
cd ml-mobileclip
pip install -e .
</code></pre>
<h3 id="42-ä¸‹è¼‰æ¨¡å‹"><a class="header" href="#42-ä¸‹è¼‰æ¨¡å‹">4.2 ä¸‹è¼‰æ¨¡å‹</a></h3>
<h4 id="æ–¹æ³•-1ä½¿ç”¨å®˜æ–¹è…³æœ¬æ¨è–¦"><a class="header" href="#æ–¹æ³•-1ä½¿ç”¨å®˜æ–¹è…³æœ¬æ¨è–¦">æ–¹æ³• 1ï¼šä½¿ç”¨å®˜æ–¹è…³æœ¬ï¼ˆæ¨è–¦ï¼‰</a></h4>
<pre><code class="language-bash"># è‡ªå‹•ä¸‹è¼‰æ‰€æœ‰æ¨¡å‹åˆ° checkpoints/ ç›®éŒ„
source get_pretrained_models.sh
</code></pre>
<h4 id="æ–¹æ³•-2å¾-huggingface-ä¸‹è¼‰"><a class="header" href="#æ–¹æ³•-2å¾-huggingface-ä¸‹è¼‰">æ–¹æ³• 2ï¼šå¾ HuggingFace ä¸‹è¼‰</a></h4>
<pre><code class="language-bash"># å®‰è£ HuggingFace CLI
pip install huggingface_hub

# ä¸‹è¼‰å–®å€‹æ¨¡å‹
hf download apple/MobileCLIP-S2

# ä¸‹è¼‰æ‰€æœ‰æ¨¡å‹
for model in S0 S1 S2 B B-LT; do
    hf download apple/MobileCLIP-$model
done
</code></pre>
<h4 id="æ–¹æ³•-3ç›´æ¥ä¸‹è¼‰é€£çµ"><a class="header" href="#æ–¹æ³•-3ç›´æ¥ä¸‹è¼‰é€£çµ">æ–¹æ³• 3ï¼šç›´æ¥ä¸‹è¼‰é€£çµ</a></h4>
<ul>
<li><a href="https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt">MobileCLIP-S0</a> (11.4M)</li>
<li><a href="https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s1.pt">MobileCLIP-S1</a> (21.5M)</li>
<li><a href="https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s2.pt">MobileCLIP-S2</a> (35.7M)</li>
<li><a href="https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_b.pt">MobileCLIP-B</a> (86.3M)</li>
<li><a href="https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_blt.pt">MobileCLIP-B-LT</a> (86.3M)</li>
</ul>
<h3 id="43-æ¨¡å‹é¸æ“‡æŒ‡å—"><a class="header" href="#43-æ¨¡å‹é¸æ“‡æŒ‡å—">4.3 æ¨¡å‹é¸æ“‡æŒ‡å—</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹</th><th>åƒæ•¸é‡</th><th>å»¶é²</th><th>ImageNetæº–ç¢ºç‡</th><th>é©ç”¨å ´æ™¯</th></tr></thead><tbody>
<tr><td><strong>S0</strong></td><td>54M</td><td>1.5+1.6ms</td><td>67.8%</td><td>å¿«é€ŸåŸå‹ã€æ¸¬è©¦</td></tr>
<tr><td><strong>S1</strong></td><td>64M</td><td>2.5+1.9ms</td><td>70.7%</td><td>è¼•é‡æ‡‰ç”¨</td></tr>
<tr><td><strong>S2</strong></td><td>78M</td><td>3.6+2.1ms</td><td>73.4%</td><td><strong>æ¨è–¦</strong>å¹³è¡¡æ•ˆèƒ½</td></tr>
<tr><td><strong>B</strong></td><td>129M</td><td>10.4+3.3ms</td><td>76.8%</td><td>é«˜ç²¾æº–éœ€æ±‚</td></tr>
<tr><td><strong>B-LT</strong></td><td>129M</td><td>10.4+3.3ms</td><td>77.2%</td><td><strong>æœ€é«˜ç²¾æº–åº¦</strong></td></tr>
</tbody></table>
</div>
<p><strong>å»ºè­°</strong>ï¼š</p>
<ul>
<li>é–‹ç™¼æ¸¬è©¦ï¼šç”¨ <strong>S2</strong>ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰</li>
<li>æ­£å¼ç”¢å“ï¼šç”¨ <strong>B-LT</strong>ï¼ˆæœ€ä½³æ•ˆæœï¼‰</li>
</ul>
<hr />
<h2 id="5-mobileclip-åƒæ•¸è©³è§£"><a class="header" href="#5-mobileclip-åƒæ•¸è©³è§£">5. MobileCLIP åƒæ•¸è©³è§£</a></h2>
<h3 id="51-æ¨¡å‹åˆå§‹åŒ–åƒæ•¸"><a class="header" href="#51-æ¨¡å‹åˆå§‹åŒ–åƒæ•¸">5.1 æ¨¡å‹åˆå§‹åŒ–åƒæ•¸</a></h3>
<pre><code class="language-python">import mobileclip

# åŸºæœ¬è¼‰å…¥
model, _, preprocess = mobileclip.create_model_and_transforms(
    model_name='mobileclip_s2',           # æ¨¡å‹é¸æ“‡
    pretrained='/path/to/mobileclip_s2.pt' # æ¨¡å‹è·¯å¾‘
)

# MobileCLIP2 éœ€è¦é¡å¤–åƒæ•¸
import open_clip

model_kwargs = {}
if model_name in ['MobileCLIP2-S0', 'MobileCLIP2-S2', 'MobileCLIP2-B']:
    model_kwargs = {
        "image_mean": (0, 0, 0),  # åœ–ç‰‡å‡å€¼
        "image_std": (1, 1, 1)     # åœ–ç‰‡æ¨™æº–å·®
    }

model, _, preprocess = open_clip.create_model_and_transforms(
    model_name,
    pretrained=model_path,
    **model_kwargs
)
</code></pre>
<h3 id="52-åœ–ç‰‡é è™•ç†åƒæ•¸"><a class="header" href="#52-åœ–ç‰‡é è™•ç†åƒæ•¸">5.2 åœ–ç‰‡é è™•ç†åƒæ•¸</a></h3>
<p>é è™•ç†æµç¨‹è‡ªå‹•åŒ…å«ï¼š</p>
<pre><code class="language-python"># å…§å»ºçš„ preprocess å‡½æ•¸åŸ·è¡Œï¼š
# 1. Resize: èª¿æ•´åˆ° 224x224
# 2. Center Crop: ä¸­å¿ƒè£å‰ª
# 3. ToTensor: è½‰ç‚ºå¼µé‡
# 4. Normalize: æ¨™æº–åŒ–

# ä½¿ç”¨æ–¹å¼
from PIL import Image
image = Image.open('product.jpg').convert('RGB')
image_tensor = preprocess(image).unsqueeze(0)  # å¢åŠ  batch ç¶­åº¦
</code></pre>
<p><strong>æ‰‹å‹•è‡ªå®šç¾©é è™•ç†</strong>ï¼š</p>
<pre><code class="language-python">from torchvision import transforms

custom_preprocess = transforms.Compose([
    transforms.Resize(256),              # èª¿æ•´å¤§å°
    transforms.CenterCrop(224),          # ä¸­å¿ƒè£å‰ª
    transforms.ToTensor(),               # è½‰å¼µé‡
    transforms.Normalize(                # æ¨™æº–åŒ–
        mean=[0.48145466, 0.4578275, 0.40821073],
        std=[0.26862954, 0.26130258, 0.27577711]
    )
])
</code></pre>
<h3 id="53-æ¨è«–åƒæ•¸"><a class="header" href="#53-æ¨è«–åƒæ•¸">5.3 æ¨è«–åƒæ•¸</a></h3>
<pre><code class="language-python">import torch

# åŸºæœ¬æ¨è«–
with torch.no_grad():                    # é—œé–‰æ¢¯åº¦è¨ˆç®—ï¼ˆåŠ é€Ÿï¼‰
    with torch.cuda.amp.autocast():      # æ··åˆç²¾åº¦ï¼ˆå¯é¸ï¼‰
        # åœ–ç‰‡ç·¨ç¢¼
        image_features = model.encode_image(image_tensor)
        
        # æ–‡å­—ç·¨ç¢¼
        text_features = model.encode_text(text_tokens)
        
        # ç‰¹å¾µæ­£è¦åŒ–ï¼ˆé‡è¦ï¼ï¼‰
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
</code></pre>
<p><strong>åƒæ•¸èªªæ˜</strong>ï¼š</p>
<ul>
<li><code>torch.no_grad()</code>ï¼šä¸è¨ˆç®—æ¢¯åº¦ï¼Œç¯€çœè¨˜æ†¶é«”å’Œæ™‚é–“</li>
<li><code>torch.cuda.amp.autocast()</code>ï¼šè‡ªå‹•æ··åˆç²¾åº¦ï¼ŒåŠ é€Ÿé‹ç®—ï¼ˆéœ€è¦ GPUï¼‰</li>
<li><code>100.0 *</code>ï¼šæº«åº¦åƒæ•¸ï¼Œæ§åˆ¶ softmax çš„éŠ³åˆ©åº¦</li>
<li><code>.softmax(dim=-1)</code>ï¼šå°‡ç›¸ä¼¼åº¦è½‰ç‚ºæ©Ÿç‡åˆ†ä½ˆ</li>
</ul>
<h3 id="54-æº«åº¦åƒæ•¸temperatureèª¿æ•´"><a class="header" href="#54-æº«åº¦åƒæ•¸temperatureèª¿æ•´">5.4 æº«åº¦åƒæ•¸ï¼ˆTemperatureï¼‰èª¿æ•´</a></h3>
<pre><code class="language-python"># æº«åº¦åƒæ•¸å½±éŸ¿ç›¸ä¼¼åº¦çš„éŠ³åˆ©åº¦
temperature = 100.0  # é è¨­å€¼

# é«˜æº«åº¦ (&gt; 100)ï¼šåˆ†ä½ˆæ›´å¹³æ»‘ï¼Œå·®ç•°è¼ƒå°
# ä½æº«åº¦ (&lt; 100)ï¼šåˆ†ä½ˆæ›´å°–éŠ³ï¼Œå·®ç•°æ›´æ˜é¡¯

similarity = (temperature * image_features @ text_features.T).softmax(dim=-1)
</code></pre>
<p><strong>å¯¦é©—å»ºè­°</strong>ï¼š</p>
<pre><code class="language-python"># ä¸åŒå ´æ™¯çš„æº«åº¦å»ºè­°
temperatures = {
    'å•†å“æœå°‹': 100.0,      # é è¨­ï¼Œå¹³è¡¡
    'ç²¾ç¢ºåŒ¹é…': 50.0,       # æ›´åš´æ ¼
    'æ¨¡ç³Šæœå°‹': 150.0,      # æ›´å¯¬é¬†
}
</code></pre>
<h3 id="55-ç›¸ä¼¼åº¦é–¾å€¼è¨­å®š"><a class="header" href="#55-ç›¸ä¼¼åº¦é–¾å€¼è¨­å®š">5.5 ç›¸ä¼¼åº¦é–¾å€¼è¨­å®š</a></h3>
<pre><code class="language-python">class SearchConfig:
    """æœå°‹é…ç½®"""
    
    # ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆ0-1ï¼‰
    HIGH_CONFIDENCE = 0.85    # é«˜ä¿¡å¿ƒ
    MEDIUM_CONFIDENCE = 0.75  # ä¸­ä¿¡å¿ƒ
    LOW_CONFIDENCE = 0.60     # ä½ä¿¡å¿ƒ
    
    # Top-K è¨­å®š
    MAX_RESULTS = 5           # æœ€å¤šè¿”å›çµæœ
    
    # å»é‡è¨­å®š
    ENABLE_DEDUP = True       # å•Ÿç”¨å»é‡

# ä½¿ç”¨ç¯„ä¾‹
def search_with_config(query, config):
    results = calculate_similarity(query)
    
    # æ ¹æ“šä¿¡å¿ƒåº¦ç¯©é¸
    if config.use_high_threshold:
        results = [r for r in results if r.score &gt;= config.HIGH_CONFIDENCE]
    
    return results[:config.MAX_RESULTS]
</code></pre>
<h3 id="56-æ‰¹æ¬¡è™•ç†åƒæ•¸"><a class="header" href="#56-æ‰¹æ¬¡è™•ç†åƒæ•¸">5.6 æ‰¹æ¬¡è™•ç†åƒæ•¸</a></h3>
<pre><code class="language-python">def batch_encode_images(images, batch_size=32):
    """
    æ‰¹æ¬¡è™•ç†åœ–ç‰‡ï¼Œæé«˜æ•ˆç‡
    
    Args:
        images: åœ–ç‰‡åˆ—è¡¨
        batch_size: æ¯æ‰¹è™•ç†æ•¸é‡
    """
    all_features = []
    
    for i in range(0, len(images), batch_size):
        batch = images[i:i+batch_size]
        
        # æ‰¹æ¬¡é è™•ç†
        batch_tensors = torch.stack([preprocess(img) for img in batch])
        
        with torch.no_grad():
            features = model.encode_image(batch_tensors)
            features /= features.norm(dim=-1, keepdim=True)
            all_features.append(features)
    
    return torch.cat(all_features, dim=0)
</code></pre>
<hr />
<h2 id="6-å¸¸ç”¨æ‡‰ç”¨å ´æ™¯"><a class="header" href="#6-å¸¸ç”¨æ‡‰ç”¨å ´æ™¯">6. å¸¸ç”¨æ‡‰ç”¨å ´æ™¯</a></h2>
<h3 id="61-é›¶æ¨£æœ¬åœ–ç‰‡åˆ†é¡"><a class="header" href="#61-é›¶æ¨£æœ¬åœ–ç‰‡åˆ†é¡">6.1 é›¶æ¨£æœ¬åœ–ç‰‡åˆ†é¡</a></h3>
<pre><code class="language-python">from PIL import Image
import mobileclip
import torch

# è¼‰å…¥æ¨¡å‹
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s2',
    pretrained='checkpoints/mobileclip_s2.pt'
)
tokenizer = mobileclip.get_tokenizer('mobileclip_s2')

# æº–å‚™åœ–ç‰‡
image = Image.open('product.jpg').convert('RGB')
image_input = preprocess(image).unsqueeze(0)

# æº–å‚™å€™é¸æ¨™ç±¤
labels = ["æ‰‹æ©Ÿ", "ç­†è¨˜å‹é›»è…¦", "å¹³æ¿", "è€³æ©Ÿ", "ç›¸æ©Ÿ"]
text_inputs = tokenizer(labels)

# æ¨è«–
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)
    
    # æ­£è¦åŒ–
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)

# é¡¯ç¤ºçµæœ
for label, score in zip(labels, similarity[0]):
    print(f"{label}: {score.item()*100:.2f}%")

# è¼¸å‡ºç¯„ä¾‹ï¼š
# æ‰‹æ©Ÿ: 85.32%
# ç­†è¨˜å‹é›»è…¦: 8.45%
# å¹³æ¿: 4.23%
# è€³æ©Ÿ: 1.50%
# ç›¸æ©Ÿ: 0.50%
</code></pre>
<h3 id="62-åœ–ç‰‡ç›¸ä¼¼åº¦æœå°‹"><a class="header" href="#62-åœ–ç‰‡ç›¸ä¼¼åº¦æœå°‹">6.2 åœ–ç‰‡ç›¸ä¼¼åº¦æœå°‹</a></h3>
<pre><code class="language-python">def find_similar_images(query_image, image_database, top_k=5):
    """
    åœ¨åœ–ç‰‡è³‡æ–™åº«ä¸­æ‰¾å‡ºæœ€ç›¸ä¼¼çš„åœ–ç‰‡
    
    Args:
        query_image: æŸ¥è©¢åœ–ç‰‡
        image_database: åœ–ç‰‡è³‡æ–™åº«ï¼ˆå·²é è¨ˆç®—å‘é‡ï¼‰
        top_k: è¿”å›å‰ K å€‹çµæœ
    """
    # ç·¨ç¢¼æŸ¥è©¢åœ–ç‰‡
    query_tensor = preprocess(query_image).unsqueeze(0)
    with torch.no_grad():
        query_features = model.encode_image(query_tensor)
        query_features /= query_features.norm(dim=-1, keepdim=True)
    
    # è¨ˆç®—èˆ‡è³‡æ–™åº«ä¸­æ‰€æœ‰åœ–ç‰‡çš„ç›¸ä¼¼åº¦
    similarities = []
    for img_id, img_features in image_database.items():
        similarity = (query_features @ img_features.T).item()
        similarities.append((img_id, similarity))
    
    # æ’åºä¸¦è¿”å› top-k
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:top_k]

# ä½¿ç”¨ç¯„ä¾‹
similar_images = find_similar_images(
    query_image=Image.open('query.jpg'),
    image_database=precomputed_vectors,
    top_k=5
)

for img_id, score in similar_images:
    print(f"åœ–ç‰‡ {img_id}: ç›¸ä¼¼åº¦ {score*100:.1f}%")
</code></pre>
<h3 id="63-æ–‡å­—æœå°‹åœ–ç‰‡"><a class="header" href="#63-æ–‡å­—æœå°‹åœ–ç‰‡">6.3 æ–‡å­—æœå°‹åœ–ç‰‡</a></h3>
<pre><code class="language-python">def text_to_image_search(query_text, image_database, threshold=0.7):
    """
    ç”¨æ–‡å­—æè¿°æœå°‹åœ–ç‰‡
    
    Args:
        query_text: æ–‡å­—æè¿°
        image_database: åœ–ç‰‡è³‡æ–™åº«
        threshold: ç›¸ä¼¼åº¦é–¾å€¼
    """
    # ç·¨ç¢¼æŸ¥è©¢æ–‡å­—
    text_tokens = tokenizer([query_text])
    with torch.no_grad():
        text_features = model.encode_text(text_tokens)
        text_features /= text_features.norm(dim=-1, keepdim=True)
    
    # æœå°‹åŒ¹é…åœ–ç‰‡
    results = []
    for img_id, img_features in image_database.items():
        similarity = (text_features @ img_features.T).item()
        if similarity &gt;= threshold:
            results.append({
                'id': img_id,
                'similarity': similarity
            })
    
    results.sort(key=lambda x: x['similarity'], reverse=True)
    return results

# ä½¿ç”¨ç¯„ä¾‹
results = text_to_image_search(
    query_text="ä¸€é›™ç´…è‰²çš„é‹å‹•é‹",
    image_database=product_vectors,
    threshold=0.7
)

for result in results[:5]:
    print(f"å•†å“ {result['id']}: {result['similarity']*100:.1f}%")
</code></pre>
<h3 id="64-åœ–ç‰‡å»é‡"><a class="header" href="#64-åœ–ç‰‡å»é‡">6.4 åœ–ç‰‡å»é‡</a></h3>
<pre><code class="language-python">def detect_duplicates(images, similarity_threshold=0.95):
    """
    æª¢æ¸¬é‡è¤‡æˆ–ç›¸ä¼¼åœ–ç‰‡
    
    Args:
        images: åœ–ç‰‡åˆ—è¡¨
        similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆè¶Šé«˜è¶Šåš´æ ¼ï¼‰
    """
    # è¨ˆç®—æ‰€æœ‰åœ–ç‰‡çš„å‘é‡
    features = []
    for img in images:
        img_tensor = preprocess(img).unsqueeze(0)
        with torch.no_grad():
            feat = model.encode_image(img_tensor)
            feat /= feat.norm(dim=-1, keepdim=True)
            features.append(feat)
    
    # æ‰¾å‡ºé‡è¤‡çµ„
    duplicates = []
    seen = set()
    
    for i in range(len(features)):
        if i in seen:
            continue
        
        group = [i]
        for j in range(i+1, len(features)):
            if j in seen:
                continue
            
            similarity = (features[i] @ features[j].T).item()
            if similarity &gt;= similarity_threshold:
                group.append(j)
                seen.add(j)
        
        if len(group) &gt; 1:
            duplicates.append(group)
    
    return duplicates

# ä½¿ç”¨ç¯„ä¾‹
image_list = [Image.open(f'img_{i}.jpg') for i in range(100)]
duplicate_groups = detect_duplicates(image_list, similarity_threshold=0.95)

print(f"æ‰¾åˆ° {len(duplicate_groups)} çµ„é‡è¤‡åœ–ç‰‡")
for i, group in enumerate(duplicate_groups):
    print(f"çµ„ {i+1}: åœ–ç‰‡ {group}")
</code></pre>
<h3 id="65-å¤šæ¨¡æ…‹æª¢ç´¢hybrid-search"><a class="header" href="#65-å¤šæ¨¡æ…‹æª¢ç´¢hybrid-search">6.5 å¤šæ¨¡æ…‹æª¢ç´¢ï¼ˆHybrid Searchï¼‰</a></h3>
<pre><code class="language-python">def hybrid_search(query_image=None, query_text=None, 
                  image_database=None, weight_image=0.5):
    """
    çµåˆåœ–ç‰‡å’Œæ–‡å­—çš„æ··åˆæœå°‹
    
    Args:
        query_image: æŸ¥è©¢åœ–ç‰‡ï¼ˆå¯é¸ï¼‰
        query_text: æŸ¥è©¢æ–‡å­—ï¼ˆå¯é¸ï¼‰
        image_database: åœ–ç‰‡è³‡æ–™åº«
        weight_image: åœ–ç‰‡æ¬Šé‡ï¼ˆ0-1ï¼Œæ–‡å­—æ¬Šé‡ç‚º 1-weight_imageï¼‰
    """
    # è¨ˆç®—åœ–ç‰‡ç‰¹å¾µ
    if query_image:
        img_tensor = preprocess(query_image).unsqueeze(0)
        with torch.no_grad():
            img_features = model.encode_image(img_tensor)
            img_features /= img_features.norm(dim=-1, keepdim=True)
    else:
        img_features = None
    
    # è¨ˆç®—æ–‡å­—ç‰¹å¾µ
    if query_text:
        text_tokens = tokenizer([query_text])
        with torch.no_grad():
            text_features = model.encode_text(text_tokens)
            text_features /= text_features.norm(dim=-1, keepdim=True)
    else:
        text_features = None
    
    # æ··åˆæœå°‹
    results = []
    for img_id, db_features in image_database.items():
        score = 0
        
        if img_features is not None:
            img_sim = (img_features @ db_features.T).item()
            score += weight_image * img_sim
        
        if text_features is not None:
            text_sim = (text_features @ db_features.T).item()
            score += (1 - weight_image) * text_sim
        
        results.append({'id': img_id, 'score': score})
    
    results.sort(key=lambda x: x['score'], reverse=True)
    return results

# ä½¿ç”¨ç¯„ä¾‹ï¼šæ—¢æœ‰åœ–ç‰‡åˆæœ‰æ–‡å­—æè¿°
results = hybrid_search(
    query_image=Image.open('example.jpg'),
    query_text="ç´…è‰²é‹å‹•é‹",
    image_database=product_vectors,
    weight_image=0.6  # 60% çœ‹åœ–ç‰‡ï¼Œ40% çœ‹æ–‡å­—
)
</code></pre>
<hr />
<h2 id="7-å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹"><a class="header" href="#7-å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹">7. å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹</a></h2>
<h3 id="71-å•†å“æœå°‹å®Œæ•´ç³»çµ±"><a class="header" href="#71-å•†å“æœå°‹å®Œæ•´ç³»çµ±">7.1 å•†å“æœå°‹å®Œæ•´ç³»çµ±</a></h3>
<pre><code class="language-python">import torch
import mobileclip
from PIL import Image
from typing import List, Dict, Optional
import numpy as np
from dataclasses import dataclass

@dataclass
class Product:
    """å•†å“è³‡æ–™çµæ§‹"""
    id: str
    name: str
    price: float
    description: str
    image_path: str
    category: str

class ProductSearchEngine:
    """å•†å“æœå°‹å¼•æ“"""
    
    def __init__(self, model_name='mobileclip_s2', model_path=None):
        """
        åˆå§‹åŒ–æœå°‹å¼•æ“
        
        Args:
            model_name: æ¨¡å‹åç¨±
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        """
        print("ğŸ”§ æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
        self.model, _, self.preprocess = mobileclip.create_model_and_transforms(
            model_name,
            pretrained=model_path
        )
        self.tokenizer = mobileclip.get_tokenizer(model_name)
        self.model.eval()  # è¨­ç‚ºè©•ä¼°æ¨¡å¼
        
        # ç”¢å“å‘é‡è³‡æ–™åº«
        self.product_vectors = {}
        self.products = {}
        
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    
    def add_product(self, product: Product):
        """
        æ–°å¢å•†å“åˆ°è³‡æ–™åº«
        
        Args:
            product: å•†å“ç‰©ä»¶
        """
        # è¼‰å…¥ä¸¦é è™•ç†åœ–ç‰‡
        image = Image.open(product.image_path).convert('RGB')
        image_tensor = self.preprocess(image).unsqueeze(0)
        
        # è¨ˆç®—åœ–ç‰‡å‘é‡
        with torch.no_grad():
            features = self.model.encode_image(image_tensor)
            features /= features.norm(dim=-1, keepdim=True)
        
        # å­˜å…¥è³‡æ–™åº«
        self.product_vectors[product.id] = features
        self.products[product.id] = product
        
        print(f"âœ… å·²æ–°å¢å•†å“: {product.name}")
    
    def batch_add_products(self, products: List[Product], batch_size=32):
        """
        æ‰¹æ¬¡æ–°å¢å•†å“ï¼ˆæ›´é«˜æ•ˆï¼‰
        
        Args:
            products: å•†å“åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        print(f"ğŸ“¦ æ­£åœ¨æ‰¹æ¬¡è™•ç† {len(products)} å€‹å•†å“...")
        
        for i in range(0, len(products), batch_size):
            batch = products[i:i+batch_size]
            
            # æ‰¹æ¬¡è¼‰å…¥åœ–ç‰‡
            images = []
            for product in batch:
                image = Image.open(product.image_path).convert('RGB')
                images.append(self.preprocess(image))
            
            # æ‰¹æ¬¡è¨ˆç®—å‘é‡
            batch_tensor = torch.stack(images)
            with torch.no_grad():
                features = self.model.encode_image(batch_tensor)
                features /= features.norm(dim=-1, keepdim=True)
            
            # å­˜å…¥è³‡æ–™åº«
            for j, product in enumerate(batch):
                self.product_vectors[product.id] = features[j:j+1]
                self.products[product.id] = product
            
            print(f"é€²åº¦: {min(i+batch_size, len(products))}/{len(products)}")
        
        print("âœ… æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
    
    def search_by_image(self, 
                       query_image: Image.Image,
                       top_k: int = 5,
                       min_similarity: float = 0.7) -&gt; List[Dict]:
        """
        ç”¨åœ–ç‰‡æœå°‹å•†å“
        
        Args:
            query_image: æŸ¥è©¢åœ–ç‰‡
            top_k: è¿”å›å‰ K å€‹çµæœ
            min_similarity: æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼
        
        Returns:
            æœå°‹çµæœåˆ—è¡¨
        """
        # ç·¨ç¢¼æŸ¥è©¢åœ–ç‰‡
        query_tensor = self.preprocess(query_image).unsqueeze(0)
        with torch.no_grad():
            query_features = self.model.encode_image(query_tensor)
            query_features /= query_features.norm(dim=-1, keepdim=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        results = []
        for product_id, product_features in self.product_vectors.items():
            similarity = (query_features @ product_features.T).item()
            
            if similarity &gt;= min_similarity:
                product = self.products[product_id]
                results.append({
                    'product_id': product.id,
                    'name': product.name,
                    'price': product.price,
                    'description': product.description,
                    'category': product.category,
                    'similarity': similarity,
                    'confidence': f"{similarity*100:.1f}%"
                })
        
        # æ’åºä¸¦è¿”å›å‰ K å€‹
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def search_by_text(self,
                      query_text: str,
                      top_k: int = 5,
                      min_similarity: float = 0.6) -&gt; List[Dict]:
        """
        ç”¨æ–‡å­—æœå°‹å•†å“
        
        Args:
            query_text: æŸ¥è©¢æ–‡å­—
            top_k: è¿”å›å‰ K å€‹çµæœ
            min_similarity: æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼
        """
        # ç·¨ç¢¼æŸ¥è©¢æ–‡å­—
        text_tokens = self.tokenizer([query_text])
        with torch.no_grad():
            query_features = self.model.encode_text(text_tokens)
            query_features /= query_features.norm(dim=-1, keepdim=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦ä¸¦è¿”å›çµæœ
        results = []
        for product_id, product_features in self.product_vectors.items():
            similarity = (query_features @ product_features.T).item()
            
            if similarity &gt;= min_similarity:
                product = self.products[product_id]
                results.append({
                    'product_id': product.id,
                    'name': product.name,
                    'price': product.price,
                    'description': product.description,
                    'category': product.category,
                    'similarity': similarity,
                    'confidence': f"{similarity*100:.1f}%"
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def hybrid_search(self,
                     query_image: Optional[Image.Image] = None,
                     query_text: Optional[str] = None,
                     image_weight: float = 0.6,
                     top_k: int = 5) -&gt; List[Dict]:
        """
        æ··åˆæœå°‹ï¼šçµåˆåœ–ç‰‡å’Œæ–‡å­—
        
        Args:
            query_image: æŸ¥è©¢åœ–ç‰‡ï¼ˆå¯é¸ï¼‰
            query_text: æŸ¥è©¢æ–‡å­—ï¼ˆå¯é¸ï¼‰
            image_weight: åœ–ç‰‡æ¬Šé‡ï¼ˆ0-1ï¼‰
            top_k: è¿”å›å‰ K å€‹çµæœ
        """
        if query_image is None and query_text is None:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›åœ–ç‰‡æˆ–æ–‡å­—")
        
        # è¨ˆç®—åœ–ç‰‡ç‰¹å¾µ
        img_features = None
        if query_image:
            query_tensor = self.preprocess(query_image).unsqueeze(0)
            with torch.no_grad():
                img_features = self.model.encode_image(query_tensor)
                img_features /= img_features.norm(dim=-1, keepdim=True)
        
        # è¨ˆç®—æ–‡å­—ç‰¹å¾µ
        text_features = None
        if query_text:
            text_tokens = self.tokenizer([query_text])
            with torch.no_grad():
                text_features = self.model.encode_text(text_tokens)
                text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # æ··åˆè¨ˆç®—ç›¸ä¼¼åº¦
        results = []
        for product_id, product_features in self.product_vectors.items():
            score = 0
            
            if img_features is not None:
                img_sim = (img_features @ product_features.T).item()
                score += image_weight * img_sim
            
            if text_features is not None:
                text_sim = (text_features @ product_features.T).item()
                score += (1 - image_weight) * text_sim
            
            product = self.products[product_id]
            results.append({
                'product_id': product.id,
                'name': product.name,
                'price': product.price,
                'description': product.description,
                'category': product.category,
                'similarity': score,
                'confidence': f"{score*100:.1f}%"
            })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]
    
    def save_database(self, filepath: str):
        """å„²å­˜å‘é‡è³‡æ–™åº«"""
        torch.save({
            'product_vectors': self.product_vectors,
            'products': self.products
        }, filepath)
        print(f"âœ… è³‡æ–™åº«å·²å„²å­˜è‡³: {filepath}")
    
    def load_database(self, filepath: str):
        """è¼‰å…¥å‘é‡è³‡æ–™åº«"""
        data = torch.load(filepath)
        self.product_vectors = data['product_vectors']
        self.products = data['products']
        print(f"âœ… å·²è¼‰å…¥ {len(self.products)} å€‹å•†å“")


# ============== ä½¿ç”¨ç¯„ä¾‹ ==============

def main():
    # 1. åˆå§‹åŒ–æœå°‹å¼•æ“
    engine = ProductSearchEngine(
        model_name='mobileclip_s2',
        model_path='checkpoints/mobileclip_s2.pt'
    )
    
    # 2. æº–å‚™å•†å“è³‡æ–™
    products = [
        Product(
            id='P001',
            name='Nike æ°£å¢Šé‹å‹•é‹',
            price=3200,
            description='é»‘è‰²ç¶“å…¸æ¬¾ï¼Œèˆ’é©é€æ°£',
            image_path='products/nike_shoes.jpg',
            category='é‹é¡'
        ),
        Product(
            id='P002',
            name='Adidas ä¼‘é–’é‹',
            price=2800,
            description='ç™½è‰²ç°¡ç´„è¨­è¨ˆ',
            image_path='products/adidas_shoes.jpg',
            category='é‹é¡'
        ),
        # ... æ›´å¤šå•†å“
    ]
    
    # 3. æ‰¹æ¬¡æ–°å¢å•†å“
    engine.batch_add_products(products)
    
    # 4. å„²å­˜è³‡æ–™åº«ï¼ˆé¸ç”¨ï¼‰
    engine.save_database('product_database.pt')
    
    # 5. æœå°‹ç¯„ä¾‹
    
    # æ–¹æ³• Aï¼šåœ–ç‰‡æœå°‹
    query_image = Image.open('customer_photo.jpg')
    results = engine.search_by_image(query_image, top_k=3)
    
    print("\nğŸ“¸ åœ–ç‰‡æœå°‹çµæœï¼š")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['name']}")
        print(f"   åƒ¹æ ¼: NT${result['price']}")
        print(f"   ä¿¡å¿ƒåº¦: {result['confidence']}")
        print()
    
    # æ–¹æ³• Bï¼šæ–‡å­—æœå°‹
    results = engine.search_by_text("ç´…è‰²é‹å‹•é‹", top_k=3)
    
    print("\nğŸ” æ–‡å­—æœå°‹çµæœï¼š")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['name']} - {result['confidence']}")
    
    # æ–¹æ³• Cï¼šæ··åˆæœå°‹
    results = engine.hybrid_search(
        query_image=Image.open('customer_photo.jpg'),
        query_text="èˆ’é©çš„é‹å‹•é‹",
        image_weight=0.7,  # 70% çœ‹åœ–ç‰‡ï¼Œ30% çœ‹æ–‡å­—
        top_k=3
    )
    
    print("\nğŸ¯ æ··åˆæœå°‹çµæœï¼š")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['name']} - {result['confidence']}")

if __name__ == '__main__':
    main()
</code></pre>
<h3 id="72-å¯¦æ™‚ç›¸æ©Ÿæœå°‹ios-é¢¨æ ¼"><a class="header" href="#72-å¯¦æ™‚ç›¸æ©Ÿæœå°‹ios-é¢¨æ ¼">7.2 å¯¦æ™‚ç›¸æ©Ÿæœå°‹ï¼ˆiOS é¢¨æ ¼ï¼‰</a></h3>
<pre><code class="language-python">import cv2
import torch
import mobileclip
from PIL import Image
import time

class RealtimeProductSearch:
    """å¯¦æ™‚å•†å“æœå°‹"""
    
    def __init__(self, model_path, database_path):
        # è¼‰å…¥æ¨¡å‹
        self.model, _, self.preprocess = mobileclip.create_model_and_transforms(
            'mobileclip_s2',
            pretrained=model_path
        )
        self.model.eval()
        
        # è¼‰å…¥å•†å“è³‡æ–™åº«
        data = torch.load(database_path)
        self.product_vectors = data['product_vectors']
        self.products = data['products']
        
        print("âœ… å¯¦æ™‚æœå°‹ç³»çµ±å·²å°±ç·’")
    
    def search_frame(self, frame, threshold=0.75):
        """
        æœå°‹å–®ä¸€ç•«é¢
        
        Args:
            frame: OpenCV ç•«é¢ (BGR)
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
        """
        # è½‰æ›ç‚º RGB PIL Image
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        # é è™•ç†ä¸¦ç·¨ç¢¼
        image_tensor = self.preprocess(pil_image).unsqueeze(0)
        with torch.no_grad():
            query_features = self.model.encode_image(image_tensor)
            query_features /= query_features.norm(dim=-1, keepdim=True)
        
        # æ‰¾æœ€ä½³åŒ¹é…
        best_match = None
        best_score = threshold
        
        for product_id, product_features in self.product_vectors.items():
            similarity = (query_features @ product_features.T).item()
            if similarity &gt; best_score:
                best_score = similarity
                best_match = self.products[product_id]
        
        return best_match, best_score
    
    def run_camera(self, camera_id=0):
        """
        é‹è¡Œç›¸æ©Ÿå¯¦æ™‚æœå°‹
        
        Args:
            camera_id: ç›¸æ©Ÿç·¨è™Ÿ
        """
        cap = cv2.VideoCapture(camera_id)
        
        print("ğŸ“· ç›¸æ©Ÿå·²å•Ÿå‹•ï¼ŒæŒ‰ 'q' é€€å‡º")
        
        # FPS è¨ˆç®—
        fps_time = time.time()
        fps_counter = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ¯ 10 å¹€æœå°‹ä¸€æ¬¡ï¼ˆæ¸›å°‘é‹ç®—ï¼‰
            if fps_counter % 10 == 0:
                product, score = self.search_frame(frame)
                
                # åœ¨ç•«é¢ä¸Šé¡¯ç¤ºçµæœ
                if product:
                    text = f"{product.name} ({score*100:.1f}%)"
                    price_text = f"NT$ {product.price}"
                    
                    # ç¹ªè£½åŠé€æ˜èƒŒæ™¯
                    overlay = frame.copy()
                    cv2.rectangle(overlay, (10, 10), (400, 100), (0, 0, 0), -1)
                    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
                    
                    # ç¹ªè£½æ–‡å­—
                    cv2.putText(frame, text, (20, 40),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
                    cv2.putText(frame, price_text, (20, 75),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # è¨ˆç®—ä¸¦é¡¯ç¤º FPS
            fps_counter += 1
            if time.time() - fps_time &gt; 1:
                fps = fps_counter / (time.time() - fps_time)
                fps_counter = 0
                fps_time = time.time()
            
            cv2.putText(frame, f"FPS: {fps:.1f}", (frame.shape[1]-150, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # é¡¯ç¤ºç•«é¢
            cv2.imshow('Product Search', frame)
            
            # æŒ‰ 'q' é€€å‡º
            if cv2.waitKey(1) &amp; 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == '__main__':
    searcher = RealtimeProductSearch(
        model_path='checkpoints/mobileclip_s2.pt',
        database_path='product_database.pt'
    )
    searcher.run_camera(camera_id=0)
</code></pre>
<hr />
<h2 id="8-æ•ˆèƒ½å„ªåŒ–å»ºè­°"><a class="header" href="#8-æ•ˆèƒ½å„ªåŒ–å»ºè­°">8. æ•ˆèƒ½å„ªåŒ–å»ºè­°</a></h2>
<h3 id="81-æ¨¡å‹é¸æ“‡å„ªåŒ–"><a class="header" href="#81-æ¨¡å‹é¸æ“‡å„ªåŒ–">8.1 æ¨¡å‹é¸æ“‡å„ªåŒ–</a></h3>
<pre><code class="language-python"># æ ¹æ“šè£ç½®é¸æ“‡æ¨¡å‹
import platform

def select_optimal_model():
    """è‡ªå‹•é¸æ“‡æœ€é©åˆçš„æ¨¡å‹"""
    
    system = platform.system()
    
    if torch.cuda.is_available():
        # æœ‰ GPUï¼šä½¿ç”¨æœ€å¤§æ¨¡å‹
        return 'mobileclip_blt', 'checkpoints/mobileclip_blt.pt'
    
    elif system == 'Darwin':  # macOS
        # Apple Siliconï¼šä½¿ç”¨ä¸­ç­‰æ¨¡å‹
        return 'mobileclip_s2', 'checkpoints/mobileclip_s2.pt'
    
    else:
        # ä¸€èˆ¬ CPUï¼šä½¿ç”¨å°æ¨¡å‹
        return 'mobileclip_s0', 'checkpoints/mobileclip_s0.pt'

model_name, model_path = select_optimal_model()
print(f"âœ… é¸æ“‡æ¨¡å‹: {model_name}")
</code></pre>
<h3 id="82-è¨˜æ†¶é«”å„ªåŒ–"><a class="header" href="#82-è¨˜æ†¶é«”å„ªåŒ–">8.2 è¨˜æ†¶é«”å„ªåŒ–</a></h3>
<pre><code class="language-python"># ä½¿ç”¨åŠç²¾åº¦æµ®é»æ•¸ï¼ˆFP16ï¼‰
model = model.half()  # è½‰ç‚º FP16ï¼Œè¨˜æ†¶é«”æ¸›åŠ

# æ¨è«–æ™‚ä½¿ç”¨
with torch.cuda.amp.autocast():
    features = model.encode_image(image)

# æ‰¹æ¬¡è™•ç†æ™‚æ¸…ç†è¨˜æ†¶é«”
import gc

def batch_process_with_cleanup(images, batch_size=32):
    for i in range(0, len(images), batch_size):
        batch = images[i:i+batch_size]
        # ... è™•ç†æ‰¹æ¬¡ ...
        
        # æ¸…ç†è¨˜æ†¶é«”
        torch.cuda.empty_cache()
        gc.collect()
</code></pre>
<h3 id="83-é€Ÿåº¦å„ªåŒ–"><a class="header" href="#83-é€Ÿåº¦å„ªåŒ–">8.3 é€Ÿåº¦å„ªåŒ–</a></h3>
<pre><code class="language-python"># 1. é å…ˆè¨ˆç®—æ‰€æœ‰å•†å“å‘é‡ï¼ˆåªåšä¸€æ¬¡ï¼‰
def precompute_all_products(products):
    """é å…ˆè¨ˆç®—ï¼Œå¤§å¹…æå‡æœå°‹é€Ÿåº¦"""
    print("æ­£åœ¨é è¨ˆç®—æ‰€æœ‰å•†å“å‘é‡...")
    
    all_features = []
    for product in products:
        image = Image.open(product.image_path).convert('RGB')
        image_tensor = preprocess(image).unsqueeze(0)
        
        with torch.no_grad():
            features = model.encode_image(image_tensor)
            features /= features.norm(dim=-1, keepdim=True)
            all_features.append(features)
    
    # åˆä½µç‚ºä¸€å€‹å¤§çŸ©é™£ï¼ˆæ›´å¿«çš„çŸ©é™£é‹ç®—ï¼‰
    return torch.cat(all_features, dim=0)

# 2. ä½¿ç”¨å‘é‡åŒ–é‹ç®—
def fast_batch_search(query_features, all_product_features):
    """å‘é‡åŒ–æœå°‹ï¼Œæ¯”è¿´åœˆå¿« 10-100 å€"""
    # ä¸€æ¬¡è¨ˆç®—æ‰€æœ‰ç›¸ä¼¼åº¦
    similarities = (query_features @ all_product_features.T)
    
    # æ‰¾å‡ºæœ€ä½³åŒ¹é…
    best_idx = similarities.argmax().item()
    best_score = similarities[0, best_idx].item()
    
    return best_idx, best_score

# 3. ä½¿ç”¨ TorchScriptï¼ˆåŠ é€Ÿ 50%ï¼‰
traced_model = torch.jit.trace(
    model,
    (torch.randn(1, 3, 224, 224),)
)
traced_model.save('mobileclip_traced.pt')

# è¼‰å…¥åŠ é€Ÿæ¨¡å‹
fast_model = torch.jit.load('mobileclip_traced.pt')
</code></pre>
<h3 id="84-è³‡æ–™åº«å„ªåŒ–"><a class="header" href="#84-è³‡æ–™åº«å„ªåŒ–">8.4 è³‡æ–™åº«å„ªåŒ–</a></h3>
<pre><code class="language-python">import numpy as np
from scipy.spatial.distance import cdist

class OptimizedDatabase:
    """å„ªåŒ–çš„å‘é‡è³‡æ–™åº«"""
    
    def __init__(self):
        self.vectors = None  # numpy array
        self.product_ids = []
        self.products = {}
    
    def build_index(self, product_vectors):
        """å»ºç«‹ç´¢å¼•ï¼ˆä½¿ç”¨ numpy åŠ é€Ÿï¼‰"""
        vectors = []
        for pid, vec in product_vectors.items():
            vectors.append(vec.cpu().numpy())
            self.product_ids.append(pid)
        
        # è½‰ç‚º numpy arrayï¼ˆæ›´å¿«ï¼‰
        self.vectors = np.vstack(vectors)
        print(f"âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼š{len(self.product_ids)} å€‹å•†å“")
    
    def search(self, query_vector, top_k=5):
        """ä½¿ç”¨ numpy åŠ é€Ÿæœå°‹"""
        query_np = query_vector.cpu().numpy()
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–ï¼‰
        similarities = np.dot(query_np, self.vectors.T)[0]
        
        # æ‰¾å‡º top-k
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                'product_id': self.product_ids[idx],
                'similarity': float(similarities[idx])
            })
        
        return results

# ä½¿ç”¨ç¯„ä¾‹
db = OptimizedDatabase()
db.build_index(product_vectors)

# æœå°‹é€Ÿåº¦æå‡ 5-10 å€ï¼
results = db.search(query_features, top_k=5)
</code></pre>
<h3 id="85-å¿«å–ç­–ç•¥"><a class="header" href="#85-å¿«å–ç­–ç•¥">8.5 å¿«å–ç­–ç•¥</a></h3>
<pre><code class="language-python">from functools import lru_cache
import hashlib

class CachedSearchEngine:
    """å¸¶å¿«å–çš„æœå°‹å¼•æ“"""
    
    def __init__(self, cache_size=1000):
        self.cache_size = cache_size
        self._setup_cache()
    
    def _setup_cache(self):
        """è¨­å®š LRU å¿«å–"""
        
        @lru_cache(maxsize=self.cache_size)
        def _cached_encode_image(image_hash):
            """å¿«å–åœ–ç‰‡ç·¨ç¢¼çµæœ"""
            # é€™è£¡è¿”å›é è¨ˆç®—çš„çµæœ
            return self._encode_image_impl(image_hash)
        
        self._cached_encode = _cached_encode_image
    
    def _image_to_hash(self, image):
        """å°‡åœ–ç‰‡è½‰ç‚º hashï¼ˆç”¨æ–¼å¿«å–éµï¼‰"""
        # ç°¡å–®çš„ hash æ–¹æ³•
        image_bytes = image.tobytes()
        return hashlib.md5(image_bytes).hexdigest()
    
    def search_with_cache(self, query_image):
        """ä½¿ç”¨å¿«å–çš„æœå°‹"""
        img_hash = self._image_to_hash(query_image)
        
        # å˜—è©¦å¾å¿«å–ç²å–
        features = self._cached_encode(img_hash)
        
        # æœå°‹
        return self._search_impl(features)

# ç†±é–€å•†å“çš„æœå°‹æœƒè®Šå¾—è¶…å¿«ï¼
</code></pre>
<h3 id="86-æ•ˆèƒ½åŸºæº–æ¸¬è©¦"><a class="header" href="#86-æ•ˆèƒ½åŸºæº–æ¸¬è©¦">8.6 æ•ˆèƒ½åŸºæº–æ¸¬è©¦</a></h3>
<pre><code class="language-python">import time

def benchmark_search_speed(engine, num_queries=100):
    """æ¸¬è©¦æœå°‹é€Ÿåº¦"""
    
    print("ğŸ”¬ é–‹å§‹æ•ˆèƒ½æ¸¬è©¦...")
    
    # æº–å‚™æ¸¬è©¦åœ–ç‰‡
    test_images = [
        Image.open(f'test_{i}.jpg') 
        for i in range(num_queries)
    ]
    
    # æ¸¬è©¦æœå°‹é€Ÿåº¦
    start_time = time.time()
    
    for img in test_images:
        results = engine.search_by_image(img, top_k=1)
    
    end_time = time.time()
    
    # è¨ˆç®—çµ±è¨ˆ
    total_time = end_time - start_time
    avg_time = total_time / num_queries
    qps = num_queries / total_time
    
    print(f"\nğŸ“Š æ•ˆèƒ½æ¸¬è©¦çµæœï¼š")
    print(f"   ç¸½æ™‚é–“: {total_time:.2f} ç§’")
    print(f"   å¹³å‡å»¶é²: {avg_time*1000:.2f} ms")
    print(f"   QPS (æŸ¥è©¢/ç§’): {qps:.1f}")
    
    return {
        'total_time': total_time,
        'avg_latency': avg_time,
        'qps': qps
    }

# é‹è¡Œæ¸¬è©¦
benchmark_search_speed(engine, num_queries=100)
</code></pre>
<hr />
<h2 id="é™„éŒ„"><a class="header" href="#é™„éŒ„">é™„éŒ„</a></h2>
<h3 id="a-å¸¸è¦‹å•é¡Œ-faq"><a class="header" href="#a-å¸¸è¦‹å•é¡Œ-faq">A. å¸¸è¦‹å•é¡Œ FAQ</a></h3>
<p><strong>Q1: ç‚ºä»€éº¼æˆ‘çš„æœå°‹çµæœä¸æº–ç¢ºï¼Ÿ</strong></p>
<ul>
<li>æª¢æŸ¥åœ–ç‰‡å“è³ªï¼ˆæ¸…æ™°åº¦ã€å…‰ç·šï¼‰</li>
<li>ç¢ºèªé–¾å€¼è¨­å®šæ˜¯å¦åˆç†</li>
<li>è€ƒæ…®ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ B-LTï¼‰</li>
<li>å˜—è©¦æ··åˆæœå°‹ï¼ˆåœ–ç‰‡+æ–‡å­—ï¼‰</li>
</ul>
<p><strong>Q2: å¦‚ä½•è™•ç†å¤šèªè¨€æ–‡å­—æœå°‹ï¼Ÿ</strong></p>
<ul>
<li>MobileCLIP æ”¯æ´å¤šèªè¨€ï¼ˆåŒ…å«ä¸­æ–‡ï¼‰</li>
<li>ç›´æ¥ä½¿ç”¨ä¸­æ–‡æè¿°å³å¯</li>
<li>æ•ˆæœå¯èƒ½ç•¥éœæ–¼è‹±æ–‡</li>
</ul>
<p><strong>Q3: æ¨¡å‹å¯ä»¥å¾®èª¿ï¼ˆfine-tuneï¼‰å—ï¼Ÿ</strong></p>
<ul>
<li>å¯ä»¥ï¼Œä½†éœ€è¦å¤§é‡è³‡æ–™å’Œé‹ç®—è³‡æº</li>
<li>å»ºè­°å…ˆç”¨é è¨“ç·´æ¨¡å‹æ¸¬è©¦</li>
<li>å¦‚éœ€å¾®èª¿ï¼Œåƒè€ƒ OpenCLIP çš„è¨“ç·´è…³æœ¬</li>
</ul>
<p><strong>Q4: èƒ½åœ¨æ‰‹æ©Ÿä¸Šé‹è¡Œå—ï¼Ÿ</strong></p>
<ul>
<li>iOSï¼šéœ€è½‰æ›ç‚º Core ML æ ¼å¼</li>
<li>Androidï¼šéœ€è½‰æ›ç‚º TensorFlow Lite</li>
<li>é æœŸå»¶é²ï¼š3-15msï¼ˆæ ¹æ“šæ¨¡å‹å¤§å°ï¼‰</li>
</ul>
<p><strong>Q5: è³‡æ–™åº«æœ‰å¹¾è¬å€‹å•†å“æœƒå¤ªæ…¢å—ï¼Ÿ</strong></p>
<ul>
<li>é è¨ˆç®—å‘é‡å¾Œï¼Œ10è¬å•†å“æœå°‹ &lt; 1ç§’</li>
<li>å¯ä½¿ç”¨å‘é‡è³‡æ–™åº«ï¼ˆå¦‚ Faissï¼‰é€²ä¸€æ­¥åŠ é€Ÿ</li>
<li>è€ƒæ…®ä½¿ç”¨ GPU åŠ é€Ÿ</li>
</ul>
<h3 id="b-åƒè€ƒè³‡æº"><a class="header" href="#b-åƒè€ƒè³‡æº">B. åƒè€ƒè³‡æº</a></h3>
<p><strong>å®˜æ–¹è³‡æº</strong>ï¼š</p>
<ul>
<li>GitHub: https://github.com/apple/ml-mobileclip</li>
<li>è«–æ–‡: https://arxiv.org/abs/2311.17049</li>
<li>HuggingFace: https://huggingface.co/apple/MobileCLIP-S2</li>
</ul>
<p><strong>ç›¸é—œå·¥å…·</strong>ï¼š</p>
<ul>
<li>OpenCLIP: https://github.com/mlfoundations/open_clip</li>
<li>Core ML Tools: https://coremltools.readme.io</li>
<li>PyTorch: https://pytorch.org</li>
</ul>
<p><strong>å­¸ç¿’è³‡æº</strong>ï¼š</p>
<ul>
<li>CLIP åŸç†è§£èªª</li>
<li>å‘é‡æœå°‹æœ€ä½³å¯¦è¸</li>
<li>iOS Core ML é–‹ç™¼æŒ‡å—</li>
</ul>
<h3 id="c-æ¨¡å‹è¦æ ¼å°ç…§è¡¨"><a class="header" href="#c-æ¨¡å‹è¦æ ¼å°ç…§è¡¨">C. æ¨¡å‹è¦æ ¼å°ç…§è¡¨</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹</th><th>åœ–ç‰‡ç·¨ç¢¼å™¨</th><th>æ–‡å­—ç·¨ç¢¼å™¨</th><th>ç¸½åƒæ•¸</th><th>åœ–ç‰‡å»¶é²</th><th>æ–‡å­—å»¶é²</th><th>Top-1 æº–ç¢ºç‡</th></tr></thead><tbody>
<tr><td>S0</td><td>11.4M</td><td>42.4M</td><td>53.8M</td><td>1.5ms</td><td>1.6ms</td><td>67.8%</td></tr>
<tr><td>S1</td><td>21.5M</td><td>42.4M</td><td>63.9M</td><td>2.5ms</td><td>1.9ms</td><td>70.7%</td></tr>
<tr><td>S2</td><td>35.7M</td><td>42.4M</td><td>78.1M</td><td>3.6ms</td><td>2.1ms</td><td>73.4%</td></tr>
<tr><td>B</td><td>86.3M</td><td>42.4M</td><td>128.7M</td><td>10.4ms</td><td>3.3ms</td><td>76.8%</td></tr>
<tr><td>B-LT</td><td>86.3M</td><td>42.4M</td><td>128.7M</td><td>10.4ms</td><td>3.3ms</td><td>77.2%</td></tr>
</tbody></table>
</div>
<p><em>å»¶é²æ¸¬è©¦å¹³è‡ºï¼šiPhone 12 Pro Max</em></p>
<hr />
<h2 id="çµèª"><a class="header" href="#çµèª">çµèª</a></h2>
<p>MobileCLIP æ˜¯ä¸€å€‹å¼·å¤§çš„å·¥å…·ï¼Œç‰¹åˆ¥é©åˆï¼š</p>
<ul>
<li>âœ… éœ€è¦é›¢ç·šåŸ·è¡Œçš„æ‡‰ç”¨</li>
<li>âœ… å°å»¶é²æ•æ„Ÿçš„å ´æ™¯</li>
<li>âœ… è¡Œå‹•è£ç½®éƒ¨ç½²</li>
<li>âœ… é›¶æ¨£æœ¬å­¸ç¿’éœ€æ±‚</li>
</ul>
<p><strong>é–‹å§‹ä½¿ç”¨å»ºè­°</strong>ï¼š</p>
<ol>
<li>å…ˆç”¨ S2 æ¨¡å‹å¿«é€ŸåŸå‹</li>
<li>æ¸¬è©¦çœŸå¯¦å ´æ™¯æ•ˆæœ</li>
<li>æ ¹æ“šéœ€æ±‚é¸æ“‡æœ€çµ‚æ¨¡å‹</li>
<li>å„ªåŒ–è³‡æ–™åº«å’Œæœå°‹æµç¨‹</li>
</ol>
<p>ç¥ä½ é–‹ç™¼é †åˆ©ï¼ğŸš€</p>
<hr />
<p><strong>æ–‡ä»¶ç‰ˆæœ¬</strong>ï¼šv1.0<br />
<strong>ä½œè€…</strong>ï¼šClaude<br />
<strong>æœ€å¾Œæ›´æ–°</strong>ï¼š2025-10-27</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../cv/02_MobileCLIPèˆ‡æ·±åº¦å­¸ç¿’.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../cv/mobileclip-complete-guide.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../cv/02_MobileCLIPèˆ‡æ·±åº¦å­¸ç¿’.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../cv/mobileclip-complete-guide.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
