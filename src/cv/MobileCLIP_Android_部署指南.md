# MobileCLIP Android å®Œæ•´éƒ¨ç½²æŒ‡å—

> **æ›´æ–°æ—¥æœŸ**ï¼š2025-10-27  
> **é›£åº¦ç­‰ç´š**ï¼šä¸­ç­‰  
> **é è¨ˆæ™‚é–“**ï¼š2-3 å°æ™‚

---

## ğŸ“‹ ç›®éŒ„

1. [æ–¹æ¡ˆé¸æ“‡](#1-æ–¹æ¡ˆé¸æ“‡)
2. [ç’°å¢ƒæº–å‚™](#2-ç’°å¢ƒæº–å‚™)
3. [æ–¹æ¡ˆ Aï¼šONNX Runtimeï¼ˆæ¨è–¦ï¼‰](#3-æ–¹æ¡ˆ-a-onnx-runtimeæ¨è–¦)
4. [æ–¹æ¡ˆ Bï¼šTensorFlow Lite](#4-æ–¹æ¡ˆ-b-tensorflow-lite)
5. [Android å°ˆæ¡ˆå¯¦ä½œ](#5-android-å°ˆæ¡ˆå¯¦ä½œ)
6. [å®Œæ•´å•†å“æœå°‹ App](#6-å®Œæ•´å•†å“æœå°‹-app)
7. [æ•ˆèƒ½å„ªåŒ–](#7-æ•ˆèƒ½å„ªåŒ–)
8. [å¸¸è¦‹å•é¡Œ](#8-å¸¸è¦‹å•é¡Œ)

---

## 1. æ–¹æ¡ˆé¸æ“‡

### å…©ç¨®éƒ¨ç½²æ–¹æ¡ˆå°æ¯”

| ç‰¹æ€§ | ONNX Runtime | TensorFlow Lite |
|------|-------------|-----------------|
| **è½‰æ›è¤‡é›œåº¦** | â­â­ (ç°¡å–®) | â­â­â­â­ (è¤‡é›œ) |
| **æ•ˆèƒ½** | â­â­â­â­ (å„ª) | â­â­â­â­â­ (æœ€å„ª) |
| **APK é«”ç©** | +8-15 MB | +3-8 MB |
| **ç¡¬é«”åŠ é€Ÿ** | NNAPI, CPU | NNAPI, GPU, EdgeTPU |
| **ç¶­è­·é›£åº¦** | ä½ | ä¸­ |
| **æ¨è–¦åº¦** | âœ… **æ¨è–¦æ–°æ‰‹** | é€²éšç”¨æˆ¶ |

### æ¨è–¦æ–¹æ¡ˆ

```
ã€æ¨è–¦ã€‘ONNX Runtime
ç†ç”±ï¼š
âœ… è½‰æ›æ­¥é©Ÿå°‘ï¼ˆPyTorch â†’ ONNXï¼Œä¸€æ­¥åˆ°ä½ï¼‰
âœ… ç¨‹å¼ç¢¼ç°¡å–®
âœ… æ•ˆèƒ½è¶³å¤ å¥½
âœ… ç¶­è­·å®¹æ˜“

ã€é€²éšã€‘TensorFlow Lite
é©åˆï¼š
- éœ€è¦æ¥µè‡´æ•ˆèƒ½
- å·²æœ‰ TFLite ç¶“é©—
- é¡˜æ„èŠ±æ™‚é–“èª¿è©¦
```

---

## 2. ç’°å¢ƒæº–å‚™

### 2.1 é–‹ç™¼ç’°å¢ƒéœ€æ±‚

**é›»è…¦ç«¯ï¼ˆæ¨¡å‹è½‰æ›ï¼‰**ï¼š
```bash
# ä½œæ¥­ç³»çµ±
Windows 10/11, macOS, Linux

# Python ç’°å¢ƒ
Python 3.8-3.10

# å¿…è¦å·¥å…·
- Git
- Python pip
- Conda (æ¨è–¦)
```

**Android é–‹ç™¼ç«¯**ï¼š
```bash
# Android Studio
Android Studio Hedgehog | 2023.1.1 æˆ–æ›´æ–°ç‰ˆæœ¬

# SDK ç‰ˆæœ¬
- Min SDK: 24 (Android 7.0)
- Target SDK: 34 (Android 14)
- Compile SDK: 34

# NDK (ONNX Runtime éœ€è¦)
NDK ç‰ˆæœ¬ 25.x æˆ–æ›´æ–°
```

### 2.2 Python ç’°å¢ƒè¨­ç½®

```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
conda create -n mobileclip-android python=3.10
conda activate mobileclip-android

# å®‰è£åŸºç¤å¥—ä»¶
pip install torch torchvision
pip install onnx onnxruntime
pip install pillow numpy

# å®‰è£ MobileCLIP
git clone https://github.com/apple/ml-mobileclip.git
cd ml-mobileclip
pip install -e .

# é©—è­‰å®‰è£
python -c "import mobileclip; print('âœ… MobileCLIP å®‰è£æˆåŠŸ')"
```

---

## 3. æ–¹æ¡ˆ A: ONNX Runtimeï¼ˆæ¨è–¦ï¼‰

### 3.1 æ¨¡å‹è½‰æ›ç‚º ONNX

#### æ­¥é©Ÿ 1ï¼šè½‰æ›è…³æœ¬

å‰µå»º `convert_to_onnx.py`ï¼š

```python
#!/usr/bin/env python3
"""
MobileCLIP PyTorch â†’ ONNX è½‰æ›è…³æœ¬
"""

import torch
import mobileclip
import onnx
from onnx import version_converter

def convert_image_encoder(model_name='mobileclip_s2', 
                         model_path='checkpoints/mobileclip_s2.pt',
                         output_path='mobileclip_image_encoder.onnx'):
    """
    è½‰æ›åœ–ç‰‡ç·¨ç¢¼å™¨ç‚º ONNX
    
    Args:
        model_name: æ¨¡å‹åç¨±
        model_path: PyTorch æ¨¡å‹è·¯å¾‘
        output_path: è¼¸å‡º ONNX æª”æ¡ˆè·¯å¾‘
    """
    print(f"ğŸ”„ é–‹å§‹è½‰æ› {model_name} åœ–ç‰‡ç·¨ç¢¼å™¨...")
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name,
        pretrained=model_path
    )
    model.eval()
    
    # ç²å–åœ–ç‰‡ç·¨ç¢¼å™¨
    image_encoder = model.visual
    
    # å‰µå»ºç¤ºä¾‹è¼¸å…¥ï¼ˆbatch_size=1, channels=3, height=224, width=224ï¼‰
    dummy_input = torch.randn(1, 3, 224, 224)
    
    # å°å‡ºç‚º ONNX
    torch.onnx.export(
        image_encoder,
        dummy_input,
        output_path,
        input_names=['image'],
        output_names=['image_features'],
        dynamic_axes={
            'image': {0: 'batch_size'},
            'image_features': {0: 'batch_size'}
        },
        opset_version=14,  # ONNX Runtime æ”¯æ´è‰¯å¥½çš„ç‰ˆæœ¬
        do_constant_folding=True,
        export_params=True
    )
    
    # é©—è­‰æ¨¡å‹
    onnx_model = onnx.load(output_path)
    onnx.checker.check_model(onnx_model)
    
    print(f"âœ… åœ–ç‰‡ç·¨ç¢¼å™¨è½‰æ›å®Œæˆï¼š{output_path}")
    print(f"   æ¨¡å‹å¤§å°ï¼š{os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
    
    return output_path


def convert_text_encoder(model_name='mobileclip_s2',
                        model_path='checkpoints/mobileclip_s2.pt',
                        output_path='mobileclip_text_encoder.onnx'):
    """
    è½‰æ›æ–‡å­—ç·¨ç¢¼å™¨ç‚º ONNX
    
    Args:
        model_name: æ¨¡å‹åç¨±
        model_path: PyTorch æ¨¡å‹è·¯å¾‘
        output_path: è¼¸å‡º ONNX æª”æ¡ˆè·¯å¾‘
    """
    print(f"ğŸ”„ é–‹å§‹è½‰æ› {model_name} æ–‡å­—ç·¨ç¢¼å™¨...")
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name,
        pretrained=model_path
    )
    model.eval()
    tokenizer = mobileclip.get_tokenizer(model_name)
    
    # ç²å–æ–‡å­—ç·¨ç¢¼å™¨
    text_encoder = model.text
    
    # å‰µå»ºç¤ºä¾‹è¼¸å…¥ï¼ˆæ–‡å­— tokenï¼‰
    dummy_text = tokenizer(["a photo of a cat"])
    
    # å°å‡ºç‚º ONNX
    torch.onnx.export(
        text_encoder,
        dummy_text,
        output_path,
        input_names=['text'],
        output_names=['text_features'],
        dynamic_axes={
            'text': {0: 'batch_size'},
            'text_features': {0: 'batch_size'}
        },
        opset_version=14,
        do_constant_folding=True,
        export_params=True
    )
    
    # é©—è­‰æ¨¡å‹
    onnx_model = onnx.load(output_path)
    onnx.checker.check_model(onnx_model)
    
    print(f"âœ… æ–‡å­—ç·¨ç¢¼å™¨è½‰æ›å®Œæˆï¼š{output_path}")
    print(f"   æ¨¡å‹å¤§å°ï¼š{os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
    
    return output_path


def optimize_onnx_model(input_path, output_path):
    """
    å„ªåŒ– ONNX æ¨¡å‹ï¼ˆæ¸›å°é«”ç©ã€æå‡é€Ÿåº¦ï¼‰
    
    Args:
        input_path: è¼¸å…¥ ONNX æ¨¡å‹
        output_path: å„ªåŒ–å¾Œçš„è¼¸å‡ºè·¯å¾‘
    """
    print(f"âš¡ å„ªåŒ–æ¨¡å‹ï¼š{input_path}")
    
    from onnxruntime.quantization import quantize_dynamic, QuantType
    
    # å‹•æ…‹é‡åŒ–ï¼ˆFP32 â†’ INT8ï¼‰
    quantize_dynamic(
        input_path,
        output_path,
        weight_type=QuantType.QUInt8  # 8-bit é‡åŒ–
    )
    
    original_size = os.path.getsize(input_path) / 1024 / 1024
    optimized_size = os.path.getsize(output_path) / 1024 / 1024
    
    print(f"âœ… å„ªåŒ–å®Œæˆ")
    print(f"   åŸå§‹å¤§å°ï¼š{original_size:.2f} MB")
    print(f"   å„ªåŒ–å¾Œï¼š{optimized_size:.2f} MB")
    print(f"   å£“ç¸®ç‡ï¼š{(1 - optimized_size/original_size)*100:.1f}%")


def verify_conversion(onnx_path, pytorch_model):
    """
    é©—è­‰è½‰æ›å¾Œçš„ ONNX æ¨¡å‹è¼¸å‡ºæ˜¯å¦æ­£ç¢º
    
    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾‘
        pytorch_model: PyTorch æ¨¡å‹
    """
    print("ğŸ” é©—è­‰æ¨¡å‹è¼¸å‡º...")
    
    import onnxruntime as ort
    
    # å‰µå»ºæ¸¬è©¦è¼¸å…¥
    test_input = torch.randn(1, 3, 224, 224)
    
    # PyTorch æ¨è«–
    with torch.no_grad():
        pytorch_output = pytorch_model.visual(test_input).numpy()
    
    # ONNX æ¨è«–
    session = ort.InferenceSession(onnx_path)
    onnx_output = session.run(
        None,
        {'image': test_input.numpy()}
    )[0]
    
    # æ¯”è¼ƒè¼¸å‡º
    diff = np.abs(pytorch_output - onnx_output).max()
    print(f"âœ… æœ€å¤§è¼¸å‡ºå·®ç•°ï¼š{diff:.6f}")
    
    if diff < 1e-5:
        print("âœ… è½‰æ›æ­£ç¢ºï¼")
    else:
        print("âš ï¸  è¼¸å‡ºæœ‰å·®ç•°ï¼Œè«‹æª¢æŸ¥")


if __name__ == '__main__':
    import os
    import sys
    import numpy as np
    
    # é…ç½®
    MODEL_NAME = 'mobileclip_s2'  # å¯æ”¹ç‚º s0, s1, b, blt
    MODEL_PATH = 'checkpoints/mobileclip_s2.pt'
    OUTPUT_DIR = 'android_models'
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("=" * 50)
    print("ğŸš€ MobileCLIP â†’ ONNX è½‰æ›å·¥å…·")
    print("=" * 50)
    
    # 1. è½‰æ›åœ–ç‰‡ç·¨ç¢¼å™¨
    image_onnx = convert_image_encoder(
        model_name=MODEL_NAME,
        model_path=MODEL_PATH,
        output_path=f'{OUTPUT_DIR}/mobileclip_image.onnx'
    )
    
    # 2. è½‰æ›æ–‡å­—ç·¨ç¢¼å™¨
    text_onnx = convert_text_encoder(
        model_name=MODEL_NAME,
        model_path=MODEL_PATH,
        output_path=f'{OUTPUT_DIR}/mobileclip_text.onnx'
    )
    
    # 3. å„ªåŒ–æ¨¡å‹ï¼ˆå¯é¸ï¼Œä½†æ¨è–¦ï¼‰
    print("\n" + "=" * 50)
    print("âš¡ é–‹å§‹å„ªåŒ–æ¨¡å‹...")
    print("=" * 50)
    
    optimize_onnx_model(
        image_onnx,
        f'{OUTPUT_DIR}/mobileclip_image_quantized.onnx'
    )
    
    optimize_onnx_model(
        text_onnx,
        f'{OUTPUT_DIR}/mobileclip_text_quantized.onnx'
    )
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰è½‰æ›å®Œæˆï¼")
    print("=" * 50)
    print(f"\nğŸ“ æ¨¡å‹æª”æ¡ˆä½ç½®ï¼š{os.path.abspath(OUTPUT_DIR)}")
    print("\nå¯ç”¨æ¨¡å‹ï¼š")
    print("  1. mobileclip_image.onnx (åŸå§‹)")
    print("  2. mobileclip_image_quantized.onnx (å„ªåŒ–ï¼Œæ¨è–¦)")
    print("  3. mobileclip_text.onnx (åŸå§‹)")
    print("  4. mobileclip_text_quantized.onnx (å„ªåŒ–ï¼Œæ¨è–¦)")
    print("\nğŸ’¡ å»ºè­°ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬ä»¥æ¸›å°‘ APK å¤§å°")
```

#### æ­¥é©Ÿ 2ï¼šåŸ·è¡Œè½‰æ›

```bash
# ç¢ºä¿å·²ä¸‹è¼‰æ¨¡å‹
cd ml-mobileclip
source get_pretrained_models.sh

# åŸ·è¡Œè½‰æ›
python convert_to_onnx.py

# è¼¸å‡ºçµæœ
# âœ… android_models/
#    â”œâ”€â”€ mobileclip_image.onnx          (35MB)
#    â”œâ”€â”€ mobileclip_image_quantized.onnx (9MB) ğŸ‘ˆ æ¨è–¦
#    â”œâ”€â”€ mobileclip_text.onnx           (42MB)
#    â””â”€â”€ mobileclip_text_quantized.onnx (11MB) ğŸ‘ˆ æ¨è–¦
```

### 3.2 Android å°ˆæ¡ˆè¨­ç½®

#### æ­¥é©Ÿ 1ï¼šå‰µå»º Android å°ˆæ¡ˆ

```
Android Studio > New Project > Empty Views Activity

å°ˆæ¡ˆè¨­å®šï¼š
- Name: MobileCLIPDemo
- Package: com.example.mobileclip
- Language: Kotlin
- Minimum SDK: API 24 (Android 7.0)
```

#### æ­¥é©Ÿ 2ï¼šæ·»åŠ ä¾è³´

`app/build.gradle.kts`ï¼š

```kotlin
plugins {
    id("com.android.application")
    id("org.jetbrains.kotlin.android")
}

android {
    namespace = "com.example.mobileclip"
    compileSdk = 34
    
    defaultConfig {
        applicationId = "com.example.mobileclip"
        minSdk = 24
        targetSdk = 34
        versionCode = 1
        versionName = "1.0"
        
        // ONNX Runtime éœ€è¦
        ndk {
            abiFilters += listOf("arm64-v8a", "armeabi-v7a")
        }
    }
    
    buildTypes {
        release {
            isMinifyEnabled = true
            proguardFiles(
                getDefaultProguardFile("proguard-android-optimize.txt"),
                "proguard-rules.pro"
            )
        }
    }
    
    compileOptions {
        sourceCompatibility = JavaVersion.VERSION_1_8
        targetCompatibility = JavaVersion.VERSION_1_8
    }
    
    kotlinOptions {
        jvmTarget = "1.8"
    }
    
    buildFeatures {
        viewBinding = true
    }
}

dependencies {
    // ONNX Runtime
    implementation("com.microsoft.onnxruntime:onnxruntime-android:1.17.0")
    
    // åœ–ç‰‡è™•ç†
    implementation("androidx.camera:camera-camera2:1.3.1")
    implementation("androidx.camera:camera-lifecycle:1.3.1")
    implementation("androidx.camera:camera-view:1.3.1")
    
    // Coroutines
    implementation("org.jetbrains.kotlinx:kotlinx-coroutines-android:1.7.3")
    
    // ViewModel
    implementation("androidx.lifecycle:lifecycle-viewmodel-ktx:2.7.0")
    implementation("androidx.lifecycle:lifecycle-runtime-ktx:2.7.0")
    
    // UI
    implementation("androidx.core:core-ktx:1.12.0")
    implementation("androidx.appcompat:appcompat:1.6.1")
    implementation("com.google.android.material:material:1.11.0")
    implementation("androidx.constraintlayout:constraintlayout:2.1.4")
}
```

#### æ­¥é©Ÿ 3ï¼šæ”¾ç½®æ¨¡å‹æª”æ¡ˆ

```
1. åœ¨ Android Studio ä¸­ï¼š
   å³éµé»æ“Š app > New > Folder > Assets Folder

2. å°‡ ONNX æ¨¡å‹è¤‡è£½åˆ° app/src/main/assets/
   app/src/main/assets/
   â”œâ”€â”€ mobileclip_image_quantized.onnx
   â””â”€â”€ mobileclip_text_quantized.onnx
```

### 3.3 æ ¸å¿ƒæ¨è«–ç¨‹å¼ç¢¼

å‰µå»º `MobileCLIPInference.kt`ï¼š

```kotlin
package com.example.mobileclip

import android.content.Context
import android.graphics.Bitmap
import ai.onnxruntime.*
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.FloatBuffer

/**
 * MobileCLIP æ¨è«–å¼•æ“
 */
class MobileCLIPInference(private val context: Context) {
    
    private var imageSession: OrtSession? = null
    private var textSession: OrtSession? = null
    private val ortEnvironment = OrtEnvironment.getEnvironment()
    
    companion object {
        private const val IMAGE_MODEL = "mobileclip_image_quantized.onnx"
        private const val TEXT_MODEL = "mobileclip_text_quantized.onnx"
        
        // åœ–ç‰‡é è™•ç†åƒæ•¸
        private const val INPUT_SIZE = 224
        private val MEAN = floatArrayOf(0.48145466f, 0.4578275f, 0.40821073f)
        private val STD = floatArrayOf(0.26862954f, 0.26130258f, 0.27577711f)
        
        // ç‰¹å¾µç¶­åº¦
        private const val FEATURE_DIM = 512
    }
    
    /**
     * åˆå§‹åŒ–æ¨¡å‹
     */
    suspend fun initialize() = withContext(Dispatchers.IO) {
        try {
            // è¼‰å…¥åœ–ç‰‡ç·¨ç¢¼å™¨
            val imageModelBytes = context.assets.open(IMAGE_MODEL).readBytes()
            imageSession = ortEnvironment.createSession(imageModelBytes)
            
            // è¼‰å…¥æ–‡å­—ç·¨ç¢¼å™¨
            val textModelBytes = context.assets.open(TEXT_MODEL).readBytes()
            textSession = ortEnvironment.createSession(textModelBytes)
            
            println("âœ… MobileCLIP æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        } catch (e: Exception) {
            println("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: ${e.message}")
            throw e
        }
    }
    
    /**
     * ç·¨ç¢¼åœ–ç‰‡ç‚ºå‘é‡
     */
    suspend fun encodeImage(bitmap: Bitmap): FloatArray = withContext(Dispatchers.Default) {
        requireNotNull(imageSession) { "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« initialize()" }
        
        // 1. é è™•ç†åœ–ç‰‡
        val preprocessed = preprocessImage(bitmap)
        
        // 2. å‰µå»º ONNX è¼¸å…¥
        val inputTensor = OnnxTensor.createTensor(
            ortEnvironment,
            FloatBuffer.wrap(preprocessed),
            longArrayOf(1, 3, INPUT_SIZE.toLong(), INPUT_SIZE.toLong())
        )
        
        // 3. åŸ·è¡Œæ¨è«–
        val inputs = mapOf("image" to inputTensor)
        val outputs = imageSession!!.run(inputs)
        
        // 4. ç²å–è¼¸å‡º
        val output = outputs[0].value as Array<FloatArray>
        val features = output[0]
        
        // 5. L2 æ­£è¦åŒ–
        normalizeFeatures(features)
        
        // 6. æ¸…ç†
        inputTensor.close()
        outputs.close()
        
        return@withContext features
    }
    
    /**
     * ç·¨ç¢¼æ–‡å­—ç‚ºå‘é‡
     */
    suspend fun encodeText(text: String): FloatArray = withContext(Dispatchers.Default) {
        requireNotNull(textSession) { "æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« initialize()" }
        
        // 1. Tokenize æ–‡å­—
        val tokens = tokenize(text)
        
        // 2. å‰µå»º ONNX è¼¸å…¥
        val inputTensor = OnnxTensor.createTensor(
            ortEnvironment,
            tokens,
            longArrayOf(1, tokens.size.toLong())
        )
        
        // 3. åŸ·è¡Œæ¨è«–
        val inputs = mapOf("text" to inputTensor)
        val outputs = textSession!!.run(inputs)
        
        // 4. ç²å–è¼¸å‡º
        val output = outputs[0].value as Array<FloatArray>
        val features = output[0]
        
        // 5. L2 æ­£è¦åŒ–
        normalizeFeatures(features)
        
        // 6. æ¸…ç†
        inputTensor.close()
        outputs.close()
        
        return@withContext features
    }
    
    /**
     * è¨ˆç®—å…©å€‹å‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦
     */
    fun calculateSimilarity(features1: FloatArray, features2: FloatArray): Float {
        require(features1.size == features2.size) { "ç‰¹å¾µç¶­åº¦ä¸åŒ¹é…" }
        
        var dotProduct = 0f
        for (i in features1.indices) {
            dotProduct += features1[i] * features2[i]
        }
        
        return dotProduct
    }
    
    /**
     * åœ–ç‰‡é è™•ç†
     * æ­¥é©Ÿï¼šResize â†’ Center Crop â†’ Normalize
     */
    private fun preprocessImage(bitmap: Bitmap): FloatArray {
        // 1. Resize åˆ° 256x256
        val resized = Bitmap.createScaledBitmap(bitmap, 256, 256, true)
        
        // 2. Center Crop åˆ° 224x224
        val startX = (256 - INPUT_SIZE) / 2
        val startY = (256 - INPUT_SIZE) / 2
        val cropped = Bitmap.createBitmap(resized, startX, startY, INPUT_SIZE, INPUT_SIZE)
        
        // 3. è½‰æ›ç‚º Float Array (CHW format)
        val floatArray = FloatArray(3 * INPUT_SIZE * INPUT_SIZE)
        val pixels = IntArray(INPUT_SIZE * INPUT_SIZE)
        cropped.getPixels(pixels, 0, INPUT_SIZE, 0, 0, INPUT_SIZE, INPUT_SIZE)
        
        for (i in pixels.indices) {
            val pixel = pixels[i]
            
            // RGB å€¼ (0-255)
            val r = ((pixel shr 16) and 0xFF) / 255f
            val g = ((pixel shr 8) and 0xFF) / 255f
            val b = (pixel and 0xFF) / 255f
            
            // æ¨™æº–åŒ–ä¸¦è½‰ç‚º CHW æ ¼å¼
            floatArray[i] = (r - MEAN[0]) / STD[0]  // R channel
            floatArray[INPUT_SIZE * INPUT_SIZE + i] = (g - MEAN[1]) / STD[1]  // G channel
            floatArray[2 * INPUT_SIZE * INPUT_SIZE + i] = (b - MEAN[2]) / STD[2]  // B channel
        }
        
        return floatArray
    }
    
    /**
     * æ–‡å­— Tokenization
     * ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²ä½¿ç”¨å®Œæ•´çš„ CLIP tokenizer
     */
    private fun tokenize(text: String): IntArray {
        // é€™æ˜¯ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›éƒ¨ç½²æ™‚éœ€è¦ä½¿ç”¨å®Œæ•´çš„ tokenizer
        // å¯ä»¥è€ƒæ…®ä½¿ç”¨ Hugging Face tokenizers æˆ–è‡ªå·±å¯¦ä½œ
        
        // æš«æ™‚è¿”å›å›ºå®šé•·åº¦çš„ token array
        val tokens = IntArray(77) { 0 }  // CLIP çš„ context length æ˜¯ 77
        
        // TODO: å¯¦ä½œå®Œæ•´çš„ tokenization
        // 1. è½‰å°å¯«
        // 2. åˆ†è©
        // 3. è½‰ç‚º token ID
        // 4. Padding åˆ° 77
        
        return tokens
    }
    
    /**
     * L2 æ­£è¦åŒ–
     */
    private fun normalizeFeatures(features: FloatArray) {
        var norm = 0f
        for (value in features) {
            norm += value * value
        }
        norm = kotlin.math.sqrt(norm)
        
        for (i in features.indices) {
            features[i] /= norm
        }
    }
    
    /**
     * é‡‹æ”¾è³‡æº
     */
    fun close() {
        imageSession?.close()
        textSession?.close()
    }
}
```

### 3.4 ç°¡å–®æ¸¬è©¦ Activity

å‰µå»º `MainActivity.kt`ï¼š

```kotlin
package com.example.mobileclip

import android.graphics.BitmapFactory
import android.os.Bundle
import android.widget.Button
import android.widget.ImageView
import android.widget.TextView
import androidx.appcompat.app.AppCompatActivity
import androidx.lifecycle.lifecycleScope
import kotlinx.coroutines.launch

class MainActivity : AppCompatActivity() {
    
    private lateinit var mobileCLIP: MobileCLIPInference
    private lateinit var imageView: ImageView
    private lateinit var resultText: TextView
    private lateinit var searchButton: Button
    
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)
        
        imageView = findViewById(R.id.imageView)
        resultText = findViewById(R.id.resultText)
        searchButton = findViewById(R.id.searchButton)
        
        // åˆå§‹åŒ– MobileCLIP
        mobileCLIP = MobileCLIPInference(this)
        
        lifecycleScope.launch {
            try {
                resultText.text = "è¼‰å…¥æ¨¡å‹ä¸­..."
                mobileCLIP.initialize()
                resultText.text = "âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼"
                searchButton.isEnabled = true
            } catch (e: Exception) {
                resultText.text = "âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š${e.message}"
            }
        }
        
        // æ¸¬è©¦æŒ‰éˆ•
        searchButton.setOnClickListener {
            testImageSearch()
        }
    }
    
    private fun testImageSearch() {
        lifecycleScope.launch {
            try {
                resultText.text = "è™•ç†ä¸­..."
                
                // 1. è¼‰å…¥æ¸¬è©¦åœ–ç‰‡ï¼ˆå¾ assets æˆ–ç›¸æ©Ÿï¼‰
                val bitmap = BitmapFactory.decodeResource(resources, R.drawable.test_image)
                imageView.setImageBitmap(bitmap)
                
                // 2. ç·¨ç¢¼åœ–ç‰‡
                val imageFeatures = mobileCLIP.encodeImage(bitmap)
                
                // 3. æº–å‚™å€™é¸æ–‡å­—
                val candidates = listOf(
                    "ä¸€éš»è²“",
                    "ä¸€éš»ç‹—",
                    "ä¸€è¼›è»Š",
                    "ä¸€è‡ºé›»è…¦",
                    "ä¸€æ”¯æ‰‹æ©Ÿ"
                )
                
                // 4. è¨ˆç®—ç›¸ä¼¼åº¦
                val results = mutableListOf<Pair<String, Float>>()
                for (text in candidates) {
                    val textFeatures = mobileCLIP.encodeText(text)
                    val similarity = mobileCLIP.calculateSimilarity(
                        imageFeatures,
                        textFeatures
                    )
                    results.add(text to similarity)
                }
                
                // 5. æ’åºä¸¦é¡¯ç¤ºçµæœ
                results.sortByDescending { it.second }
                val resultString = results.joinToString("\n") { (text, score) ->
                    "$text: ${(score * 100).toInt()}%"
                }
                
                resultText.text = "æœå°‹çµæœï¼š\n$resultString"
                
            } catch (e: Exception) {
                resultText.text = "âŒ è™•ç†å¤±æ•—ï¼š${e.message}"
            }
        }
    }
    
    override fun onDestroy() {
        super.onDestroy()
        mobileCLIP.close()
    }
}
```

### 3.5 ä½ˆå±€æª”æ¡ˆ

`res/layout/activity_main.xml`ï¼š

```xml
<?xml version="1.0" encoding="utf-8"?>
<androidx.constraintlayout.widget.ConstraintLayout 
    xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:app="http://schemas.android.com/apk/res-auto"
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:padding="16dp">
    
    <ImageView
        android:id="@+id/imageView"
        android:layout_width="300dp"
        android:layout_height="300dp"
        android:scaleType="centerCrop"
        android:background="@color/material_grey_300"
        app:layout_constraintTop_toTopOf="parent"
        app:layout_constraintStart_toStartOf="parent"
        app:layout_constraintEnd_toEndOf="parent"
        android:contentDescription="æ¸¬è©¦åœ–ç‰‡" />
    
    <Button
        android:id="@+id/searchButton"
        android:layout_width="0dp"
        android:layout_height="wrap_content"
        android:text="é–‹å§‹æœå°‹"
        android:enabled="false"
        app:layout_constraintTop_toBottomOf="@id/imageView"
        app:layout_constraintStart_toStartOf="parent"
        app:layout_constraintEnd_toEndOf="parent"
        android:layout_marginTop="16dp" />
    
    <TextView
        android:id="@+id/resultText"
        android:layout_width="0dp"
        android:layout_height="0dp"
        android:text="åˆå§‹åŒ–ä¸­..."
        android:textSize="16sp"
        android:padding="16dp"
        app:layout_constraintTop_toBottomOf="@id/searchButton"
        app:layout_constraintBottom_toBottomOf="parent"
        app:layout_constraintStart_toStartOf="parent"
        app:layout_constraintEnd_toEndOf="parent"
        android:layout_marginTop="16dp" />
    
</androidx.constraintlayout.widget.ConstraintLayout>
```

---

## 4. æ–¹æ¡ˆ B: TensorFlow Lite

### 4.1 æ¨¡å‹è½‰æ›ï¼ˆè¤‡é›œï¼‰

#### æ­¥é©Ÿ 1ï¼šONNX â†’ TensorFlow

```python
# å®‰è£è½‰æ›å·¥å…·
pip install onnx-tf tensorflow

# è½‰æ›è…³æœ¬
import onnx
from onnx_tf.backend import prepare

# è¼‰å…¥ ONNX æ¨¡å‹
onnx_model = onnx.load("mobileclip_image.onnx")

# è½‰æ›ç‚º TensorFlow
tf_rep = prepare(onnx_model)

# å„²å­˜
tf_rep.export_graph("mobileclip_image_tf")
```

#### æ­¥é©Ÿ 2ï¼šTensorFlow â†’ TFLite

```python
import tensorflow as tf

# è¼‰å…¥ TensorFlow æ¨¡å‹
converter = tf.lite.TFLiteConverter.from_saved_model("mobileclip_image_tf")

# å„ªåŒ–è¨­å®š
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

# è½‰æ›
tflite_model = converter.convert()

# å„²å­˜
with open("mobileclip_image.tflite", "wb") as f:
    f.write(tflite_model)
```

### 4.2 Android æ•´åˆï¼ˆTFLiteï¼‰

`build.gradle.kts`ï¼š

```kotlin
dependencies {
    // TensorFlow Lite
    implementation("org.tensorflow:tensorflow-lite:2.14.0")
    implementation("org.tensorflow:tensorflow-lite-gpu:2.14.0")
    implementation("org.tensorflow:tensorflow-lite-support:0.4.4")
}
```

æ¨è«–ç¨‹å¼ç¢¼ï¼š

```kotlin
import org.tensorflow.lite.Interpreter
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel
import java.io.FileInputStream

class TFLiteMobileCLIP(context: Context) {
    
    private var interpreter: Interpreter? = null
    
    fun initialize() {
        val model = loadModelFile(context, "mobileclip_image.tflite")
        interpreter = Interpreter(model)
    }
    
    fun encodeImage(bitmap: Bitmap): FloatArray {
        val input = preprocessImage(bitmap)
        val output = Array(1) { FloatArray(512) }
        
        interpreter?.run(input, output)
        
        return output[0]
    }
    
    private fun loadModelFile(context: Context, modelPath: String): MappedByteBuffer {
        val fileDescriptor = context.assets.openFd(modelPath)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }
}
```

---

## 5. Android å°ˆæ¡ˆå¯¦ä½œ

### 5.1 æ¬Šé™è¨­å®š

`AndroidManifest.xml`ï¼š

```xml
<?xml version="1.0" encoding="utf-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    
    <!-- ç›¸æ©Ÿæ¬Šé™ -->
    <uses-feature android:name="android.hardware.camera" />
    <uses-permission android:name="android.permission.CAMERA" />
    
    <!-- å„²å­˜æ¬Šé™ -->
    <uses-permission android:name="android.permission.READ_EXTERNAL_STORAGE" />
    <uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" />
    
    <!-- ç¶²è·¯æ¬Šé™ï¼ˆå¦‚éœ€ä¸‹è¼‰æ¨¡å‹ï¼‰-->
    <uses-permission android:name="android.permission.INTERNET" />
    
    <application
        android:allowBackup="true"
        android:icon="@mipmap/ic_launcher"
        android:label="@string/app_name"
        android:theme="@style/Theme.MobileCLIP">
        
        <activity
            android:name=".MainActivity"
            android:exported="true">
            <intent-filter>
                <action android:name="android.intent.action.MAIN" />
                <category android:name="android.intent.category.LAUNCHER" />
            </intent-filter>
        </activity>
        
    </application>
    
</manifest>
```

### 5.2 ç›¸æ©Ÿæ•´åˆ

å‰µå»º `CameraHelper.kt`ï¼š

```kotlin
package com.example.mobileclip

import android.Manifest
import android.content.Context
import android.content.pm.PackageManager
import android.graphics.Bitmap
import android.graphics.BitmapFactory
import android.graphics.ImageFormat
import android.graphics.Rect
import android.graphics.YuvImage
import androidx.camera.core.*
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.core.content.ContextCompat
import androidx.lifecycle.LifecycleOwner
import java.io.ByteArrayOutputStream
import java.util.concurrent.ExecutorService
import java.util.concurrent.Executors

class CameraHelper(
    private val context: Context,
    private val lifecycleOwner: LifecycleOwner,
    private val previewView: PreviewView
) {
    
    private var cameraExecutor: ExecutorService = Executors.newSingleThreadExecutor()
    private var imageAnalyzer: ImageAnalysis? = null
    
    fun startCamera(onImageCaptured: (Bitmap) -> Unit) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)
        
        cameraProviderFuture.addListener({
            val cameraProvider = cameraProviderFuture.get()
            
            // é è¦½
            val preview = Preview.Builder().build().also {
                it.setSurfaceProvider(previewView.surfaceProvider)
            }
            
            // åœ–ç‰‡åˆ†æ
            imageAnalyzer = ImageAnalysis.Builder()
                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
                .build()
                .also {
                    it.setAnalyzer(cameraExecutor, { imageProxy ->
                        val bitmap = imageProxyToBitmap(imageProxy)
                        onImageCaptured(bitmap)
                        imageProxy.close()
                    })
                }
            
            // é¸æ“‡å¾Œç½®é¡é ­
            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA
            
            try {
                cameraProvider.unbindAll()
                cameraProvider.bindToLifecycle(
                    lifecycleOwner,
                    cameraSelector,
                    preview,
                    imageAnalyzer
                )
            } catch (e: Exception) {
                e.printStackTrace()
            }
            
        }, ContextCompat.getMainExecutor(context))
    }
    
    private fun imageProxyToBitmap(imageProxy: ImageProxy): Bitmap {
        val yuvImage = YuvImage(
            imageProxy.planes[0].buffer.array(),
            ImageFormat.NV21,
            imageProxy.width,
            imageProxy.height,
            null
        )
        
        val out = ByteArrayOutputStream()
        yuvImage.compressToJpeg(
            Rect(0, 0, imageProxy.width, imageProxy.height),
            100,
            out
        )
        
        val imageBytes = out.toByteArray()
        return BitmapFactory.decodeByteArray(imageBytes, 0, imageBytes.size)
    }
    
    fun shutdown() {
        cameraExecutor.shutdown()
    }
    
    companion object {
        fun hasPermissions(context: Context): Boolean {
            return ContextCompat.checkSelfPermission(
                context,
                Manifest.permission.CAMERA
            ) == PackageManager.PERMISSION_GRANTED
        }
    }
}
```

---

## 6. å®Œæ•´å•†å“æœå°‹ App

### 6.1 ç”¢å“è³‡æ–™çµæ§‹

```kotlin
data class Product(
    val id: String,
    val name: String,
    val price: Double,
    val description: String,
    val imageUrl: String,
    val features: FloatArray  // é è¨ˆç®—çš„åœ–ç‰‡ç‰¹å¾µ
)

data class SearchResult(
    val product: Product,
    val similarity: Float
) {
    val confidencePercent: Int get() = (similarity * 100).toInt()
}
```

### 6.2 ç”¢å“è³‡æ–™åº«ç®¡ç†

```kotlin
class ProductDatabase(private val context: Context) {
    
    private val products = mutableListOf<Product>()
    private lateinit var mobileCLIP: MobileCLIPInference
    
    suspend fun initialize() {
        mobileCLIP = MobileCLIPInference(context)
        mobileCLIP.initialize()
        
        // è¼‰å…¥ç”¢å“è³‡æ–™
        loadProducts()
    }
    
    private suspend fun loadProducts() {
        // å¾è³‡æ–™åº«æˆ– assets è¼‰å…¥ç”¢å“
        // é€™è£¡æ˜¯ç¤ºä¾‹è³‡æ–™
        
        val productData = listOf(
            Triple("P001", "Nike é‹å‹•é‹", 3200.0),
            Triple("P002", "Adidas ä¼‘é–’é‹", 2800.0),
            Triple("P003", "iPhone 15", 32900.0),
            // ... æ›´å¤šç”¢å“
        )
        
        for ((id, name, price) in productData) {
            // è¼‰å…¥ç”¢å“åœ–ç‰‡
            val bitmap = loadProductImage(id)
            
            // é è¨ˆç®—ç‰¹å¾µå‘é‡
            val features = mobileCLIP.encodeImage(bitmap)
            
            products.add(Product(
                id = id,
                name = name,
                price = price,
                description = "",
                imageUrl = "",
                features = features
            ))
        }
    }
    
    suspend fun search(queryBitmap: Bitmap, topK: Int = 5): List<SearchResult> {
        // ç·¨ç¢¼æŸ¥è©¢åœ–ç‰‡
        val queryFeatures = mobileCLIP.encodeImage(queryBitmap)
        
        // è¨ˆç®—æ‰€æœ‰ç”¢å“çš„ç›¸ä¼¼åº¦
        val results = products.map { product ->
            val similarity = mobileCLIP.calculateSimilarity(
                queryFeatures,
                product.features
            )
            SearchResult(product, similarity)
        }
        
        // æ’åºä¸¦è¿”å› top-K
        return results.sortedByDescending { it.similarity }.take(topK)
    }
    
    private fun loadProductImage(productId: String): Bitmap {
        // å¾ assets æˆ–ç¶²è·¯è¼‰å…¥ç”¢å“åœ–ç‰‡
        return BitmapFactory.decodeResource(
            context.resources,
            R.drawable.product_placeholder
        )
    }
}
```

### 6.3 æœå°‹ Activity

```kotlin
class SearchActivity : AppCompatActivity() {
    
    private lateinit var cameraHelper: CameraHelper
    private lateinit var productDatabase: ProductDatabase
    private lateinit var binding: ActivitySearchBinding
    
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        binding = ActivitySearchBinding.inflate(layoutInflater)
        setContentView(binding.root)
        
        // åˆå§‹åŒ–
        lifecycleScope.launch {
            showLoading(true)
            productDatabase = ProductDatabase(this@SearchActivity)
            productDatabase.initialize()
            showLoading(false)
            
            startCamera()
        }
        
        // æ‹ç…§æŒ‰éˆ•
        binding.captureButton.setOnClickListener {
            captureAndSearch()
        }
    }
    
    private fun startCamera() {
        if (CameraHelper.hasPermissions(this)) {
            cameraHelper = CameraHelper(this, this, binding.previewView)
            cameraHelper.startCamera { bitmap ->
                // å³æ™‚é è¦½ï¼ˆå¯é¸ï¼‰
            }
        } else {
            requestPermissions(
                arrayOf(Manifest.permission.CAMERA),
                REQUEST_CAMERA_PERMISSION
            )
        }
    }
    
    private fun captureAndSearch() {
        lifecycleScope.launch {
            try {
                showLoading(true)
                
                // å¾ç›¸æ©Ÿç²å–ç•¶å‰ç•«é¢
                val bitmap = getCurrentFrame()
                
                // æœå°‹
                val results = productDatabase.search(bitmap, topK = 3)
                
                // é¡¯ç¤ºçµæœ
                displayResults(results)
                
            } catch (e: Exception) {
                Toast.makeText(this@SearchActivity, "æœå°‹å¤±æ•—ï¼š${e.message}", Toast.LENGTH_SHORT).show()
            } finally {
                showLoading(false)
            }
        }
    }
    
    private fun displayResults(results: List<SearchResult>) {
        binding.resultsRecyclerView.adapter = SearchResultsAdapter(results) { product ->
            // é»æ“Šç”¢å“ï¼Œé¡¯ç¤ºè©³æƒ…
            showProductDetails(product)
        }
    }
    
    private fun showProductDetails(product: Product) {
        // é¡¯ç¤ºç”¢å“è©³æƒ…é é¢
        val intent = Intent(this, ProductDetailActivity::class.java)
        intent.putExtra("PRODUCT_ID", product.id)
        startActivity(intent)
    }
    
    companion object {
        private const val REQUEST_CAMERA_PERMISSION = 100
    }
}
```

### 6.4 æœå°‹çµæœ Adapter

```kotlin
class SearchResultsAdapter(
    private val results: List<SearchResult>,
    private val onItemClick: (Product) -> Unit
) : RecyclerView.Adapter<SearchResultsAdapter.ViewHolder>() {
    
    class ViewHolder(view: View) : RecyclerView.ViewHolder(view) {
        val productImage: ImageView = view.findViewById(R.id.productImage)
        val productName: TextView = view.findViewById(R.id.productName)
        val productPrice: TextView = view.findViewById(R.id.productPrice)
        val confidenceBar: ProgressBar = view.findViewById(R.id.confidenceBar)
        val confidenceText: TextView = view.findViewById(R.id.confidenceText)
    }
    
    override fun onCreateViewHolder(parent: ViewGroup, viewType: Int): ViewHolder {
        val view = LayoutInflater.from(parent.context)
            .inflate(R.layout.item_search_result, parent, false)
        return ViewHolder(view)
    }
    
    override fun onBindViewHolder(holder: ViewHolder, position: Int) {
        val result = results[position]
        val product = result.product
        
        holder.productName.text = product.name
        holder.productPrice.text = "NT$ ${product.price.toInt()}"
        holder.confidenceBar.progress = result.confidencePercent
        holder.confidenceText.text = "${result.confidencePercent}%"
        
        // è¼‰å…¥åœ–ç‰‡ï¼ˆä½¿ç”¨ Glide æˆ– Coilï¼‰
        // Glide.with(holder.itemView).load(product.imageUrl).into(holder.productImage)
        
        holder.itemView.setOnClickListener {
            onItemClick(product)
        }
    }
    
    override fun getItemCount() = results.size
}
```

---

## 7. æ•ˆèƒ½å„ªåŒ–

### 7.1 æ¨¡å‹é‡åŒ–

```python
# æ›´æ¿€é€²çš„é‡åŒ–
from onnxruntime.quantization import quantize_dynamic, QuantType

quantize_dynamic(
    'mobileclip_image.onnx',
    'mobileclip_image_int8.onnx',
    weight_type=QuantType.QInt8  # INT8 é‡åŒ–
)

# çµæœï¼š
# - åŸå§‹ï¼š35 MB â†’ INT8ï¼š9 MBï¼ˆæ¸›å°‘ 74%ï¼‰
# - ç²¾æº–åº¦æå¤±ï¼š< 1%
# - æ¨è«–é€Ÿåº¦ï¼šå¿« 1.5-2 å€
```

### 7.2 æ‰¹æ¬¡è™•ç†

```kotlin
suspend fun batchSearch(bitmaps: List<Bitmap>): List<FloatArray> {
    return withContext(Dispatchers.Default) {
        bitmaps.map { bitmap ->
            async { mobileCLIP.encodeImage(bitmap) }
        }.awaitAll()
    }
}
```

### 7.3 å‘é‡è³‡æ–™åº«åŠ é€Ÿ

```kotlin
class FastProductSearch {
    
    // ä½¿ç”¨ KD-Tree æˆ– HNSW åŠ é€Ÿæœå°‹
    private val index = mutableListOf<Pair<Product, FloatArray>>()
    
    fun buildIndex(products: List<Product>) {
        index.clear()
        products.forEach { product ->
            index.add(product to product.features)
        }
    }
    
    fun search(query: FloatArray, k: Int = 5): List<SearchResult> {
        // ä½¿ç”¨è¿‘ä¼¼æœ€è¿‘é„°æœå°‹ï¼ˆANNï¼‰
        // å°æ–¼ 10,000+ ç”¢å“ï¼Œé€Ÿåº¦æå‡ 10-100 å€
        
        return index
            .map { (product, features) ->
                val similarity = cosineSimilarity(query, features)
                SearchResult(product, similarity)
            }
            .sortedByDescending { it.similarity }
            .take(k)
    }
}
```

### 7.4 ä½¿ç”¨ç¡¬é«”åŠ é€Ÿ

```kotlin
// ONNX Runtime ç¡¬é«”åŠ é€Ÿ
val sessionOptions = OrtSession.SessionOptions()

// ä½¿ç”¨ NNAPI (Android Neural Networks API)
sessionOptions.addNNAPI()

// æˆ–ä½¿ç”¨ GPU
sessionOptions.addDirectML(0)

val session = ortEnvironment.createSession(modelBytes, sessionOptions)
```

---

## 8. å¸¸è¦‹å•é¡Œ

### Q1: æ¨¡å‹æª”æ¡ˆå¤ªå¤§ï¼ŒAPK è¶…é 100MB æ€éº¼è¾¦ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

1. **ä½¿ç”¨ Android App Bundle**
```gradle
// build.gradle
android {
    bundle {
        language {
            enableSplit = true
        }
        density {
            enableSplit = true
        }
        abi {
            enableSplit = true
        }
    }
}
```

2. **å‹•æ…‹ä¸‹è¼‰æ¨¡å‹**
```kotlin
// ä½¿ç”¨ Firebase ML Model Downloader
val conditions = CustomModelDownloadConditions.Builder()
    .requireWifi()
    .build()

FirebaseModelDownloader.getInstance()
    .getModel("mobileclip", DownloadType.LOCAL_MODEL, conditions)
    .addOnSuccessListener { model ->
        // ä½¿ç”¨ä¸‹è¼‰çš„æ¨¡å‹
    }
```

3. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹**
- MobileCLIP-S0 (9MB quantized) è€Œé B-LT (40MB)

### Q2: æ¨è«–é€Ÿåº¦å¤ªæ…¢æ€éº¼è¾¦ï¼Ÿ

**å„ªåŒ–æ–¹æ¡ˆ**ï¼š

```kotlin
// 1. ä½¿ç”¨é‡åŒ–æ¨¡å‹
// 2. é è¨ˆç®—ç”¢å“ç‰¹å¾µ
// 3. ä½¿ç”¨ç¡¬é«”åŠ é€Ÿ
// 4. é™ä½åœ–ç‰‡è§£æåº¦

// å¯¦éš›æ¸¬è©¦ï¼š
// - S0 é‡åŒ– + NNAPIï¼š5-10ms
// - S2 é‡åŒ– + CPUï¼š15-25ms
// - B é‡åŒ– + CPUï¼š40-60ms
```

### Q3: è¨˜æ†¶é«”ä¸è¶³ï¼ˆOOMï¼‰æ€éº¼è¾¦ï¼Ÿ

```kotlin
// 1. åŠæ™‚é‡‹æ”¾ Bitmap
bitmap.recycle()

// 2. ä½¿ç”¨ BitmapFactory.Options
val options = BitmapFactory.Options()
options.inSampleSize = 2  // ç¸®å° 2 å€
options.inPreferredConfig = Bitmap.Config.RGB_565  // æ¸›å°‘è¨˜æ†¶é«”

// 3. åˆ†æ‰¹è™•ç†
products.chunked(50).forEach { batch ->
    processBatch(batch)
    System.gc()
}
```

### Q4: å¦‚ä½•å¯¦ä½œå®Œæ•´çš„ Tokenizerï¼Ÿ

```kotlin
// ç°¡åŒ–æ–¹æ¡ˆï¼šä½¿ç”¨é å…ˆè¨ˆç®—çš„æ–‡å­—ç‰¹å¾µ
class PrecomputedTextFeatures {
    private val textFeatures = mapOf(
        "ç´…è‰²" to floatArrayOf(/* 512 ç¶­å‘é‡ */),
        "é‹å­" to floatArrayOf(/* 512 ç¶­å‘é‡ */),
        // ... æ›´å¤šå¸¸ç”¨è©
    )
    
    fun getFeatures(text: String): FloatArray? {
        return textFeatures[text]
    }
}

// å®Œæ•´æ–¹æ¡ˆï¼šç§»æ¤ CLIP tokenizer
// å¯åƒè€ƒï¼šhttps://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py
```

### Q5: Android 6.0 ä»¥ä¸‹æ”¯æ´å—ï¼Ÿ

```kotlin
// æœ€ä½æ”¯æ´åˆ° API 24 (Android 7.0)
// å¦‚éœ€æ”¯æ´æ›´èˆŠç‰ˆæœ¬ï¼š

android {
    defaultConfig {
        minSdk = 21  // Android 5.0
        
        // ä½†éœ€è¦è™•ç†ç›¸å®¹æ€§
        ndk {
            abiFilters += listOf("armeabi-v7a", "arm64-v8a")
        }
    }
}

// æ³¨æ„ï¼šæŸäº›ç¡¬é«”åŠ é€ŸåŠŸèƒ½å¯èƒ½ä¸å¯ç”¨
```

---

## é™„éŒ„

### A. å®Œæ•´å°ˆæ¡ˆçµæ§‹

```
MobileCLIPDemo/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ mobileclip_image_quantized.onnx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mobileclip_text_quantized.onnx
â”‚   â”‚   â”‚   â”œâ”€â”€ java/com/example/mobileclip/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MainActivity.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SearchActivity.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MobileCLIPInference.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ProductDatabase.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CameraHelper.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ SearchResultsAdapter.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ res/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ activity_main.xml
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ activity_search.xml
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ item_search_result.xml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ values/
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ strings.xml
â”‚   â”‚   â”‚   â””â”€â”€ AndroidManifest.xml
â”‚   â”‚   â””â”€â”€ build.gradle.kts
â”‚   â””â”€â”€ build.gradle.kts
â””â”€â”€ gradle.properties
```

### B. æ¸¬è©¦ Checklist

- [ ] æ¨¡å‹æˆåŠŸè¼‰å…¥
- [ ] åœ–ç‰‡é è™•ç†æ­£ç¢º
- [ ] æ¨è«–çµæœåˆç†
- [ ] è¨˜æ†¶é«”ä½¿ç”¨æ­£å¸¸ï¼ˆ< 200MBï¼‰
- [ ] æ¨è«–é€Ÿåº¦å¯æ¥å—ï¼ˆ< 100msï¼‰
- [ ] ç›¸æ©Ÿæ¬Šé™æ­£å¸¸
- [ ] æœå°‹çµæœæ­£ç¢º
- [ ] UI éŸ¿æ‡‰æµæš¢

### C. æ•ˆèƒ½åŸºæº–

| è£ç½® | æ¨¡å‹ | æ¨è«–æ™‚é–“ | è¨˜æ†¶é«” |
|------|------|---------|--------|
| Pixel 7 | S2-Quant + NNAPI | 8ms | 80MB |
| Samsung S21 | S2-Quant + CPU | 22ms | 95MB |
| å°ç±³ 11 | B-Quant + CPU | 55ms | 150MB |

### D. åƒè€ƒè³‡æº

- **ONNX Runtime Android**ï¼šhttps://onnxruntime.ai/docs/tutorials/mobile/
- **TensorFlow Lite**ï¼šhttps://www.tensorflow.org/lite
- **CameraX**ï¼šhttps://developer.android.com/training/camerax
- **MobileCLIP è«–æ–‡**ï¼šhttps://arxiv.org/abs/2311.17049

---

## çµèª

æ­å–œï¼ğŸ‰ ä½ å·²ç¶“æŒæ¡äº†å°‡ MobileCLIP éƒ¨ç½²åˆ° Android çš„å®Œæ•´æµç¨‹ã€‚

**é—œéµè¦é»**ï¼š
1. âœ… ONNX Runtime æ˜¯æœ€ç°¡å–®çš„æ–¹æ¡ˆ
2. âš¡ é‡åŒ–æ¨¡å‹å¯æ¸›å°‘ 70% é«”ç©
3. ğŸ“± é è¨ˆç®—ç”¢å“ç‰¹å¾µæ˜¯é—œéµå„ªåŒ–
4. ğŸš€ ç¡¬é«”åŠ é€Ÿå¯æå‡ 3-5 å€é€Ÿåº¦

**ä¸‹ä¸€æ­¥**ï¼š
- å¯¦ä½œå®Œæ•´çš„å•†å“è³‡æ–™åº«
- åŠ å…¥æ–‡å­—æœå°‹åŠŸèƒ½
- å„ªåŒ– UI/UX
- ä¸Šæ¶ Google Play

ç¥ä½ é–‹ç™¼é †åˆ©ï¼æœ‰å•é¡Œæ­¡è¿éš¨æ™‚è©¢å• ğŸ˜Š

---

**æ–‡ä»¶ç‰ˆæœ¬**ï¼šv1.0  
**ä½œè€…**ï¼šClaude  
**æœ€å¾Œæ›´æ–°**ï¼š2025-10-27
