# MobileCLIP å®Œæ•´ä½¿ç”¨æŒ‡å—

> Apple MobileCLIP æ¨¡å‹ä»‹ç´¹ + å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹

---

## ğŸ“‘ ç›®éŒ„

- [æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡](#æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡)
- [å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®](#å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®)
- [åŸºç¤ä½¿ç”¨ç¯„ä¾‹](#åŸºç¤ä½¿ç”¨ç¯„ä¾‹)
- [é€²éšæ‡‰ç”¨ç¯„ä¾‹](#é€²éšæ‡‰ç”¨ç¯„ä¾‹)
- [æ•ˆèƒ½å„ªåŒ–æŠ€å·§](#æ•ˆèƒ½å„ªåŒ–æŠ€å·§)
- [å¸¸è¦‹å•é¡Œè™•ç†](#å¸¸è¦‹å•é¡Œè™•ç†)

---

## æ¨¡å‹ä»‹ç´¹èˆ‡é¸æ“‡

### ğŸ“Š äº”å€‹æ¨¡å‹å®Œæ•´å°æ¯”

| æ¨¡å‹ | åƒæ•¸é‡ | æ¨è«–é€Ÿåº¦ | ImageNet æº–ç¢ºåº¦ | 38 å€‹æ•¸æ“šé›†å¹³å‡ | æª”æ¡ˆå¤§å° | é©ç”¨å ´æ™¯ |
|------|--------|----------|----------------|----------------|---------|----------|
| **S0** | 54M | 3.1ms | 67.8% | 58.1% | ~45 MB | æ¥µè‡´è¼•é‡ï¼Œä½éšæ‰‹æ©Ÿ |
| **S1** | 63M | 3.3ms | 72.6% | 61.3% | ~55 MB | å¹³è¡¡é€Ÿåº¦èˆ‡æº–ç¢ºåº¦ â­ |
| **S2** | 82M | 4.2ms | 75.7% | 63.7% | ~70 MB | è¼ƒé«˜æº–ç¢ºåº¦éœ€æ±‚ â­ |
| **B** | 86M | 5.4ms | 76.8% | 65.2% | ~75 MB | é«˜æº–ç¢ºåº¦ï¼Œä¸­éšæ‰‹æ©Ÿ |
| **B (LT)** | 86M | 5.4ms | 77.2% | 65.8% | ~75 MB | æœ€é«˜æº–ç¢ºåº¦ |

> â­ æ¨è–¦ï¼šä»¥åœ–æ‰¾åœ–æ‡‰ç”¨å„ªå…ˆé¸æ“‡ **S1** æˆ– **S2**

---

### ğŸ” æ¨¡å‹è©³ç´°èªªæ˜

#### **mobileclip_s0.pt** - æ¥µè‡´è¼•é‡ç‰ˆ
```
âœ“ ç‰¹é»ï¼šé«”ç©æœ€å°ã€é€Ÿåº¦æœ€å¿«
âœ“ åƒæ•¸ï¼š11.4M (åœ–åƒ) + 42.4M (æ–‡å­—) = 53.8M
âœ“ é€Ÿåº¦ï¼š1.5ms (åœ–åƒ) + 1.6ms (æ–‡å­—) = 3.1ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 67.8%

é©åˆï¼š
- å…¥é–€ç´šæ‰‹æ©Ÿã€IoT è£ç½®
- å³æ™‚è™•ç†éœ€æ±‚ï¼ˆç›¸æ©Ÿ Appï¼‰
- éœ€è¦æ¥µä½å»¶é²çš„æ‡‰ç”¨

æ¯”è¼ƒï¼š
èˆ‡ OpenAI ViT-B/16 æº–ç¢ºåº¦ç›¸ç•¶ï¼Œä½†å¿« 4.8 å€ã€å° 2.8 å€
```

#### **mobileclip_s1.pt** - è¼•é‡å¹³è¡¡ç‰ˆ â­
```
âœ“ ç‰¹é»ï¼šè¼•é‡èˆ‡æº–ç¢ºåº¦çš„æœ€ä½³å¹³è¡¡é»
âœ“ åƒæ•¸ï¼šç´„ 63M
âœ“ é€Ÿåº¦ï¼šç´„ 3.3ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 72.6%

é©åˆï¼š
- ä¸€èˆ¬æ‰‹æ©Ÿæ‡‰ç”¨
- å¤§å¤šæ•¸ä»¥åœ–æ‰¾åœ–å ´æ™¯
- å¹³è¡¡æ•ˆèƒ½èˆ‡å“è³ª

æ¨è–¦ç†ç”±ï¼š
- å¿«é€Ÿé©—è­‰ POC çš„æœ€ä½³é¸æ“‡
- Android ç§»æ¤æœ€å®¹æ˜“
- æº–ç¢ºåº¦å·²è¶³å¤ å¤§å¤šæ•¸å ´æ™¯
```

#### **mobileclip_s2.pt** - ä¸­ç­‰è¦æ¨¡ç‰ˆ â­
```
âœ“ ç‰¹é»ï¼šæ›´é«˜æº–ç¢ºåº¦ï¼Œä»ä¿æŒè¼•é‡
âœ“ åƒæ•¸ï¼šç´„ 82M
âœ“ é€Ÿåº¦ï¼šç´„ 4.2ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 75.7%

é©åˆï¼š
- ä¸­é«˜éšæ‰‹æ©Ÿ
- éœ€è¦è¼ƒé«˜è¾¨è­˜æº–ç¢ºåº¦çš„æ‡‰ç”¨
- é›»å•†ã€åœ–ç‰‡æœå°‹ç­‰å ´æ™¯

æ¯”è¼ƒï¼š
æ¯” SigLIP ViT-B/16 å¿« 2.3 å€ã€å° 2.1 å€ï¼Œä½†æº–ç¢ºåº¦æ›´é«˜
```

#### **mobileclip_b.pt** - æ¨™æº–å¤§æ¨¡å‹
```
âœ“ ç‰¹é»ï¼šé«˜æº–ç¢ºåº¦ç‰ˆæœ¬
âœ“ åƒæ•¸ï¼šç´„ 86M
âœ“ é€Ÿåº¦ï¼šç´„ 5.4ms
âœ“ æº–ç¢ºåº¦ï¼šImageNet 76.8%

é©åˆï¼š
- æ——è‰¦æ‰‹æ©Ÿã€å¹³æ¿
- å°ˆæ¥­æ‡‰ç”¨ï¼ˆè¨­è¨ˆã€å‰µæ„å·¥å…·ï¼‰
- å°æº–ç¢ºåº¦è¦æ±‚é«˜çš„å ´æ™¯
```

#### **mobileclip_blt.pt** - é•·è¨“ç·´ç‰ˆæœ¬
```
âœ“ ç‰¹é»ï¼šB ç‰ˆæœ¬çš„å¢å¼·è¨“ç·´ç‰ˆï¼Œæº–ç¢ºåº¦æœ€é«˜
âœ“ åƒæ•¸ï¼š86Mï¼ˆèˆ‡ B ç›¸åŒï¼‰
âœ“ é€Ÿåº¦ï¼š5.4msï¼ˆèˆ‡ B ç›¸åŒï¼‰
âœ“ æº–ç¢ºåº¦ï¼šImageNet 77.2%ï¼ˆæœ€é«˜ï¼‰
âœ“ è¨“ç·´ï¼šä½¿ç”¨æ›´é•·çš„è¨“ç·´æ™‚é–“ï¼ˆ600k iterationsï¼‰

é©åˆï¼š
- éœ€è¦æœ€ä½³æº–ç¢ºåº¦çš„æ‡‰ç”¨
- æœå‹™ç«¯éƒ¨ç½²ï¼ˆè¨˜æ†¶é«”ä¸å—é™ï¼‰
- å“è³ªå„ªå…ˆçš„å ´æ™¯

æ¯”è¼ƒï¼š
æº–ç¢ºåº¦è¶…è¶Š OpenAI ViT-L/14@336
```

---

### ğŸ¯ æ¨¡å‹é¸æ“‡æ±ºç­–æ¨¹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  éœ€è¦æœ€å¿«é€Ÿåº¦ï¼Ÿ              â”‚
â”‚  â””â”€ Yes â†’ S0                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ No
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  éœ€è¦æœ€é«˜æº–ç¢ºåº¦ï¼Ÿ            â”‚
â”‚  â””â”€ Yes â†’ B (LT)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ No
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä»‹æ–¼å…©è€…ä¹‹é–“ï¼Ÿ              â”‚
â”‚  â”œâ”€ åå‘é€Ÿåº¦ â†’ S1 â­        â”‚
â”‚  â”œâ”€ å¹³è¡¡ â†’ S2 â­            â”‚
â”‚  â””â”€ åå‘æº–ç¢ºåº¦ â†’ B          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“± å¯¦éš›æ‡‰ç”¨å ´æ™¯æ¨è–¦

| æ‡‰ç”¨å ´æ™¯ | æ¨è–¦æ¨¡å‹ | ç†ç”± |
|---------|---------|------|
| ç›¸æ©Ÿå³æ™‚è¾¨è­˜ | **S0** | é€Ÿåº¦å„ªå…ˆï¼Œä½å»¶é² |
| æ‰‹æ©Ÿç›¸ç°¿æœå°‹ | **S1** æˆ– **S2** | å¹³è¡¡é«”é©— |
| é›»å•†ä»¥åœ–æ‰¾åœ– | **S2** æˆ– **B** | æº–ç¢ºåº¦é‡è¦ |
| å°ˆæ¥­åœ–ç‰‡ç®¡ç† | **B (LT)** | å“è³ªå„ªå…ˆ |
| IoT/é‚Šç·£è¨­å‚™ | **S0** | è³‡æºå—é™ |
| æœå‹™ç«¯ API | **B (LT)** | ç„¡è³‡æºé™åˆ¶ |

---

## å®‰è£èˆ‡ç’°å¢ƒè¨­ç½®

### ğŸ“¦ å®‰è£ä¾è³´

```bash
# å®‰è£å¿…è¦å¥—ä»¶
pip install torch torchvision pillow numpy tqdm matplotlib

# å®‰è£ MobileCLIP
pip install git+https://github.com/apple/ml-mobileclip.git
```

### ğŸ“¥ ä¸‹è¼‰é è¨“ç·´æ¨¡å‹

```bash
# å»ºç«‹æ¨¡å‹è³‡æ–™å¤¾
mkdir -p checkpoints
cd checkpoints

# ä¸‹è¼‰æ¨¡å‹ï¼ˆé¸ä¸€å€‹æˆ–å¤šå€‹ï¼‰
# S0 - æœ€è¼•é‡ï¼ˆå»ºè­°å…ˆä¸‹è¼‰é€™å€‹æ¸¬è©¦ï¼‰
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt

# S1 - å¹³è¡¡ç‰ˆï¼ˆæ¨è–¦ï¼‰
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s1.pt

# S2 - ä¸­ç­‰è¦æ¨¡ï¼ˆæ¨è–¦ï¼‰
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s2.pt

# B - å¤§æ¨¡å‹
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_b.pt

# B (LT) - æœ€ä½³æº–ç¢ºåº¦
wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_blt.pt
```

æˆ–ä½¿ç”¨å®˜æ–¹è…³æœ¬ï¼š
```bash
# ä¸‹è¼‰æ‰€æœ‰æ¨¡å‹
source get_pretrained_models.sh
```

---

## åŸºç¤ä½¿ç”¨ç¯„ä¾‹

### ğŸ¯ ç¯„ä¾‹ 1ï¼šå–®å¼µåœ–ç‰‡ç‰¹å¾µæå–ï¼ˆæœ€åŸºæœ¬ï¼‰

```python
import torch
import mobileclip
from PIL import Image

# ========== 1. è¼‰å…¥æ¨¡å‹ ==========
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s1',  # æ¨¡å‹åç¨±ï¼šs0, s1, s2, b
    pretrained='checkpoints/mobileclip_s1.pt'  # æ¬Šé‡æª”æ¡ˆè·¯å¾‘
)

# è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
model.eval()

# é¸æ“‡è£ç½®ï¼ˆGPU æˆ– CPUï¼‰
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥ï¼Œä½¿ç”¨è£ç½®: {device}")


# ========== 2. å¾åœ–ç‰‡æª”æ¡ˆè½‰æ›æˆ tensor ==========

# è®€å–åœ–ç‰‡ï¼ˆæ”¯æ´ jpg, png ç­‰æ ¼å¼ï¼‰
image_path = "my_cat.jpg"
image = Image.open(image_path).convert('RGB')  # ç¢ºä¿æ˜¯ RGB æ ¼å¼

print(f"âœ“ åŸå§‹åœ–ç‰‡å¤§å°: {image.size}")

# ä½¿ç”¨ preprocess é€²è¡Œé è™•ç†ï¼ˆresize, normalize ç­‰ï¼‰
image_tensor = preprocess(image)  # è¼¸å‡º shape: (3, H, W)

print(f"âœ“ é è™•ç†å¾Œ tensor shape: {image_tensor.shape}")

# å¢åŠ  batch ç¶­åº¦ (1, 3, H, W)
image_tensor = image_tensor.unsqueeze(0)

print(f"âœ“ åŠ å…¥ batch ç¶­åº¦å¾Œ: {image_tensor.shape}")

# ç§»å‹•åˆ°å°æ‡‰è£ç½®
image_tensor = image_tensor.to(device)


# ========== 3. æå–ç‰¹å¾µ ==========

with torch.no_grad():  # ä¸éœ€è¦è¨ˆç®—æ¢¯åº¦
    image_features = model.encode_image(image_tensor)
    
    # L2 æ­¸ä¸€åŒ–ï¼ˆé‡è¦ï¼ç”¨æ–¼è¨ˆç®—ç›¸ä¼¼åº¦ï¼‰
    image_features = image_features / image_features.norm(dim=-1, keepdim=True)

print(f"âœ“ ç‰¹å¾µå‘é‡ shape: {image_features.shape}")  # (1, 512)
print(f"âœ“ ç‰¹å¾µå‘é‡ç¯„ä¾‹ï¼ˆå‰ 10 ç¶­ï¼‰: {image_features[0, :10]}")

# è½‰æ›æˆ numpyï¼ˆå¦‚æœéœ€è¦å„²å­˜æˆ–é€²ä¸€æ­¥è™•ç†ï¼‰
features_numpy = image_features.cpu().numpy()
print(f"âœ“ Numpy æ ¼å¼ shape: {features_numpy.shape}")
```

**è¼¸å‡ºç¯„ä¾‹ï¼š**
```
âœ“ æ¨¡å‹å·²è¼‰å…¥ï¼Œä½¿ç”¨è£ç½®: cpu
âœ“ åŸå§‹åœ–ç‰‡å¤§å°: (1920, 1080)
âœ“ é è™•ç†å¾Œ tensor shape: torch.Size([3, 256, 256])
âœ“ åŠ å…¥ batch ç¶­åº¦å¾Œ: torch.Size([1, 3, 256, 256])
âœ“ ç‰¹å¾µå‘é‡ shape: torch.Size([1, 512])
âœ“ ç‰¹å¾µå‘é‡ç¯„ä¾‹ï¼ˆå‰ 10 ç¶­ï¼‰: tensor([ 0.0234, -0.1234,  0.0567, ...])
âœ“ Numpy æ ¼å¼ shape: (1, 512)
```

---

### ğŸ“ é—œéµæ¦‚å¿µèªªæ˜

#### **image_tensor çš„å®Œæ•´è½‰æ›æµç¨‹**

```python
# æ­¥é©Ÿ 1: è®€å–åœ–ç‰‡æª”æ¡ˆ
image = Image.open("cat.jpg").convert('RGB')
# â†’ PIL.Image ç‰©ä»¶ï¼Œä¾‹å¦‚ (1920, 1080, 3)

# æ­¥é©Ÿ 2: é è™•ç†ï¼ˆresize, normalizeï¼‰
image_tensor = preprocess(image)
# â†’ torch.Tensor, shape: (3, H, W)ï¼Œä¾‹å¦‚ (3, 256, 256)
# â†’ å€¼ç¯„åœå·²è¢«æ¨™æº–åŒ–ï¼ˆé€šå¸¸æ˜¯ [-1, 1] æˆ– [0, 1]ï¼‰

# æ­¥é©Ÿ 3: å¢åŠ  batch ç¶­åº¦
image_tensor = image_tensor.unsqueeze(0)
# â†’ shape: (1, 3, H, W)ï¼Œä¾‹å¦‚ (1, 3, 256, 256)

# æ­¥é©Ÿ 4: ç§»åˆ°å°æ‡‰è£ç½®
image_tensor = image_tensor.to(device)
# â†’ å¦‚æœæœ‰ GPU å°±ç§»åˆ° GPUï¼Œå¦å‰‡ç•™åœ¨ CPU

# æ­¥é©Ÿ 5: æå–ç‰¹å¾µ
image_features = model.encode_image(image_tensor)
# â†’ shape: (1, 512)ï¼Œå°±æ˜¯ä½ è¦çš„ç‰¹å¾µå‘é‡ï¼
```

#### **preprocess åšäº†ä»€éº¼ï¼Ÿ**

`preprocess` æ˜¯ MobileCLIP æä¾›çš„é è™•ç†å‡½æ•¸ï¼Œç­‰åŒæ–¼ï¼š

```python
from torchvision import transforms

preprocess = transforms.Compose([
    transforms.Resize(256),              # èª¿æ•´å¤§å°
    transforms.CenterCrop(256),          # ä¸­å¿ƒè£åˆ‡
    transforms.ToTensor(),               # è½‰æˆ Tensor
    transforms.Normalize(                # æ¨™æº–åŒ–
        mean=[0.485, 0.456, 0.406],      # ImageNet mean
        std=[0.229, 0.224, 0.225]        # ImageNet std
    )
])
```

---

## é€²éšæ‡‰ç”¨ç¯„ä¾‹

### ğŸš€ ç¯„ä¾‹ 2ï¼šæ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡ï¼ˆæ›´å¿«ï¼‰

```python
import torch
import mobileclip
from PIL import Image
from pathlib import Path
from tqdm import tqdm

# è¼‰å…¥æ¨¡å‹
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s1',
    pretrained='checkpoints/mobileclip_s1.pt'
)
model.eval()
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)


# ========== æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡ ==========

def extract_features_batch(image_paths, batch_size=32):
    """
    æ‰¹æ¬¡æå–å¤šå¼µåœ–ç‰‡çš„ç‰¹å¾µ
    
    Args:
        image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
    
    Returns:
        features: (N, 512) çš„ç‰¹å¾µçŸ©é™£
        valid_paths: æˆåŠŸè™•ç†çš„åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
    """
    all_features = []
    valid_paths = []
    
    # åˆ†æ‰¹è™•ç†
    for i in tqdm(range(0, len(image_paths), batch_size), desc="æå–ç‰¹å¾µ"):
        batch_paths = image_paths[i:i+batch_size]
        
        # è¼‰å…¥ä¸¦é è™•ç†é€™ä¸€æ‰¹åœ–ç‰‡
        batch_images = []
        batch_valid_paths = []
        
        for path in batch_paths:
            try:
                img = Image.open(path).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid_paths.append(path)
            except Exception as e:
                print(f"âš  ç„¡æ³•è®€å– {path}: {e}")
                continue
        
        if not batch_images:
            continue
        
        # å †ç–Šæˆ batch (B, 3, H, W)
        batch_tensor = torch.stack(batch_images).to(device)
        
        # æå–ç‰¹å¾µ
        with torch.no_grad():
            features = model.encode_image(batch_tensor)
            # L2 æ­¸ä¸€åŒ–
            features = features / features.norm(dim=-1, keepdim=True)
        
        all_features.append(features.cpu())
        valid_paths.extend(batch_valid_paths)
    
    # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
    if all_features:
        all_features = torch.cat(all_features, dim=0)
    else:
        all_features = torch.empty(0, 512)
    
    return all_features, valid_paths


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========

# æƒæåœ–ç‰‡è³‡æ–™å¤¾
image_folder = "./my_photos"
image_paths = []

for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp']:
    image_paths.extend(list(Path(image_folder).glob(ext)))

image_paths = [str(p) for p in image_paths]
print(f"æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")

# æ‰¹æ¬¡æå–ç‰¹å¾µ
features, valid_paths = extract_features_batch(image_paths, batch_size=32)

print(f"âœ“ ç‰¹å¾µçŸ©é™£ shape: {features.shape}")  # (N, 512)
print(f"âœ“ æˆåŠŸè™•ç† {len(valid_paths)} å¼µåœ–ç‰‡")

# å„²å­˜ç‰¹å¾µ
import numpy as np
np.savez('image_features.npz', 
         features=features.numpy(),
         paths=valid_paths)
print("âœ“ ç‰¹å¾µå·²å„²å­˜åˆ° image_features.npz")
```

---

### ğŸ” ç¯„ä¾‹ 3ï¼šä»¥åœ–æ‰¾åœ–ï¼ˆå®Œæ•´æµç¨‹ï¼‰

#### æ­¥é©Ÿ 1: å»ºç«‹åœ–ç‰‡ç´¢å¼•

```python
import torch
import mobileclip
from PIL import Image
import numpy as np
from pathlib import Path
from tqdm import tqdm

def build_image_index(image_folder, model_name='mobileclip_s1', output_file='index.npz'):
    """
    å»ºç«‹åœ–ç‰‡ç´¢å¼•
    
    Args:
        image_folder: åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±
        output_file: ç´¢å¼•è¼¸å‡ºæª”æ¡ˆ
    
    Returns:
        features_matrix: (N, 512) ç‰¹å¾µçŸ©é™£
        image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
    """
    
    # è¼‰å…¥æ¨¡å‹
    print("ğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name,
        pretrained=f'checkpoints/{model_name}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")
    
    # æƒææ‰€æœ‰åœ–ç‰‡
    print("\nğŸ“‚ æƒæåœ–ç‰‡...")
    image_paths = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.bmp']:
        image_paths.extend(Path(image_folder).glob(ext))
    
    image_paths = [str(p) for p in image_paths]
    print(f"âœ“ æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")
    
    # æ‰¹æ¬¡æå–ç‰¹å¾µ
    print("\nğŸ¨ æå–ç‰¹å¾µ...")
    all_features = []
    valid_paths = []
    batch_size = 32
    
    for i in tqdm(range(0, len(image_paths), batch_size)):
        batch_paths = image_paths[i:i+batch_size]
        batch_images = []
        batch_valid = []
        
        for img_path in batch_paths:
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid.append(img_path)
            except Exception as e:
                print(f"âš  è·³é {img_path}: {e}")
        
        if not batch_images:
            continue
        
        # æ‰¹æ¬¡æ¨è«–
        batch_tensor = torch.stack(batch_images).to(device)
        
        with torch.no_grad():
            features = model.encode_image(batch_tensor)
            features = features / features.norm(dim=-1, keepdim=True)
        
        all_features.append(features.cpu().numpy())
        valid_paths.extend(batch_valid)
    
    # åˆä½µç‰¹å¾µ
    features_matrix = np.vstack(all_features)
    
    # å„²å­˜ç´¢å¼•
    print(f"\nğŸ’¾ å„²å­˜ç´¢å¼•...")
    np.savez(output_file, 
             features=features_matrix,
             paths=valid_paths,
             model_name=model_name)
    
    print(f"âœ“ ç´¢å¼•å·²å»ºç«‹: {output_file}")
    print(f"âœ“ ç‰¹å¾µçŸ©é™£ shape: {features_matrix.shape}")
    print(f"âœ“ åœ–ç‰‡æ•¸é‡: {len(valid_paths)}")
    
    return features_matrix, valid_paths


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========
if __name__ == '__main__':
    features, paths = build_image_index(
        image_folder='./my_photos',
        model_name='mobileclip_s1',
        output_file='photo_index.npz'
    )
```

#### æ­¥é©Ÿ 2: æœå°‹ç›¸ä¼¼åœ–ç‰‡

```python
import torch
import mobileclip
from PIL import Image
import numpy as np

def search_similar_images(query_image_path, 
                         index_file='index.npz', 
                         top_k=5,
                         model_name='mobileclip_s1'):
    """
    æœå°‹ç›¸ä¼¼åœ–ç‰‡
    
    Args:
        query_image_path: æŸ¥è©¢åœ–ç‰‡è·¯å¾‘
        index_file: ç´¢å¼•æª”æ¡ˆè·¯å¾‘
        top_k: è¿”å›çµæœæ•¸é‡
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±
    
    Returns:
        results: [(image_path, similarity_score), ...]
    """
    
    # è¼‰å…¥ç´¢å¼•
    print(f"ğŸ“‚ è¼‰å…¥ç´¢å¼•: {index_file}")
    data = np.load(index_file, allow_pickle=True)
    index_features = data['features']  # (N, 512)
    image_paths = data['paths'].tolist()
    
    print(f"âœ“ è¼‰å…¥ {len(image_paths)} å¼µåœ–ç‰‡çš„ç´¢å¼•")
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name,
        pretrained=f'checkpoints/{model_name}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # æå–æŸ¥è©¢åœ–ç‰‡ç‰¹å¾µ
    print(f"\nğŸ” æå–æŸ¥è©¢åœ–ç‰‡ç‰¹å¾µ...")
    query_img = Image.open(query_image_path).convert('RGB')
    query_tensor = preprocess(query_img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        query_feat = model.encode_image(query_tensor)
        query_feat = query_feat / query_feat.norm(dim=-1, keepdim=True)
    
    query_feat = query_feat.cpu().numpy()  # (1, 512)
    
    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆçŸ©é™£ä¹˜æ³•ï¼‰
    print(f"\nğŸ“Š è¨ˆç®—ç›¸ä¼¼åº¦...")
    similarities = np.dot(index_features, query_feat.T).squeeze()  # (N,)
    
    # å– Top-K
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    # é¡¯ç¤ºçµæœ
    print(f"\n{'='*60}")
    print(f"æŸ¥è©¢åœ–ç‰‡: {query_image_path}")
    print(f"{'='*60}")
    print(f"\nTop-{top_k} æœ€ç›¸ä¼¼åœ–ç‰‡:\n")
    
    results = []
    for i, idx in enumerate(top_indices):
        path = image_paths[idx]
        score = similarities[idx]
        print(f"{i+1}. {path}")
        print(f"   ç›¸ä¼¼åº¦: {score:.4f} ({score*100:.2f}%)\n")
        results.append((path, float(score)))
    
    return results


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========
if __name__ == '__main__':
    results = search_similar_images(
        query_image_path='./query_cat.jpg',
        index_file='photo_index.npz',
        top_k=5,
        model_name='mobileclip_s1'
    )
```

---

### ğŸ“Š ç¯„ä¾‹ 4ï¼šè¦–è¦ºåŒ–æœå°‹çµæœ

```python
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

def visualize_search_results(query_path, results, top_k=5, save_path='search_results.png'):
    """
    è¦–è¦ºåŒ–æœå°‹çµæœ
    
    Args:
        query_path: æŸ¥è©¢åœ–ç‰‡è·¯å¾‘
        results: [(image_path, score), ...] æœå°‹çµæœ
        top_k: é¡¯ç¤ºæ•¸é‡
        save_path: å„²å­˜è·¯å¾‘
    """
    
    # è¨­å®šåœ–è¡¨
    fig, axes = plt.subplots(1, top_k+1, figsize=(3*(top_k+1), 3))
    
    # é¡¯ç¤ºæŸ¥è©¢åœ–ç‰‡
    query_img = Image.open(query_path)
    axes[0].imshow(query_img)
    axes[0].set_title('Query Image', fontsize=12, fontweight='bold', color='red')
    axes[0].axis('off')
    axes[0].set_facecolor('#ffe6e6')
    
    # é¡¯ç¤ºæœå°‹çµæœ
    for i, (img_path, score) in enumerate(results[:top_k]):
        try:
            img = Image.open(img_path)
            axes[i+1].imshow(img)
            axes[i+1].set_title(
                f'#{i+1}\nScore: {score:.3f}', 
                fontsize=10,
                color='green' if score > 0.8 else 'blue'
            )
            axes[i+1].axis('off')
        except Exception as e:
            print(f"ç„¡æ³•è¼‰å…¥åœ–ç‰‡ {img_path}: {e}")
            axes[i+1].text(0.5, 0.5, 'Error', ha='center', va='center')
            axes[i+1].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
    plt.show()
    
    print(f"âœ“ æœå°‹çµæœå·²å„²å­˜åˆ° {save_path}")


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========
if __name__ == '__main__':
    # å…ˆæœå°‹
    results = search_similar_images(
        query_image_path='./query_cat.jpg',
        index_file='photo_index.npz',
        top_k=5
    )
    
    # è¦–è¦ºåŒ–
    visualize_search_results(
        query_path='./query_cat.jpg',
        results=results,
        top_k=5
    )
```

---

### ğŸ¯ ç¯„ä¾‹ 5ï¼šå®Œæ•´çš„ CLI å·¥å…·

#### build_index.py - å»ºç«‹ç´¢å¼•

```python
#!/usr/bin/env python3
"""
å»ºç«‹åœ–ç‰‡ç´¢å¼•
ç”¨æ³•: python build_index.py --images ./photos --output index.npz --model mobileclip_s1
"""

import argparse
import torch
import mobileclip
from PIL import Image
import numpy as np
from pathlib import Path
from tqdm import tqdm

def main():
    parser = argparse.ArgumentParser(description='å»ºç«‹åœ–ç‰‡ç´¢å¼•')
    parser.add_argument('--images', required=True, help='åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘')
    parser.add_argument('--output', default='index.npz', help='è¼¸å‡ºç´¢å¼•æª”æ¡ˆ')
    parser.add_argument('--model', default='mobileclip_s1', 
                       choices=['mobileclip_s0', 'mobileclip_s1', 'mobileclip_s2', 
                               'mobileclip_b', 'mobileclip_blt'],
                       help='ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    args = parser.parse_args()
    
    print("="*60)
    print("MobileCLIP åœ–ç‰‡ç´¢å¼•å»ºç«‹å·¥å…·")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥æ¨¡å‹: {args.model}")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        args.model.replace('mobileclip_', ''),
        pretrained=f'checkpoints/{args.model}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    print(f"âœ“ ä½¿ç”¨è£ç½®: {device}")
    
    # æƒæåœ–ç‰‡
    print(f"\nğŸ“‚ æƒæåœ–ç‰‡è³‡æ–™å¤¾: {args.images}")
    image_paths = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.bmp']:
        image_paths.extend(Path(args.images).glob(ext))
    image_paths = [str(p) for p in image_paths]
    print(f"âœ“ æ‰¾åˆ° {len(image_paths)} å¼µåœ–ç‰‡")
    
    if len(image_paths) == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡ï¼Œè«‹æª¢æŸ¥è·¯å¾‘")
        return
    
    # æå–ç‰¹å¾µ
    print(f"\nğŸ¨ æå–ç‰¹å¾µï¼ˆbatch_size={args.batch_size}ï¼‰...")
    all_features = []
    valid_paths = []
    
    for i in tqdm(range(0, len(image_paths), args.batch_size)):
        batch_paths = image_paths[i:i+args.batch_size]
        batch_images = []
        batch_valid = []
        
        for img_path in batch_paths:
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = preprocess(img)
                batch_images.append(img_tensor)
                batch_valid.append(img_path)
            except:
                continue
        
        if batch_images:
            batch_tensor = torch.stack(batch_images).to(device)
            with torch.no_grad():
                features = model.encode_image(batch_tensor)
                features = features / features.norm(dim=-1, keepdim=True)
            all_features.append(features.cpu().numpy())
            valid_paths.extend(batch_valid)
    
    # åˆä½µä¸¦å„²å­˜
    features_matrix = np.vstack(all_features)
    
    print(f"\nğŸ’¾ å„²å­˜ç´¢å¼•...")
    np.savez(args.output,
             features=features_matrix,
             paths=valid_paths,
             model_name=args.model)
    
    print(f"\n{'='*60}")
    print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"è¼¸å‡ºæª”æ¡ˆ: {args.output}")
    print(f"ç‰¹å¾µçŸ©é™£: {features_matrix.shape}")
    print(f"æˆåŠŸè™•ç†: {len(valid_paths)} å¼µåœ–ç‰‡")
    print(f"å¤±æ•—: {len(image_paths) - len(valid_paths)} å¼µåœ–ç‰‡")

if __name__ == '__main__':
    main()
```

#### search.py - æœå°‹ç›¸ä¼¼åœ–ç‰‡

```python
#!/usr/bin/env python3
"""
æœå°‹ç›¸ä¼¼åœ–ç‰‡
ç”¨æ³•: python search.py --query cat.jpg --index index.npz --top 5
"""

import argparse
import torch
import mobileclip
from PIL import Image
import numpy as np

def main():
    parser = argparse.ArgumentParser(description='æœå°‹ç›¸ä¼¼åœ–ç‰‡')
    parser.add_argument('--query', required=True, help='æŸ¥è©¢åœ–ç‰‡è·¯å¾‘')
    parser.add_argument('--index', required=True, help='ç´¢å¼•æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--top', type=int, default=5, help='è¿”å›çµæœæ•¸é‡')
    parser.add_argument('--visualize', action='store_true', help='è¦–è¦ºåŒ–çµæœ')
    args = parser.parse_args()
    
    print("="*60)
    print("MobileCLIP ä»¥åœ–æ‰¾åœ–å·¥å…·")
    print("="*60)
    
    # è¼‰å…¥ç´¢å¼•
    print(f"\nğŸ“‚ è¼‰å…¥ç´¢å¼•: {args.index}")
    data = np.load(args.index, allow_pickle=True)
    index_features = data['features']
    image_paths = data['paths'].tolist()
    model_name = str(data['model_name'])
    
    print(f"âœ“ è¼‰å…¥ {len(image_paths)} å¼µåœ–ç‰‡")
    print(f"âœ“ ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # è¼‰å…¥æ¨¡å‹
    print(f"\nğŸ“¦ è¼‰å…¥æ¨¡å‹...")
    model, _, preprocess = mobileclip.create_model_and_transforms(
        model_name.replace('mobileclip_', ''),
        pretrained=f'checkpoints/{model_name}.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # æå–æŸ¥è©¢ç‰¹å¾µ
    print(f"\nğŸ” åˆ†ææŸ¥è©¢åœ–ç‰‡: {args.query}")
    query_img = Image.open(args.query).convert('RGB')
    query_tensor = preprocess(query_img).unsqueeze(0).to(device)
    
    with torch.no_grad():
        query_feat = model.encode_image(query_tensor)
        query_feat = query_feat / query_feat.norm(dim=-1, keepdim=True)
    
    query_feat = query_feat.cpu().numpy()
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    print(f"\nğŸ“Š è¨ˆç®—ç›¸ä¼¼åº¦...")
    similarities = np.dot(index_features, query_feat.T).squeeze()
    top_indices = np.argsort(similarities)[::-1][:args.top]
    
    # é¡¯ç¤ºçµæœ
    print(f"\n{'='*60}")
    print(f"Top-{args.top} æœ€ç›¸ä¼¼åœ–ç‰‡:")
    print(f"{'='*60}\n")
    
    results = []
    for i, idx in enumerate(top_indices):
        path = image_paths[idx]
        score = similarities[idx]
        print(f"{i+1}. {path}")
        print(f"   ç›¸ä¼¼åº¦: {score:.4f} ({score*100:.2f}%)\n")
        results.append((path, float(score)))
    
    # è¦–è¦ºåŒ–ï¼ˆå¯é¸ï¼‰
    if args.visualize:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, args.top+1, figsize=(3*(args.top+1), 3))
        
        # æŸ¥è©¢åœ–ç‰‡
        axes[0].imshow(query_img)
        axes[0].set_title('Query', fontweight='bold')
        axes[0].axis('off')
        
        # çµæœ
        for i, (path, score) in enumerate(results):
            img = Image.open(path)
            axes[i+1].imshow(img)
            axes[i+1].set_title(f'#{i+1}: {score:.3f}')
            axes[i+1].axis('off')
        
        plt.tight_layout()
        plt.savefig('search_results.png', dpi=150, bbox_inches='tight')
        print(f"âœ“ è¦–è¦ºåŒ–çµæœå·²å„²å­˜: search_results.png")
        plt.show()

if __name__ == '__main__':
    main()
```

#### ä½¿ç”¨ç¯„ä¾‹

```bash
# å»ºç«‹ç´¢å¼•
python build_index.py --images ./my_photos --output photos.npz --model mobileclip_s1

# æœå°‹ç›¸ä¼¼åœ–ç‰‡
python search.py --query ./cat.jpg --index photos.npz --top 5

# æœå°‹ä¸¦è¦–è¦ºåŒ–
python search.py --query ./cat.jpg --index photos.npz --top 5 --visualize
```

---

## æ•ˆèƒ½å„ªåŒ–æŠ€å·§

### âš¡ 1. ä½¿ç”¨ GPU åŠ é€Ÿ

```python
# æª¢æŸ¥ä¸¦ä½¿ç”¨ GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

# ç¢ºèªæ˜¯å¦ä½¿ç”¨ GPU
print(f"ä½¿ç”¨è£ç½®: {device}")
print(f"GPU åç¨±: {torch.cuda.get_device_name(0)}" if torch.cuda.is_available() else "")
```

### âš¡ 2. æ‰¹æ¬¡è™•ç†ï¼ˆé€Ÿåº¦æå‡ 5-10 å€ï¼‰

```python
# âŒ ä¸å¥½ï¼šä¸€å¼µå¼µè™•ç†
for img_path in image_paths:
    tensor = preprocess(Image.open(img_path)).unsqueeze(0)
    features = model.encode_image(tensor)

# âœ… å¥½ï¼šæ‰¹æ¬¡è™•ç†
batch_size = 32
for i in range(0, len(image_paths), batch_size):
    batch = [preprocess(Image.open(p)) for p in image_paths[i:i+batch_size]]
    batch_tensor = torch.stack(batch)
    batch_features = model.encode_image(batch_tensor)  # å¿«å¾ˆå¤šï¼
```

### âš¡ 3. ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆFP16ï¼‰

```python
# ä½¿ç”¨è‡ªå‹•æ··åˆç²¾åº¦ï¼ˆAMPï¼‰
with torch.cuda.amp.autocast():
    image_features = model.encode_image(image_tensor)

# é€Ÿåº¦æå‡ç´„ 2 å€ï¼Œè¨˜æ†¶é«”æ¸›å°‘ç´„ 50%
```

### âš¡ 4. ä¸è¨ˆç®—æ¢¯åº¦

```python
# æ¨è«–æ™‚å¿…é ˆä½¿ç”¨
with torch.no_grad():
    image_features = model.encode_image(image_tensor)

# ç¯€çœè¨˜æ†¶é«”å’Œè¨ˆç®—æ™‚é–“
```

### âš¡ 5. é å…ˆè¨ˆç®—ä¸¦å¿«å–ç‰¹å¾µ

```python
# ä¸€æ¬¡æ€§å»ºç«‹ç´¢å¼•
features = extract_all_features(image_folder)
np.save('features_cache.npy', features)

# ä¹‹å¾Œç›´æ¥è¼‰å…¥
features = np.load('features_cache.npy')

# é¿å…é‡è¤‡æå–ç‰¹å¾µ
```

### âš¡ 6. ä½¿ç”¨ DataLoaderï¼ˆå¤§è¦æ¨¡è³‡æ–™ï¼‰

```python
from torch.utils.data import Dataset, DataLoader

class ImageDataset(Dataset):
    def __init__(self, image_paths, transform):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img = Image.open(self.image_paths[idx]).convert('RGB')
        return self.transform(img), self.image_paths[idx]

# ä½¿ç”¨ DataLoader
dataset = ImageDataset(image_paths, preprocess)
dataloader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)

for images, paths in dataloader:
    images = images.to(device)
    with torch.no_grad():
        features = model.encode_image(images)
```

---

## å¸¸è¦‹å•é¡Œè™•ç†

### ğŸ› éŒ¯èª¤ 1: å¿˜è¨˜åŠ  batch ç¶­åº¦

```python
# âŒ éŒ¯èª¤
image_tensor = preprocess(image)  # shape: (3, H, W)
features = model.encode_image(image_tensor)  # å ±éŒ¯ï¼

# âœ… æ­£ç¢º
image_tensor = preprocess(image).unsqueeze(0)  # shape: (1, 3, H, W)
features = model.encode_image(image_tensor)
```

### ğŸ› éŒ¯èª¤ 2: å¿˜è¨˜è½‰ RGB

```python
# âŒ éŒ¯èª¤ï¼ˆPNG å¯èƒ½æ˜¯ RGBAï¼Œç°éšåœ–æ˜¯ Lï¼‰
image = Image.open('image.png')
features = model.encode_image(preprocess(image).unsqueeze(0))  # å¯èƒ½å ±éŒ¯

# âœ… æ­£ç¢º
image = Image.open('image.png').convert('RGB')  # å¼·åˆ¶è½‰æˆ RGB
features = model.encode_image(preprocess(image).unsqueeze(0))
```

### ğŸ› éŒ¯èª¤ 3: å¿˜è¨˜ L2 æ­¸ä¸€åŒ–

```python
# âŒ éŒ¯èª¤ï¼ˆç›¸ä¼¼åº¦è¨ˆç®—ä¸æº–ç¢ºï¼‰
features = model.encode_image(image_tensor)
similarity = features @ features.T

# âœ… æ­£ç¢º
features = model.encode_image(image_tensor)
features = features / features.norm(dim=-1, keepdim=True)  # L2 æ­¸ä¸€åŒ–
similarity = features @ features.T  # æ­£ç¢ºçš„é¤˜å¼¦ç›¸ä¼¼åº¦
```

### ğŸ› éŒ¯èª¤ 4: è£ç½®ä¸åŒ¹é…

```python
# âŒ éŒ¯èª¤
model.to('cuda')
image_tensor = preprocess(image).unsqueeze(0)  # åœ¨ CPU
features = model.encode_image(image_tensor)  # å ±éŒ¯ï¼štensor ä¸åœ¨åŒä¸€è£ç½®

# âœ… æ­£ç¢º
model.to(device)
image_tensor = preprocess(image).unsqueeze(0).to(device)  # ç§»åˆ°åŒä¸€è£ç½®
features = model.encode_image(image_tensor)
```

### ğŸ› éŒ¯èª¤ 5: è¨˜æ†¶é«”ä¸è¶³ï¼ˆOOMï¼‰

```python
# è§£æ±ºæ–¹æ³• 1: æ¸›å°‘ batch size
batch_size = 16  # åŸæœ¬ 32ï¼Œæ”¹æˆ 16

# è§£æ±ºæ–¹æ³• 2: æ¸…ç† GPU è¨˜æ†¶é«”
torch.cuda.empty_cache()

# è§£æ±ºæ–¹æ³• 3: ä½¿ç”¨æ¢¯åº¦ç´¯ç©
with torch.no_grad():  # ä¸è¨ˆç®—æ¢¯åº¦
    features = model.encode_image(image_tensor)

# è§£æ±ºæ–¹æ³• 4: ä½¿ç”¨ CPU
device = 'cpu'  # æ”¹ç”¨ CPUï¼ˆè¼ƒæ…¢ä½†ä¸æœƒ OOMï¼‰
```

### ğŸ› å•é¡Œ 6: åœ–ç‰‡è®€å–å¤±æ•—

```python
# å¥å£¯çš„åœ–ç‰‡è®€å–
def load_image_safely(image_path):
    try:
        img = Image.open(image_path).convert('RGB')
        return img
    except Exception as e:
        print(f"âš  ç„¡æ³•è®€å– {image_path}: {e}")
        return None

# ä½¿ç”¨
img = load_image_safely('image.jpg')
if img is not None:
    features = extract_features(img)
```

### ğŸ”§ é™¤éŒ¯æŠ€å·§

```python
# 1. æª¢æŸ¥ tensor shape
print(f"Image tensor shape: {image_tensor.shape}")  # æ‡‰è©²æ˜¯ (1, 3, H, W)
print(f"Features shape: {features.shape}")  # æ‡‰è©²æ˜¯ (1, 512)

# 2. æª¢æŸ¥ç‰¹å¾µå‘é‡æ˜¯å¦æ­¸ä¸€åŒ–
print(f"Feature norm: {torch.norm(features)}")  # æ‡‰è©²æ¥è¿‘ 1.0

# 3. æª¢æŸ¥è£ç½®
print(f"Model device: {next(model.parameters()).device}")
print(f"Tensor device: {image_tensor.device}")

# 4. è¦–è¦ºåŒ–ç›¸ä¼¼åº¦çŸ©é™£
import matplotlib.pyplot as plt
similarity_matrix = features @ features.T
plt.imshow(similarity_matrix.cpu().numpy())
plt.colorbar()
plt.title('Similarity Matrix')
plt.show()
```

---

## ğŸ“ å®Œæ•´æ¸¬è©¦è…³æœ¬

å°‡ä»¥ä¸‹ç¨‹å¼ç¢¼å„²å­˜ç‚º `test_mobileclip.py`ï¼š

```python
#!/usr/bin/env python3
"""
MobileCLIP å®Œæ•´æ¸¬è©¦è…³æœ¬
"""

import torch
import mobileclip
from PIL import Image
import numpy as np

def test_single_image():
    """æ¸¬è©¦å–®å¼µåœ–ç‰‡ç‰¹å¾µæå–"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 1: å–®å¼µåœ–ç‰‡ç‰¹å¾µæå–")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        'mobileclip_s1',
        pretrained='checkpoints/mobileclip_s1.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥åˆ° {device}")
    
    # æ¸¬è©¦åœ–ç‰‡
    image_path = "test.jpg"  # æ›¿æ›æˆä½ çš„åœ–ç‰‡
    image = Image.open(image_path).convert('RGB')
    
    print(f"âœ“ åœ–ç‰‡å¤§å°: {image.size}")
    
    # æå–ç‰¹å¾µ
    image_tensor = preprocess(image).unsqueeze(0).to(device)
    
    with torch.no_grad():
        features = model.encode_image(image_tensor)
        features = features / features.norm(dim=-1, keepdim=True)
    
    print(f"âœ“ ç‰¹å¾µ shape: {features.shape}")
    print(f"âœ“ ç‰¹å¾µ norm: {torch.norm(features).item():.4f}")
    print(f"âœ“ ç‰¹å¾µå‰ 5 ç¶­: {features[0, :5]}")


def test_similarity():
    """æ¸¬è©¦ç›¸ä¼¼åº¦è¨ˆç®—"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 2: ç›¸ä¼¼åº¦è¨ˆç®—")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        'mobileclip_s1',
        pretrained='checkpoints/mobileclip_s1.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # å…©å¼µæ¸¬è©¦åœ–ç‰‡
    image1 = Image.open("test1.jpg").convert('RGB')
    image2 = Image.open("test2.jpg").convert('RGB')
    
    # æå–ç‰¹å¾µ
    tensor1 = preprocess(image1).unsqueeze(0).to(device)
    tensor2 = preprocess(image2).unsqueeze(0).to(device)
    
    with torch.no_grad():
        feat1 = model.encode_image(tensor1)
        feat2 = model.encode_image(tensor2)
        
        feat1 = feat1 / feat1.norm(dim=-1, keepdim=True)
        feat2 = feat2 / feat2.norm(dim=-1, keepdim=True)
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    similarity = (feat1 @ feat2.T).item()
    
    print(f"âœ“ åœ–ç‰‡ 1 ç‰¹å¾µ: {feat1.shape}")
    print(f"âœ“ åœ–ç‰‡ 2 ç‰¹å¾µ: {feat2.shape}")
    print(f"âœ“ é¤˜å¼¦ç›¸ä¼¼åº¦: {similarity:.4f} ({similarity*100:.2f}%)")


def test_batch_processing():
    """æ¸¬è©¦æ‰¹æ¬¡è™•ç†"""
    print("\n" + "="*60)
    print("æ¸¬è©¦ 3: æ‰¹æ¬¡è™•ç†")
    print("="*60)
    
    # è¼‰å…¥æ¨¡å‹
    model, _, preprocess = mobileclip.create_model_and_transforms(
        'mobileclip_s1',
        pretrained='checkpoints/mobileclip_s1.pt'
    )
    model.eval()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    
    # æ‰¹æ¬¡åœ–ç‰‡
    image_paths = ["test1.jpg", "test2.jpg", "test3.jpg"]
    images = [Image.open(p).convert('RGB') for p in image_paths]
    
    # æ‰¹æ¬¡è™•ç†
    batch_tensor = torch.stack([preprocess(img) for img in images]).to(device)
    
    print(f"âœ“ Batch shape: {batch_tensor.shape}")
    
    with torch.no_grad():
        batch_features = model.encode_image(batch_tensor)
        batch_features = batch_features / batch_features.norm(dim=-1, keepdim=True)
    
    print(f"âœ“ Batch features shape: {batch_features.shape}")
    print(f"âœ“ æ¯å¼µåœ–ç‰‡ç‰¹å¾µ norm: {torch.norm(batch_features, dim=-1)}")


def main():
    print("\n" + "ğŸš€"*30)
    print("MobileCLIP å®Œæ•´æ¸¬è©¦")
    print("ğŸš€"*30)
    
    try:
        test_single_image()
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ 1 å¤±æ•—: {e}")
    
    try:
        test_similarity()
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ 2 å¤±æ•—: {e}")
    
    try:
        test_batch_processing()
    except Exception as e:
        print(f"âŒ æ¸¬è©¦ 3 å¤±æ•—: {e}")
    
    print("\n" + "âœ…"*30)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("âœ…"*30 + "\n")


if __name__ == '__main__':
    main()
```

åŸ·è¡Œæ¸¬è©¦ï¼š
```bash
python test_mobileclip.py
```

---

## ğŸ“ å­¸ç¿’è·¯å¾‘å»ºè­°

### éšæ®µ 1: åŸºç¤ï¼ˆ1-2 å¤©ï¼‰
1. âœ… å®‰è£ç’°å¢ƒå’Œä¸‹è¼‰æ¨¡å‹
2. âœ… è·‘é€šç¯„ä¾‹ 1ï¼ˆå–®å¼µåœ–ç‰‡ï¼‰
3. âœ… ç†è§£ `preprocess` å’Œ `unsqueeze` çš„ä½œç”¨
4. âœ… æ¸¬è©¦ä¸åŒæ¨¡å‹ï¼ˆS0, S1, S2ï¼‰

### éšæ®µ 2: å¯¦æˆ°ï¼ˆ3-5 å¤©ï¼‰
1. âœ… å¯¦ä½œç¯„ä¾‹ 2ï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰
2. âœ… å¯¦ä½œç¯„ä¾‹ 3ï¼ˆä»¥åœ–æ‰¾åœ–ï¼‰
3. âœ… å»ºç«‹è‡ªå·±çš„åœ–ç‰‡ç´¢å¼•
4. âœ… æ¸¬è©¦æœå°‹åŠŸèƒ½

### éšæ®µ 3: å„ªåŒ–ï¼ˆ2-3 å¤©ï¼‰
1. âœ… å¯¦ä½œ CLI å·¥å…·
2. âœ… æ•ˆèƒ½å„ªåŒ–ï¼ˆGPUã€æ‰¹æ¬¡ï¼‰
3. âœ… è¦–è¦ºåŒ–æœå°‹çµæœ
4. âœ… éŒ¯èª¤è™•ç†å’Œå¥å£¯æ€§

### éšæ®µ 4: Android æº–å‚™ï¼ˆ3-5 å¤©ï¼‰
1. âœ… æ¨¡å‹è½‰æ›ï¼ˆTorchScriptï¼‰
2. âœ… é‡åŒ–æ¸¬è©¦ï¼ˆINT8ï¼‰
3. âœ… CPU æ•ˆèƒ½æ¸¬è©¦
4. âœ… æ’°å¯«ç§»æ¤æ–‡ä»¶

---

## ğŸ“š åƒè€ƒè³‡æº

- **å®˜æ–¹ GitHub**: https://github.com/apple/ml-mobileclip
- **è«–æ–‡**: [MobileCLIP: Fast Image-Text Models](https://arxiv.org/pdf/2311.17049.pdf)
- **HuggingFace æ¨¡å‹**: [MobileCLIP Collection](https://huggingface.co/collections/apple/mobileclip-models-datacompdr-data-665789776e1aa2b59f35f7c8)
- **PyTorch å®˜æ–¹æ–‡æª”**: https://pytorch.org/docs/stable/index.html

---

## ğŸ’¡ å¿«é€Ÿåƒè€ƒ

### æ ¸å¿ƒç¨‹å¼ç¢¼ç‰‡æ®µ

```python
# è¼‰å…¥æ¨¡å‹
model, _, preprocess = mobileclip.create_model_and_transforms(
    'mobileclip_s1', pretrained='checkpoints/mobileclip_s1.pt'
)
model.eval()
model.to('cuda' if torch.cuda.is_available() else 'cpu')

# å–®å¼µåœ–ç‰‡
image = Image.open('cat.jpg').convert('RGB')
tensor = preprocess(image).unsqueeze(0).to(device)
with torch.no_grad():
    features = model.encode_image(tensor)
    features = features / features.norm(dim=-1, keepdim=True)

# ç›¸ä¼¼åº¦è¨ˆç®—
similarity = (features1 @ features2.T).item()
```

---

**ç¥æ‚¨ä½¿ç”¨é †åˆ©ï¼æœ‰å•é¡Œéš¨æ™‚æŸ¥é–±æœ¬æŒ‡å— ğŸš€**
