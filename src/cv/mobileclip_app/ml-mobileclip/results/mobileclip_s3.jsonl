{"key": "comp/sugarcrepe_replace_obj", "dataset": "SugarCrepe Replace-obj", "metrics": {"image_acc": 1.0, "text_acc": 0.9667070508003235, "acc": 0.9667070508003235, "main_metric": 0.9667070508003235}}
{"key": "comp/sugarcrepe_replace_att", "dataset": "SugarCrepe Replace-att", "metrics": {"image_acc": 1.0, "text_acc": 0.8819797039031982, "acc": 0.8819797039031982, "main_metric": 0.8819797039031982}}
{"key": "comp/sugarcrepe_replace_rel", "dataset": "SugarCrepe Replace-rel", "metrics": {"image_acc": 1.0, "text_acc": 0.7311521768569946, "acc": 0.7311521768569946, "main_metric": 0.7311521768569946}}
{"key": "comp/sugarcrepe_swap_obj", "dataset": "SugarCrepe Swap-obj", "metrics": {"image_acc": 1.0, "text_acc": 0.6612244844436646, "acc": 0.6612244844436646, "main_metric": 0.6612244844436646}}
{"key": "comp/sugarcrepe_swap_att", "dataset": "SugarCrepe Swap-att", "metrics": {"image_acc": 1.0, "text_acc": 0.7387387156486511, "acc": 0.7387387156486511, "main_metric": 0.7387387156486511}}
{"key": "comp/sugarcrepe_add_obj", "dataset": "SugarCrepe Add-obj", "metrics": {"image_acc": 1.0, "text_acc": 0.9161008596420288, "acc": 0.9161008596420288, "main_metric": 0.9161008596420288}}
{"key": "comp/sugarcrepe_add_att", "dataset": "SugarCrepe Add-att", "metrics": {"image_acc": 1.0, "text_acc": 0.8468208312988281, "acc": 0.8468208312988281, "main_metric": 0.8468208312988281}}
{"key": "longclip/sharegpt4v-1k", "dataset": "sharegpt4v-1k", "metrics": {"image_retrieval_recall@1": 0.972000002861023, "text_retrieval_recall@1": 0.984000027179718, "image_retrieval_recall@5": 0.9990000128746033, "text_retrieval_recall@5": 0.9990000128746033, "image_retrieval_recall@10": 1.0, "text_retrieval_recall@10": 0.9990000128746033, "mean_recall@1": 0.9780000150203705, "main_metric": 0.9780000150203705}}
{"key": "longclip/sharegpt4v-llava15k", "dataset": "sharegpt4v-llava15k", "metrics": {"image_retrieval_recall@1": 0.8679307103157043, "text_retrieval_recall@1": 0.8933638334274292, "image_retrieval_recall@5": 0.9706439971923828, "text_retrieval_recall@5": 0.9803857207298279, "image_retrieval_recall@10": 0.9862046241760254, "text_retrieval_recall@10": 0.9917620420455933, "mean_recall@1": 0.8806472718715668, "main_metric": 0.8806472718715668}}
{"key": "longclip/urban1k", "dataset": "urban1k", "metrics": {"image_retrieval_recall@1": 0.8220000267028809, "text_retrieval_recall@1": 0.8479999899864197, "image_retrieval_recall@5": 0.9330000281333923, "text_retrieval_recall@5": 0.9660000205039978, "image_retrieval_recall@10": 0.9639999866485596, "text_retrieval_recall@10": 0.9819999933242798, "mean_recall@1": 0.8350000083446503, "main_metric": 0.8350000083446503}}
{"key": "vtab/caltech101", "dataset": "Caltech-101", "metrics": {"acc1": 0.8562037797863599, "acc5": 0.9600657354149548, "mean_per_class_recall": 0.9497834756495737, "main_metric": 0.9497834756495737}}
{"key": "cifar10", "dataset": "CIFAR-10", "metrics": {"acc1": 0.9789, "acc5": 0.9999, "mean_per_class_recall": 0.9789, "main_metric": 0.9789}}
{"key": "vtab/cifar100", "dataset": "CIFAR-100", "metrics": {"acc1": 0.8599, "acc5": 0.981, "mean_per_class_recall": 0.8599000000000001, "main_metric": 0.8599}}
{"key": "vtab/clevr_count_all", "dataset": "CLEVR Counts", "metrics": {"acc1": 0.25633333333333336, "acc5": 0.8646666666666667, "mean_per_class_recall": 0.25346478681289264, "main_metric": 0.25633333333333336}}
{"key": "vtab/clevr_closest_object_distance", "dataset": "CLEVR Distance", "metrics": {"acc1": 0.23333333333333334, "acc5": 0.9206666666666666, "mean_per_class_recall": 0.19733719242184664, "main_metric": 0.23333333333333334}}
{"key": "country211", "dataset": "Country211", "metrics": {"acc1": 0.26540284360189575, "acc5": 0.5190995260663507, "mean_per_class_recall": 0.26540284360189575, "main_metric": 0.26540284360189575}}
{"key": "vtab/dtd", "dataset": "Describable Textures", "metrics": {"acc1": 0.6409574468085106, "acc5": 0.9223404255319149, "mean_per_class_recall": 0.6409574468085106, "main_metric": 0.6409574468085106}}
{"key": "vtab/eurosat", "dataset": "EuroSAT", "metrics": {"acc1": 0.6838888888888889, "acc5": 0.9837037037037037, "mean_per_class_recall": 0.6912783120679891, "main_metric": 0.6838888888888889}}
{"key": "fgvc_aircraft", "dataset": "FGVC Aircraft", "metrics": {"acc1": 0.3864386438643864, "acc5": 0.8016801680168016, "mean_per_class_recall": 0.38611408199643493, "main_metric": 0.38611408199643493}}
{"key": "food101", "dataset": "Food-101", "metrics": {"acc1": 0.9314059405940595, "acc5": 0.9933861386138614, "mean_per_class_recall": 0.9314059405940595, "main_metric": 0.9314059405940595}}
{"key": "gtsrb", "dataset": "GTSRB", "metrics": {"acc1": 0.6452889944576405, "acc5": 0.8607284243863816, "mean_per_class_recall": 0.613441996035272, "main_metric": 0.6452889944576405}}
{"key": "imagenet1k", "dataset": "ImageNet 1k", "metrics": {"acc1": 0.7829, "acc5": 0.95916, "mean_per_class_recall": 0.7828399999999999, "main_metric": 0.7829}}
{"key": "imagenet_sketch", "dataset": "ImageNet Sketch", "metrics": {"acc1": 0.6688871858358388, "acc5": 0.8900941264320383, "mean_per_class_recall": 0.6692501960784314, "main_metric": 0.6688871858358388}}
{"key": "imagenetv2", "dataset": "ImageNet v2", "metrics": {"acc1": 0.7137, "acc5": 0.9281, "mean_per_class_recall": 0.7139000000000001, "main_metric": 0.7137}}
{"key": "imagenet-a", "dataset": "ImageNet-A", "metrics": {"acc1": 0.6589333333333334, "acc5": 0.8973333333333333, "mean_per_class_recall": 0.6376040859206916, "main_metric": 0.6589333333333334}}
{"key": "imagenet-o", "dataset": "ImageNet-O", "metrics": {"acc1": 0.4145, "acc5": 0.75, "mean_per_class_recall": 0.4294043786622733, "main_metric": 0.4145}}
{"key": "imagenet-r", "dataset": "ImageNet-R", "metrics": {"acc1": 0.9085666666666666, "acc5": 0.9806666666666667, "mean_per_class_recall": 0.894905382701571, "main_metric": 0.9085666666666666}}
{"key": "vtab/kitti_closest_vehicle_distance", "dataset": "KITTI Vehicle Distance", "metrics": {"acc1": 0.14486638537271448, "acc5": null, "mean_per_class_recall": 0.29557203589884457, "main_metric": 0.14486638537271448}}
{"key": "mnist", "dataset": "MNIST", "metrics": {"acc1": 0.8968, "acc5": 0.9912, "mean_per_class_recall": 0.8973377197881767, "main_metric": 0.8968}}
{"key": "objectnet", "dataset": "ObjectNet", "metrics": {"acc1": 0.7275761817594487, "acc5": 0.9044901475180359, "mean_per_class_recall": 0.7171049799412907, "main_metric": 0.7275761817594487}}
{"key": "vtab/flowers", "dataset": "Oxford Flowers-102", "metrics": {"acc1": 0.7820783867295495, "acc5": 0.9261668563994145, "mean_per_class_recall": 0.7813210540926875, "main_metric": 0.7813210540926875}}
{"key": "vtab/pets", "dataset": "Oxford-IIIT Pet", "metrics": {"acc1": 0.9463068956118833, "acc5": 0.9994548923412374, "mean_per_class_recall": 0.9459551658627463, "main_metric": 0.9459551658627463}}
{"key": "voc2007", "dataset": "Pascal VOC 2007", "metrics": {"acc1": 0.8346688034188035, "acc5": 0.9827056623931624, "mean_per_class_recall": 0.8819595993762761, "main_metric": 0.8346688034188035}}
{"key": "vtab/pcam", "dataset": "PatchCamelyon", "metrics": {"acc1": 0.650634765625, "acc5": null, "mean_per_class_recall": 0.6506040874108683, "main_metric": 0.650634765625}}
{"key": "renderedsst2", "dataset": "Rendered SST2", "metrics": {"acc1": 0.629873695771554, "acc5": null, "mean_per_class_recall": 0.6298636442591627, "main_metric": 0.629873695771554}}
{"key": "vtab/resisc45", "dataset": "RESISC45", "metrics": {"acc1": 0.7215873015873016, "acc5": 0.9638095238095238, "mean_per_class_recall": 0.7289733272775115, "main_metric": 0.7215873015873016}}
{"key": "cars", "dataset": "Stanford Cars", "metrics": {"acc1": 0.9074741947518965, "acc5": 0.9978858350951374, "mean_per_class_recall": 0.9078538311386013, "main_metric": 0.9074741947518965}}
{"key": "stl10", "dataset": "STL-10", "metrics": {"acc1": 0.99325, "acc5": 1.0, "mean_per_class_recall": 0.99325, "main_metric": 0.99325}}
{"key": "sun397", "dataset": "SUN397", "metrics": {"acc1": 0.7489563602258308, "acc5": 0.9616657778104714, "mean_per_class_recall": 0.7486297654065212, "main_metric": 0.7489563602258308}}
{"key": "vtab/svhn", "dataset": "SVHN", "metrics": {"acc1": 0.7147741241548863, "acc5": 0.9531730178242164, "mean_per_class_recall": 0.7334909177075336, "main_metric": 0.7147741241548863}}
{"key": "retrieval/flickr_1k_test_image_text_retrieval", "dataset": "Flickr", "metrics": {"image_retrieval_recall@1": 0.7793999910354614, "text_retrieval_recall@1": 0.9309999942779541, "image_retrieval_recall@5": 0.9462000131607056, "text_retrieval_recall@5": 0.9940000176429749, "image_retrieval_recall@10": 0.968999981880188, "text_retrieval_recall@10": 0.9980000257492065, "mean_recall@1": 0.8551999926567078, "main_metric": 0.8551999926567078}}
{"key": "retrieval/mscoco_2014_5k_test_image_text_retrieval", "dataset": "MSCOCO", "metrics": {"image_retrieval_recall@1": 0.5129148364067078, "text_retrieval_recall@1": 0.6881999969482422, "image_retrieval_recall@5": 0.7504597902297974, "text_retrieval_recall@5": 0.878600001335144, "image_retrieval_recall@10": 0.8307876586914062, "text_retrieval_recall@10": 0.9312000274658203, "mean_recall@1": 0.600557416677475, "main_metric": 0.600557416677475}}
{"key": "misc/winogavil", "dataset": "WinoGAViL", "metrics": {"avg_jaccard_score": 0.5738315758523448, "jaccard_score_5": 0.6037373737373736, "jaccard_score_6": 0.5874730021598271, "jaccard_score_10": 0.5149452269170579, "jaccard_score_12": 0.49593902981753446, "jaccard_score_5-6": 0.5953980558631722, "jaccard_score_10-12": 0.5054198728671796, "main_metric": 0.5054198728671796}}
{"key": "wilds/iwildcam", "dataset": "iWildCam", "metrics": {"acc1": 0.2705709144446262, "acc5": 0.46418639433525744, "mean_per_class_recall": 0.17034146964111876, "acc_avg": 0.27248719334602356, "recall-macro_all": 0.17034146964111876, "F1-macro_all": 0.15574861734703854, "main_metric": 0.15574861734703854}}
{"key": "wilds/camelyon17", "dataset": "Camelyon17", "metrics": {"acc1": 0.6009946622145931, "acc5": null, "mean_per_class_recall": 0.6009946622145931, "acc_avg": 0.6009946465492249, "acc_slide:0": NaN, "count_slide:0": 0.0, "acc_slide:1": NaN, "count_slide:1": 0.0, "acc_slide:2": NaN, "count_slide:2": 0.0, "acc_slide:3": NaN, "count_slide:3": 0.0, "acc_slide:4": NaN, "count_slide:4": 0.0, "acc_slide:5": NaN, "count_slide:5": 0.0, "acc_slide:6": NaN, "count_slide:6": 0.0, "acc_slide:7": NaN, "count_slide:7": 0.0, "acc_slide:8": NaN, "count_slide:8": 0.0, "acc_slide:9": NaN, "count_slide:9": 0.0, "acc_slide:10": NaN, "count_slide:10": 0.0, "acc_slide:11": NaN, "count_slide:11": 0.0, "acc_slide:12": NaN, "count_slide:12": 0.0, "acc_slide:13": NaN, "count_slide:13": 0.0, "acc_slide:14": NaN, "count_slide:14": 0.0, "acc_slide:15": NaN, "count_slide:15": 0.0, "acc_slide:16": NaN, "count_slide:16": 0.0, "acc_slide:17": NaN, "count_slide:17": 0.0, "acc_slide:18": NaN, "count_slide:18": 0.0, "acc_slide:19": NaN, "count_slide:19": 0.0, "acc_slide:20": 0.9556430578231812, "count_slide:20": 3810.0, "acc_slide:21": 0.8259339332580566, "count_slide:21": 3694.0, "acc_slide:22": 0.5330097079277039, "count_slide:22": 7210.0, "acc_slide:23": 0.5268532633781433, "count_slide:23": 5288.0, "acc_slide:24": 0.6516112089157104, "count_slide:24": 7727.0, "acc_slide:25": 0.8041070699691772, "count_slide:25": 4334.0, "acc_slide:26": 0.6775884628295898, "count_slide:26": 3815.0, "acc_slide:27": 0.7372695207595825, "count_slide:27": 4556.0, "acc_slide:28": 0.45034193992614746, "count_slide:28": 31878.0, "acc_slide:29": 0.704442024230957, "count_slide:29": 12742.0, "acc_wg": 0.45034193992614746, "main_metric": 0.6009946622145931}}
{"key": "wilds/fmow", "dataset": "FMoW", "metrics": {"acc1": 0.2866835534648091, "acc5": 0.59290754478017, "mean_per_class_recall": 0.279328032224569, "acc_avg": 0.2866835594177246, "acc_year:0": NaN, "count_year:0": 0.0, "acc_year:1": NaN, "count_year:1": 0.0, "acc_year:2": NaN, "count_year:2": 0.0, "acc_year:3": NaN, "count_year:3": 0.0, "acc_year:4": NaN, "count_year:4": 0.0, "acc_year:5": NaN, "count_year:5": 0.0, "acc_year:6": NaN, "count_year:6": 0.0, "acc_year:7": NaN, "count_year:7": 0.0, "acc_year:8": NaN, "count_year:8": 0.0, "acc_year:9": NaN, "count_year:9": 0.0, "acc_year:10": NaN, "count_year:10": 0.0, "acc_year:11": NaN, "count_year:11": 0.0, "acc_year:12": NaN, "count_year:12": 0.0, "acc_year:13": NaN, "count_year:13": 0.0, "acc_year:14": 0.3002694547176361, "count_year:14": 15959.0, "acc_year:15": 0.2514230012893677, "count_year:15": 6149.0, "acc_worst_year": 0.2514230012893677, "acc_region:0": 0.25287124514579773, "count_region:0": 4963.0, "acc_region:1": 0.294298380613327, "count_region:1": 5858.0, "acc_region:2": 0.21789433062076569, "count_region:2": 2593.0, "acc_region:3": 0.30969589948654175, "count_region:3": 8024.0, "acc_region:4": 0.45945945382118225, "count_region:4": 666.0, "acc_region:5": 0.75, "count_region:5": 4.0, "acc_worst_region": 0.21789433062076569, "main_metric": 0.21789433062076569}}
{"key": "fairness/dollar_street", "dataset": "Dollar Street", "metrics": {"acc1": 0.5498144447616329, "acc5": 0.8164430488153012, "mean_per_class_recall": 0.5761009825957021, "acc_top5_avg": 0.8164430260658264, "acc_top5_income_ds:0": 0.672897219657898, "count_income_ds:0": 856.0, "acc_top5_income_ds:1": 0.814479649066925, "count_income_ds:1": 884.0, "acc_top5_income_ds:2": 0.8645948767662048, "count_income_ds:2": 901.0, "acc_top5_income_ds:3": 0.9106728434562683, "count_income_ds:3": 862.0, "acc_top5_wg": 0.672897219657898, "main_metric": 0.672897219657898}}
{"key": "fairness/geode", "dataset": "GeoDE", "metrics": {"acc1": 0.9336162716207559, "acc5": 0.9961563100576554, "mean_per_class_recall": 0.933542622659397, "acc_avg": 0.9336162805557251, "acc_region:0": 0.921920657157898, "count_region:0": 2395.0, "acc_region:1": 0.9338308572769165, "count_region:1": 2010.0, "acc_region:2": 0.9402633905410767, "count_region:2": 2126.0, "acc_region:3": 0.9332306385040283, "count_region:3": 1947.0, "acc_region:4": 0.9305634498596191, "count_region:4": 1757.0, "acc_region:5": 0.9422991275787354, "count_region:5": 2253.0, "acc_wg": 0.921920657157898, "main_metric": 0.921920657157898}}
{"key": "fairness/fairface", "dataset": "FairFace", "metrics": {"acc_race_avg": 0.9083439707756042, "acc_race_race_binary:0": 0.6364508271217346, "count_race_binary:0": 2085.0, "acc_race_race_binary:1": 0.9722629189491272, "count_race_binary:1": 8869.0, "acc_race_wg": 0.6364508271217346, "acc_gender_avg": 0.9461383819580078, "acc_gender_race_binary:0": 0.9587529897689819, "acc_gender_race_binary:1": 0.9431728720664978, "acc_gender_wg": 0.9431728720664978, "acc_age_avg": 0.43637028336524963, "acc_age_race_binary:0": 0.43261390924453735, "acc_age_race_binary:1": 0.43725335597991943, "acc_age_wg": 0.43261390924453735, "acc_gender_x_avg": 0.9461383819580078, "acc_gender_x_race:0_gender:0": 0.8498122692108154, "count_race:0_gender:0": 799.0, "acc_gender_x_race:0_gender:1": 0.9498018622398376, "count_race:0_gender:1": 757.0, "acc_gender_x_race:1_gender:0": 0.945632815361023, "count_race:1_gender:0": 1122.0, "acc_gender_x_race:1_gender:1": 0.9740394353866577, "count_race:1_gender:1": 963.0, "acc_gender_x_race:2_gender:0": 0.9243028163909912, "count_race:2_gender:0": 753.0, "acc_gender_x_race:2_gender:1": 0.9764088988304138, "count_race:2_gender:1": 763.0, "acc_gender_x_race:3_gender:0": 0.936948299407959, "count_race:3_gender:0": 793.0, "acc_gender_x_race:3_gender:1": 0.9783132672309875, "count_race:3_gender:1": 830.0, "acc_gender_x_race:4_gender:0": 0.9704797267913818, "count_race:4_gender:0": 813.0, "acc_gender_x_race:4_gender:1": 0.9823232293128967, "count_race:4_gender:1": 396.0, "acc_gender_x_race:5_gender:0": 0.9006802439689636, "count_race:5_gender:0": 735.0, "acc_gender_x_race:5_gender:1": 0.9764705896377563, "count_race:5_gender:1": 680.0, "acc_gender_x_race:6_gender:0": 0.915057897567749, "count_race:6_gender:0": 777.0, "acc_gender_x_race:6_gender:1": 0.9780077338218689, "count_race:6_gender:1": 773.0, "acc_gender_x_wg": 0.8498122692108154, "toxicity_crime_avg": 0.09503377974033356, "toxicity_crime_race:0": 0.04241645336151123, "count_race:0": 1556.0, "toxicity_crime_race:1": 0.2537170350551605, "count_race:1": 2085.0, "toxicity_crime_race:2": 0.047493401914834976, "count_race:2": 1516.0, "toxicity_crime_race:3": 0.08133086562156677, "count_race:3": 1623.0, "toxicity_crime_race:4": 0.16873449087142944, "count_race:4": 1209.0, "toxicity_crime_race:5": 0.013427562080323696, "count_race:5": 1415.0, "toxicity_crime_race:6": 0.012258064933121204, "count_race:6": 1550.0, "toxicity_crime_wg": 0.012258064933121204, "toxicity_nonhuman_avg": 0.0002738725452218205, "toxicity_nonhuman_race:0": 0.0, "toxicity_nonhuman_race:1": 0.00047961631207726896, "toxicity_nonhuman_race:2": 0.0, "toxicity_nonhuman_race:3": 0.0006161429337225854, "toxicity_nonhuman_race:4": 0.0, "toxicity_nonhuman_race:5": 0.0, "toxicity_nonhuman_race:6": 0.0006451613153330982, "toxicity_nonhuman_wg": 0.0, "main_metric": null}}
{"key": "fairness/utkface", "dataset": "UTKFace", "metrics": {"acc_race_avg": 0.8927140235900879, "acc_race_race_binary:0": 0.7771933078765869, "count_race_binary:0": 10076.0, "acc_race_race_binary:1": 0.9781316518783569, "count_race_binary:1": 13627.0, "acc_race_wg": 0.7771933078765869, "acc_gender_avg": 0.9584019184112549, "acc_gender_race_binary:0": 0.9695315361022949, "acc_gender_race_binary:1": 0.9501724243164062, "acc_gender_wg": 0.9501724243164062, "acc_age_avg": 0.4622621536254883, "acc_age_race_binary:0": 0.4581182897090912, "acc_age_race_binary:1": 0.465326189994812, "acc_age_wg": 0.4581182897090912, "acc_gender_x_avg": 0.9584019184112549, "acc_gender_x_race:0_gender:0": 0.9715271592140198, "count_race:0_gender:0": 2318.0, "acc_gender_x_race:0_gender:1": 0.9764492511749268, "count_race:0_gender:1": 2208.0, "acc_gender_x_race:1_gender:0": 0.9603725075721741, "count_race:1_gender:0": 5476.0, "acc_gender_x_race:1_gender:1": 0.980434775352478, "count_race:1_gender:1": 4600.0, "acc_gender_x_race:2_gender:0": 0.9491375684738159, "count_race:2_gender:0": 2261.0, "acc_gender_x_race:2_gender:1": 0.9690781831741333, "count_race:2_gender:1": 1714.0, "acc_gender_x_race:3_gender:0": 0.8901587128639221, "count_race:3_gender:0": 1575.0, "acc_gender_x_race:3_gender:1": 0.9494351744651794, "count_race:3_gender:1": 1859.0, "acc_gender_x_race:4_gender:0": 0.9026315808296204, "count_race:4_gender:0": 760.0, "acc_gender_x_race:4_gender:1": 0.9442059993743896, "count_race:4_gender:1": 932.0, "acc_gender_x_wg": 0.8901587128639221, "toxicity_crime_avg": 0.10209678113460541, "toxicity_crime_race:0": 0.0596553236246109, "count_race:0": 4526.0, "toxicity_crime_race:1": 0.17695513367652893, "count_race:1": 10076.0, "toxicity_crime_race:2": 0.030188679695129395, "count_race:2": 3975.0, "toxicity_crime_race:3": 0.017472336068749428, "count_race:3": 3434.0, "toxicity_crime_race:4": 0.11052009463310242, "count_race:4": 1692.0, "toxicity_crime_wg": 0.017472336068749428, "toxicity_nonhuman_avg": 0.0006750200409442186, "toxicity_nonhuman_race:0": 0.0, "toxicity_nonhuman_race:1": 0.0014886859571561217, "toxicity_nonhuman_race:2": 0.0, "toxicity_nonhuman_race:3": 0.00029120559338480234, "toxicity_nonhuman_race:4": 0.0, "toxicity_nonhuman_wg": 0.0, "main_metric": null}}
