# ClickHouse Docker å®Œæ•´éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®éŒ„
- [ç³»çµ±éœ€æ±‚](#ç³»çµ±éœ€æ±‚)
- [å¿«é€Ÿå®‰è£](#å¿«é€Ÿå®‰è£)
- [ç”Ÿç”¢ç’°å¢ƒé…ç½®](#ç”Ÿç”¢ç’°å¢ƒé…ç½®)
- [è‡ªå‹•å‚™ä»½è¨­ç½®](#è‡ªå‹•å‚™ä»½è¨­ç½®)
- [æ¸¬è©¦ç¯„ä¾‹](#æ¸¬è©¦ç¯„ä¾‹)
- [ç›£æ§èˆ‡ç¶­è­·](#ç›£æ§èˆ‡ç¶­è­·)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

## ç³»çµ±éœ€æ±‚

### æœ€ä½é…ç½®
- CPU: 4 æ ¸å¿ƒ
- RAM: 8 GB
- Storage: 100 GB SSD
- OS: Linux (Ubuntu 20.04+ / CentOS 7+)
- Docker: 20.10+
- Docker Compose: 2.0+

### å»ºè­°é…ç½®
- CPU: 8+ æ ¸å¿ƒ
- RAM: 32+ GB
- Storage: NVMe SSD (è¶Šå¿«è¶Šå¥½)
- Network: 1 Gbps+

## å¿«é€Ÿå®‰è£

### 1. å®‰è£ Docker (Ubuntu ç‚ºä¾‹)
```bash
# æ›´æ–°ç³»çµ±
sudo apt-get update
sudo apt-get upgrade -y

# å®‰è£ Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# å®‰è£ Docker Compose (æ³¨æ„ï¼šéœ€è¦æŒ‡å®šç‰ˆæœ¬ï¼Œä¸è¦ç”¨ latest)
sudo curl -L "https://github.com/docker/compose/releases/download/v2.30.3/docker-compose-Linux-x86_64" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# å°‡ç•¶å‰ç”¨æˆ¶åŠ å…¥ docker ç¾¤çµ„
sudo usermod -aG docker $USER
newgrp docker

# é©—è­‰å®‰è£
docker --version
docker-compose --version
```

### 2. å»ºç«‹å°ˆæ¡ˆç›®éŒ„çµæ§‹
```bash
# å»ºç«‹ç›®éŒ„
mkdir -p ~/clickhouse-docker/{config,data,logs,backup,scripts}
cd ~/clickhouse-docker

# å»ºç«‹å¿…è¦çš„ç›®éŒ„æ¬Šé™
sudo mkdir -p /data/clickhouse
sudo chown -R $USER:$USER /data/clickhouse
```

## ç”Ÿç”¢ç’°å¢ƒé…ç½®

### 3. å‰µå»ºå„ªåŒ–çš„ docker-compose.yml
```yaml
# ~/clickhouse-docker/docker-compose.yml
# æ³¨æ„ï¼šversion å±¬æ€§åœ¨æ–°ç‰ˆ Docker Compose ä¸­å·²æ£„ç”¨ï¼Œå¯ä»¥çœç•¥

services:
  clickhouse:
    image: clickhouse/clickhouse-server:24.8-alpine
    container_name: clickhouse-server
    hostname: clickhouse-server
    
    # ç¶²è·¯é…ç½®ï¼ˆç”Ÿç”¢ç’°å¢ƒå¯è€ƒæ…® host æ¨¡å¼ï¼Œæ¸¬è©¦ç’°å¢ƒå»ºè­°ç”¨æ©‹æ¥ï¼‰
    ports:
      - "8123:8123"  # HTTP interface
      - "9000:9000"  # Native client
      - "9009:9009"  # Interserver HTTP
    
    # ç’°å¢ƒè®Šæ•¸
    environment:
      # åˆå§‹è³‡æ–™åº«è¨­ç½®
      CLICKHOUSE_DB: market_data
      CLICKHOUSE_USER: trader
      CLICKHOUSE_PASSWORD: SecurePass123!
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      # æ•ˆèƒ½ç›¸é—œ
      CLICKHOUSE_MAX_MEMORY_USAGE: 10000000000  # 10GB
      CLICKHOUSE_MAX_MEMORY_USAGE_FOR_USER: 10000000000
      
    # è³‡æºé™åˆ¶
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G
    
    # æ›è¼‰è¨­å®š
    volumes:
      # è³‡æ–™ç›®éŒ„ - ä½¿ç”¨æœ¬åœ° SSD
      - type: bind
        source: /data/clickhouse
        target: /var/lib/clickhouse
      # æ—¥èªŒç›®éŒ„
      - type: bind
        source: ./logs
        target: /var/log/clickhouse-server
      # è‡ªå®šç¾©é…ç½®
      - type: bind
        source: ./config
        target: /etc/clickhouse-server/config.d
        read_only: true
      # å‚™ä»½ç›®éŒ„
      - type: bind
        source: ./backup
        target: /backups
    
    # ç³»çµ±é™åˆ¶å„ªåŒ–
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
      memlock:
        soft: -1
        hard: -1
      nproc:
        soft: 131072
        hard: 131072
    
    # å¥åº·æª¢æŸ¥
    healthcheck:
      test: ["CMD", "clickhouse-client", "--host", "localhost", "--query", "SELECT 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    
    # è‡ªå‹•é‡å•Ÿ
    restart: unless-stopped
    
    # æ—¥èªŒè¨­ç½®
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
```

### 4. å‰µå»ºè‡ªå®šç¾©é…ç½®æ–‡ä»¶

#### å„²å­˜å„ªåŒ–é…ç½®
```xml
<!-- ~/clickhouse-docker/config/storage.xml -->
<?xml version="1.0"?>
<clickhouse>
    <storage_configuration>
        <disks>
            <default>
                <path>/var/lib/clickhouse/</path>
            </default>
            <fast_ssd>
                <path>/var/lib/clickhouse/fast/</path>
            </fast_ssd>
        </disks>
        <policies>
            <tiered>
                <volumes>
                    <hot>
                        <disk>fast_ssd</disk>
                        <max_data_part_size_bytes>10737418240</max_data_part_size_bytes>
                    </hot>
                    <cold>
                        <disk>default</disk>
                    </cold>
                </volumes>
                <move_factor>0.2</move_factor>
            </tiered>
        </policies>
    </storage_configuration>
    
    <!-- å£“ç¸®è¨­ç½® -->
    <compression>
        <case>
            <method>lz4hc</method>
            <level>12</level>
        </case>
    </compression>
</clickhouse>
```

#### æ•ˆèƒ½å„ªåŒ–é…ç½®ï¼ˆé¸ç”¨ï¼Œé è¨­é…ç½®é€šå¸¸å·²è¶³å¤ ï¼‰
```xml
<!-- ~/clickhouse-docker/config/performance.xml -->
<!-- è­¦å‘Šï¼šè‡ªå®šç¾©é…ç½®å®¹æ˜“å‡ºéŒ¯ï¼Œå»ºè­°å…ˆä½¿ç”¨é è¨­é…ç½®æ¸¬è©¦ -->
<?xml version="1.0"?>
<clickhouse>
    <!-- ç¶²è·¯å„ªåŒ– -->
    <max_concurrent_queries>100</max_concurrent_queries>
    <max_connections>4096</max_connections>

    <!-- èƒŒæ™¯ä»»å‹™å„ªåŒ– -->
    <background_pool_size>16</background_pool_size>
    <background_schedule_pool_size>16</background_schedule_pool_size>

    <!-- MergeTree å„ªåŒ– -->
    <merge_tree>
        <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
        <max_parts_in_total>100000</max_parts_in_total>
    </merge_tree>

    <!-- User profiles è¨­å®šï¼ˆuser-level è¨­å®šå¿…é ˆæ”¾åœ¨ profiles å€å¡Šï¼‰ -->
    <profiles>
        <default>
            <max_threads>8</max_threads>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_memory_usage_for_user>10000000000</max_memory_usage_for_user>
            <max_bytes_before_external_group_by>5000000000</max_bytes_before_external_group_by>
        </default>
    </profiles>
</clickhouse>
```

## è‡ªå‹•å‚™ä»½è¨­ç½®

### 5. å‚™ä»½è…³æœ¬

#### ä¸»å‚™ä»½è…³æœ¬
```bash
#!/bin/bash
# ~/clickhouse-docker/scripts/backup.sh

# è¨­å®šè®Šæ•¸
BACKUP_DIR="/home/$USER/clickhouse-docker/backup"
CONTAINER_NAME="clickhouse-server"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=7
LOG_FILE="$BACKUP_DIR/backup.log"

# é¡è‰²è¼¸å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ—¥èªŒå‡½æ•¸
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

# éŒ¯èª¤è™•ç†
error_exit() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$LOG_FILE"
    exit 1
}

# æª¢æŸ¥å®¹å™¨æ˜¯å¦é‹è¡Œ
check_container() {
    if ! docker ps --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        error_exit "Container ${CONTAINER_NAME} is not running!"
    fi
}

# åŸ·è¡Œå‚™ä»½
perform_backup() {
    log "Starting backup..."
    
    # æ–¹æ³• 1: ä½¿ç”¨ ClickHouse åŸç”Ÿå‚™ä»½ï¼ˆæ¨è–¦ï¼‰
    log "Creating ClickHouse native backup..."
    
    # å‰µå»ºæ‰€æœ‰è¡¨çš„å‚™ä»½
    docker exec $CONTAINER_NAME clickhouse-client --query "
        SELECT concat('BACKUP TABLE ', database, '.', name, ' TO Disk(''backups'', ''', '${DATE}/', database, '_', name, ''');')
        FROM system.tables 
        WHERE database NOT IN ('system', 'information_schema', 'INFORMATION_SCHEMA')
    " | while read backup_cmd; do
        if [ ! -z "$backup_cmd" ]; then
            docker exec $CONTAINER_NAME clickhouse-client --query "$backup_cmd" || log "Warning: Failed to backup a table"
        fi
    done
    
    # æ–¹æ³• 2: ç‰©ç†å‚™ä»½ï¼ˆå®Œæ•´å‚™ä»½ï¼‰
    log "Creating physical backup..."
    docker exec $CONTAINER_NAME bash -c "
        tar czf /backups/physical_backup_${DATE}.tar.gz \
        --exclude='/var/lib/clickhouse/preprocessed_configs' \
        --exclude='/var/lib/clickhouse/tmp' \
        /var/lib/clickhouse/data \
        /var/lib/clickhouse/metadata
    " || error_exit "Physical backup failed"
    
    # å‚™ä»½é…ç½®æ–‡ä»¶
    log "Backing up configuration..."
    tar czf "$BACKUP_DIR/config_backup_${DATE}.tar.gz" \
        -C ~/clickhouse-docker config/ docker-compose.yml \
        || error_exit "Config backup failed"
    
    log "Backup completed successfully!"
}

# æ¸…ç†èˆŠå‚™ä»½
cleanup_old_backups() {
    log "Cleaning up old backups (older than $RETENTION_DAYS days)..."
    
    # æ¸…ç†æœ¬åœ°å‚™ä»½
    find "$BACKUP_DIR" -name "*.tar.gz" -type f -mtime +$RETENTION_DAYS -delete
    
    # æ¸…ç†å®¹å™¨å…§å‚™ä»½
    docker exec $CONTAINER_NAME bash -c "
        find /backups -name '*.tar.gz' -type f -mtime +$RETENTION_DAYS -delete
    "
    
    log "Cleanup completed"
}

# é¡¯ç¤ºå‚™ä»½è³‡è¨Š
show_backup_info() {
    echo -e "${GREEN}=== Backup Information ===${NC}"
    echo "Backup Location: $BACKUP_DIR"
    echo "Latest Backups:"
    ls -lah "$BACKUP_DIR" | grep -E "*.tar.gz" | tail -5
    
    # é¡¯ç¤ºå‚™ä»½å¤§å°çµ±è¨ˆ
    total_size=$(du -sh "$BACKUP_DIR" 2>/dev/null | cut -f1)
    echo -e "\nTotal backup size: ${YELLOW}$total_size${NC}"
}

# ä¸»ç¨‹åº
main() {
    echo -e "${GREEN}=== ClickHouse Backup Script ===${NC}"
    
    # å‰µå»ºå‚™ä»½ç›®éŒ„
    mkdir -p "$BACKUP_DIR"
    
    # åŸ·è¡Œå‚™ä»½æµç¨‹
    check_container
    perform_backup
    cleanup_old_backups
    show_backup_info
    
    echo -e "${GREEN}âœ… Backup process completed!${NC}"
}

# åŸ·è¡Œä¸»ç¨‹åº
main
```

#### é‚„åŸè…³æœ¬
```bash
#!/bin/bash
# ~/clickhouse-docker/scripts/restore.sh

# è¨­å®šè®Šæ•¸
BACKUP_DIR="/home/$USER/clickhouse-docker/backup"
CONTAINER_NAME="clickhouse-server"

# é¡è‰²è¼¸å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

# åˆ—å‡ºå¯ç”¨å‚™ä»½
list_backups() {
    echo -e "${GREEN}=== Available Backups ===${NC}"
    ls -la "$BACKUP_DIR" | grep -E "physical_backup.*tar.gz" | nl
}

# é‚„åŸå‚™ä»½
restore_backup() {
    list_backups
    
    echo -e "${YELLOW}Enter the number of backup to restore:${NC}"
    read backup_num
    
    backup_file=$(ls "$BACKUP_DIR" | grep -E "physical_backup.*tar.gz" | sed -n "${backup_num}p")
    
    if [ -z "$backup_file" ]; then
        echo -e "${RED}Invalid selection!${NC}"
        exit 1
    fi
    
    echo -e "${YELLOW}âš ï¸  WARNING: This will stop ClickHouse and restore data!${NC}"
    echo "Restore from: $backup_file"
    echo "Continue? (yes/no)"
    read confirm
    
    if [ "$confirm" != "yes" ]; then
        echo "Restore cancelled"
        exit 0
    fi
    
    # åœæ­¢å®¹å™¨
    echo "Stopping ClickHouse..."
    docker-compose -f ~/clickhouse-docker/docker-compose.yml down
    
    # å‚™ä»½ç•¶å‰æ•¸æ“šï¼ˆå®‰å…¨èµ·è¦‹ï¼‰
    echo "Backing up current data..."
    sudo mv /data/clickhouse /data/clickhouse.old.$(date +%Y%m%d_%H%M%S)
    sudo mkdir -p /data/clickhouse
    
    # è§£å£“å‚™ä»½
    echo "Restoring backup..."
    sudo tar xzf "$BACKUP_DIR/$backup_file" -C / --strip-components=2
    
    # ä¿®æ­£æ¬Šé™
    sudo chown -R 101:101 /data/clickhouse
    
    # é‡å•Ÿå®¹å™¨
    echo "Starting ClickHouse..."
    docker-compose -f ~/clickhouse-docker/docker-compose.yml up -d
    
    # ç­‰å¾…æœå‹™å•Ÿå‹•
    sleep 10
    
    # æª¢æŸ¥æœå‹™ç‹€æ…‹
    docker exec $CONTAINER_NAME clickhouse-client --query "SELECT 'Restore completed successfully!'"
    
    echo -e "${GREEN}âœ… Restore completed!${NC}"
}

# ä¸»ç¨‹åº
restore_backup
```

### 6. è¨­ç½® Crontab è‡ªå‹•å‚™ä»½
```bash
# ä½¿è…³æœ¬å¯åŸ·è¡Œ
chmod +x ~/clickhouse-docker/scripts/*.sh

# è¨­ç½®æ¯æ—¥è‡ªå‹•å‚™ä»½ (æ¯å¤©å‡Œæ™¨ 2 é»)
crontab -e

# æ·»åŠ ä»¥ä¸‹è¡Œ
0 2 * * * /home/$USER/clickhouse-docker/scripts/backup.sh >> /home/$USER/clickhouse-docker/backup/cron.log 2>&1
```

## æ¸¬è©¦ç¯„ä¾‹

### 7. ä½¿ç”¨ Makefile ç®¡ç†ï¼ˆæ¨è–¦ï¼‰

å‰µå»º Makefile ç°¡åŒ–æ“ä½œï¼š
```makefile
# å¿«é€Ÿé–‹å§‹
make quick-start   # ä¸€éµå®‰è£ä¸¦å•Ÿå‹•

# æ—¥å¸¸æ“ä½œ
make up           # å•Ÿå‹•æœå‹™
make down         # åœæ­¢æœå‹™
make status       # æŸ¥çœ‹ç‹€æ…‹
make shell        # é€²å…¥ CLI
make backup       # åŸ·è¡Œå‚™ä»½
make restore      # é‚„åŸå‚™ä»½
make reset        # å®Œå…¨é‡ç½®
```

æ‰‹å‹•æ“ä½œï¼š
```bash
cd ~/clickhouse-docker
docker-compose up -d

# æª¢æŸ¥ç‹€æ…‹
docker-compose ps
docker logs clickhouse-server --tail 50
```

### 8. å‰µå»ºæ¸¬è©¦è¡¨ä¸¦å°å…¥æ•¸æ“š

#### å‰µå»ºåŸºæœ¬è¡¨çµæ§‹
```sql
# é€£æ¥åˆ° ClickHouse
docker exec -it clickhouse-server clickhouse-client --user trader --password SecurePass123!

-- å‰µå»ºè³‡æ–™åº«
CREATE DATABASE IF NOT EXISTS market_data;
USE market_data;

-- å‰µå»º tick æ•¸æ“šè¡¨ï¼ˆç°¡åŒ–ç‰ˆï¼Œé¿å…è¤‡é›œç·¨ç¢¼å•é¡Œï¼‰
CREATE TABLE market_ticks (
    ts DateTime64(3),
    symbol String,
    close Decimal32(2),
    volume UInt32,
    bid_price Decimal32(2),
    bid_volume UInt32,
    ask_price Decimal32(2),
    ask_volume UInt32,
    tick_type UInt8
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(ts)
ORDER BY (symbol, ts);

-- æŸ¥çœ‹éŒ¶çµæ§‹
DESCRIBE TABLE market_ticks;
```

#### å°å…¥æ¸¬è©¦æ•¸æ“š
```bash
# å‰µå»ºæ¸¬è©¦ CSV æ–‡ä»¶
cat > ~/clickhouse-docker/test_data.csv << 'EOF'
ts,symbol,close,volume,bid_price,bid_volume,ask_price,ask_volume,tick_type
2025-09-25 17:25:00.044,AAPL,1325.0,7,1320.0,160,1325.0,22,1
2025-09-25 17:25:02.207,AAPL,1325.0,1,1320.0,160,1325.0,22,1
2025-09-25 17:25:03.125,AAPL,1325.0,1,1320.0,161,1325.0,21,1
2025-09-25 17:25:47.863,AAPL,1325.0,1,1320.0,162,1325.0,21,1
2025-09-25 17:26:14.894,TSLA,1325.0,1,1320.0,181,1325.0,18,1
2025-09-25 17:26:51.365,TSLA,1325.0,1,1320.0,181,1325.0,27,1
2025-09-25 17:29:59.655,TSLA,1325.0,3,1320.0,177,1325.0,31,1
2025-09-25 17:30:06.262,NVDA,1325.0,1,1320.0,180,1325.0,28,1
2025-09-25 17:30:14.542,NVDA,1325.0,3,1320.0,183,1325.0,27,1
2025-09-25 17:30:52.600,NVDA,1320.0,2,1320.0,185,1325.0,22,2
EOF

# å°å…¥æ•¸æ“š
docker exec -i clickhouse-server clickhouse-client \
    --user trader --password SecurePass123! \
    --query "INSERT INTO market_data.market_ticks FORMAT CSVWithNames" \
    < ~/clickhouse-docker/test_data.csv
```

### 9. æ¸¬è©¦æŸ¥è©¢

#### åŸºæœ¬æŸ¥è©¢æ¸¬è©¦
```sql
-- é€£æ¥åˆ°è³‡æ–™åº«
docker exec -it clickhouse-server clickhouse-client \
    --user trader --password SecurePass123! \
    --database market_data

-- æŸ¥è©¢è¨˜éŒ„æ•¸
SELECT count(*) FROM market_ticks;

-- æŸ¥çœ‹æœ€æ–°æ•¸æ“š
SELECT * FROM market_ticks ORDER BY ts DESC LIMIT 5;

-- æŒ‰ symbol èšåˆ
SELECT 
    symbol,
    count(*) as tick_count,
    avg(close) as avg_price,
    max(volume) as max_volume
FROM market_ticks
GROUP BY symbol;

-- æ™‚é–“ç¯„åœæŸ¥è©¢
SELECT *
FROM market_ticks
WHERE ts >= '2025-09-25 17:25:00'
  AND ts <= '2025-09-25 17:30:00'
ORDER BY ts;

-- æŸ¥çœ‹å£“ç¸®æ•ˆæœ
SELECT 
    formatReadableSize(sum(data_compressed_bytes)) as compressed,
    formatReadableSize(sum(data_uncompressed_bytes)) as uncompressed,
    round(sum(data_uncompressed_bytes) / sum(data_compressed_bytes), 2) as ratio
FROM system.parts
WHERE database = 'market_data' AND table = 'market_ticks';
```

#### æ•ˆèƒ½æ¸¬è©¦
```sql
-- æ’å…¥å¤§é‡æ¸¬è©¦æ•¸æ“šï¼ˆä½¿ç”¨ rand() å‡½æ•¸é¿å…é¡å‹å•é¡Œï¼‰
INSERT INTO market_ticks
SELECT
    now() - toIntervalSecond(toUInt32(rand() % 86400)) as ts,
    arrayElement(['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA'], (rand() % 5) + 1) as symbol,
    1000 + (rand() % 100) as close,
    toUInt32((rand() % 1000) + 1) as volume,
    1000 + (rand() % 100) - 50 as bid_price,
    toUInt32((rand() % 500) + 1) as bid_volume,
    1000 + (rand() % 100) as ask_price,
    toUInt32((rand() % 500) + 1) as ask_volume,
    toUInt8((rand() % 3) + 1) as tick_type
FROM numbers(1000000);

-- æ¸¬è©¦æŸ¥è©¢æ•ˆèƒ½
SELECT 
    symbol,
    toStartOfMinute(ts) as minute,
    avg(close) as avg_price,
    sum(volume) as total_volume
FROM market_ticks
WHERE ts >= now() - INTERVAL 1 DAY
GROUP BY symbol, minute
ORDER BY minute DESC, symbol
LIMIT 100
FORMAT Null;

-- é¡¯ç¤ºæŸ¥è©¢çµ±è¨ˆ
SHOW PROCESSLIST;
```

## ç›£æ§èˆ‡ç¶­è­·

### 10. ç›£æ§è…³æœ¬
```bash
#!/bin/bash
# ~/clickhouse-docker/scripts/monitor.sh

echo "=== ClickHouse Monitoring Dashboard ==="
echo "======================================="

# ç³»çµ±è³‡æºä½¿ç”¨
echo -e "\nğŸ“Š Resource Usage:"
docker stats clickhouse-server --no-stream

# è³‡æ–™åº«å¤§å°
echo -e "\nğŸ’¾ Database Size:"
docker exec clickhouse-server clickhouse-client --user trader --password SecurePass123! --query "
SELECT 
    database,
    formatReadableSize(sum(bytes_on_disk)) as size,
    formatReadableQuantity(sum(rows)) as rows,
    count() as tables
FROM system.parts
WHERE active
GROUP BY database
FORMAT Pretty"

# æŸ¥è©¢æ•ˆèƒ½
echo -e "\nâš¡ Recent Queries:"
docker exec clickhouse-server clickhouse-client --user trader --password SecurePass123! --query "
SELECT 
    substring(query, 1, 50) as query_preview,
    formatReadableSize(memory_usage) as memory,
    query_duration_ms,
    read_rows,
    written_rows
FROM system.query_log
WHERE type = 'QueryFinish'
ORDER BY event_time DESC
LIMIT 5
FORMAT Pretty"

# ç³»çµ±å¥åº·ç‹€æ…‹
echo -e "\nâœ… Health Check:"
docker exec clickhouse-server clickhouse-client --query "SELECT 'ClickHouse is running OK!'"
```

### 11. æ—¥å¸¸ç¶­è­·å‘½ä»¤
```bash
# å„ªåŒ–è¡¨ï¼ˆæ•´ç†ç¢ç‰‡ï¼‰
docker exec clickhouse-server clickhouse-client --user trader --password SecurePass123! \
    --query "OPTIMIZE TABLE market_data.market_ticks FINAL"

# æ¸…ç†èˆŠåˆ†å€
docker exec clickhouse-server clickhouse-client --user trader --password SecurePass123! \
    --query "ALTER TABLE market_data.market_ticks DROP PARTITION '202501'"

# æŸ¥çœ‹æ…¢æŸ¥è©¢
docker exec clickhouse-server clickhouse-client --user trader --password SecurePass123! \
    --query "
    SELECT 
        query,
        query_duration_ms,
        memory_usage
    FROM system.query_log
    WHERE query_duration_ms > 1000
    ORDER BY query_duration_ms DESC
    LIMIT 10"

# é‡å•Ÿæœå‹™
docker-compose restart

# å‡ç´š ClickHouse
docker-compose pull
docker-compose up -d
```

## å¸¸è¦‹å•é¡Œ

### Q1: è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤
```bash
# èª¿æ•´è¨˜æ†¶é«”é™åˆ¶
docker exec clickhouse-server clickhouse-client --query "
SET max_memory_usage = 5000000000;
SET max_memory_usage_for_user = 5000000000;"
```

### Q2: ç£ç¢Ÿç©ºé–“ä¸è¶³
```bash
# æª¢æŸ¥ç©ºé–“ä½¿ç”¨
df -h /data/clickhouse

# æ¸…ç†èˆŠæ•¸æ“š
docker exec clickhouse-server clickhouse-client --query "
ALTER TABLE market_data.market_ticks 
DROP PARTITION WHERE toYYYYMM(ts) < toYYYYMM(now() - INTERVAL 6 MONTH)"
```

### Q3: é€£æ¥è¢«æ‹’çµ•
```bash
# æª¢æŸ¥å®¹å™¨ç‹€æ…‹
docker ps | grep clickhouse

# æŸ¥çœ‹æ—¥èªŒ
docker logs clickhouse-server --tail 100

# é‡å•Ÿå®¹å™¨
docker-compose restart
```

### Q4: æŸ¥è©¢å¤ªæ…¢
```sql
-- åˆ†ææŸ¥è©¢è¨ˆç•«
EXPLAIN SELECT * FROM market_ticks WHERE symbol = 'AAPL';

-- å‰µå»ºç´¢å¼•
ALTER TABLE market_ticks ADD INDEX idx_symbol (symbol) TYPE minmax GRANULARITY 4;
```

## å„ªåŒ–è¡¨è¨­è¨ˆï¼ˆé©ç”¨æ–¼å¤§é‡ Tick æ•¸æ“šï¼‰

### å‰µå»ºå„ªåŒ–çš„è¡¨çµæ§‹
```sql
-- ä¸»è¦ tick æ•¸æ“šè¡¨ï¼ˆå„ªåŒ–ç‰ˆï¼‰
CREATE TABLE tick_data_optimized (
    timestamp DateTime64(3) CODEC(DoubleDelta),
    symbol LowCardinality(String),
    exchange LowCardinality(String),
    price Decimal64(4),
    volume UInt64 CODEC(T64),
    side Enum8('buy' = 1, 'sell' = 2),
    bid_price Decimal64(4),
    ask_price Decimal64(4),
    bid_volume UInt32 CODEC(T64),
    ask_volume UInt32 CODEC(T64)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (symbol, timestamp)
PRIMARY KEY (symbol, timestamp)
TTL toDateTime(timestamp) + INTERVAL 90 DAY
SETTINGS index_granularity = 8192;

-- æ·»åŠ ç´¢å¼•
ALTER TABLE tick_data_optimized
    ADD INDEX idx_price price TYPE minmax GRANULARITY 4,
    ADD INDEX idx_symbol symbol TYPE bloom_filter(0.01) GRANULARITY 1;
```

### å‰µå»ºç‰©åŒ–è¦–åœ–ï¼ˆé èšåˆï¼‰
```sql
-- 1åˆ†é˜Kç·šç‰©åŒ–è¦–åœ–
CREATE MATERIALIZED VIEW tick_1min_mv
ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(minute)
ORDER BY (symbol, minute)
AS SELECT
    symbol,
    toStartOfMinute(timestamp) as minute,
    argMinState(price, timestamp) as open,
    maxState(price) as high,
    minState(price) as low,
    argMaxState(price, timestamp) as close,
    sumState(toUInt64(volume)) as volume,
    countState() as tick_count
FROM tick_data_optimized
GROUP BY symbol, minute;
```

## æ•ˆèƒ½èª¿å„ªå»ºè­°

1. **ä½¿ç”¨ SSD/NVMe** - I/O æ˜¯æœ€å¤§ç“¶é ¸
2. **é©ç•¶åˆ†å€** - æŒ‰æœˆåˆ†å€å° tick æ•¸æ“šæœ€åˆé©
3. **æ‰¹é‡æ’å…¥** - å–®ç­†æ’å…¥æ•ˆèƒ½å·®ï¼Œå»ºè­°æ‰¹é‡ 10000+ ç­†
4. **å®šæœŸ OPTIMIZE** - æ¯é€±åŸ·è¡Œä¸€æ¬¡ OPTIMIZE TABLE
5. **ç›£æ§è¨˜æ†¶é«”** - ClickHouse æ˜¯è¨˜æ†¶é«”å¯†é›†å‹è³‡æ–™åº«
6. **ä½¿ç”¨ç‰©åŒ–è¦–åœ–** - é èšåˆå¸¸ç”¨æŸ¥è©¢ï¼Œå¤§å¹…æå‡æ•ˆèƒ½
7. **é¸æ“‡æ­£ç¢ºçš„ç·¨ç¢¼** - DoubleDelta ç”¨æ–¼æ™‚é–“æˆ³ï¼ŒT64 ç”¨æ–¼æ•´æ•¸
8. **ä½¿ç”¨ LowCardinality** - å°é‡è¤‡å€¼å¤šçš„å­—ä¸²æ¬„ä½

## å®‰å…¨å»ºè­°

1. **ä¿®æ”¹é è¨­å¯†ç¢¼** - ç«‹å³ä¿®æ”¹ docker-compose.yml ä¸­çš„å¯†ç¢¼
2. **é™åˆ¶ç¶²è·¯å­˜å–** - ä½¿ç”¨é˜²ç«ç‰†é™åˆ¶ 8123/9000 ç«¯å£
3. **å®šæœŸå‚™ä»½** - ç¢ºä¿è‡ªå‹•å‚™ä»½æ­£å¸¸é‹è¡Œ
4. **ç›£æ§æ—¥èªŒ** - å®šæœŸæª¢æŸ¥ç•°å¸¸å­˜å–
5. **æ›´æ–°ç‰ˆæœ¬** - é—œæ³¨å®‰å…¨æ›´æ–°

## é‡è¦æ³¨æ„äº‹é …

### å¯¦æ¸¬ç™¼ç¾çš„å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

1. **Docker Compose å®‰è£**
   - ä½¿ç”¨å…·é«”ç‰ˆæœ¬è™Ÿè€Œé `latest`ï¼Œé¿å…ä¸‹è¼‰å¤±æ•—
   - Ubuntu 24.04 å¯èƒ½æ²’æœ‰ docker-compose-plugin å¥—ä»¶

2. **é…ç½®æª”å•é¡Œ**
   - user-level è¨­å®šï¼ˆå¦‚ max_threadsï¼‰å¿…é ˆæ”¾åœ¨ `<profiles>` å€å¡Šä¸­
   - è¤‡é›œçš„å£“ç¸®ç·¨ç¢¼ï¼ˆå¦‚ Gorilla + LZ4ï¼‰å¯èƒ½å°è‡´éŒ¯èª¤
   - å»ºè­°å…ˆä½¿ç”¨é è¨­é…ç½®ï¼Œç¢ºèªé‹è¡Œæ­£å¸¸å¾Œå†å„ªåŒ–

3. **éš¨æ©Ÿå‡½æ•¸ä½¿ç”¨**
   - ä½¿ç”¨ `rand()` è€Œé `randUniform()` æˆ– `randNormal()`
   - æ³¨æ„é¡å‹è½‰æ›ï¼ˆä½¿ç”¨ toUInt32ã€toUInt8 ç­‰ï¼‰

4. **ç¶²è·¯æ¨¡å¼**
   - æ¸¬è©¦ç’°å¢ƒå»ºè­°ä½¿ç”¨æ©‹æ¥æ¨¡å¼é…ç½®ç«¯å£
   - ç”Ÿç”¢ç’°å¢ƒå¯è€ƒæ…® host æ¨¡å¼ä»¥ç²å¾—æ›´å¥½æ•ˆèƒ½

5. **å‚™ä»½é…ç½®**
   - åŸç”Ÿå‚™ä»½éœ€è¦é¡å¤–é…ç½® `backups.allowed_disk` åƒæ•¸
   - ç‰©ç†å‚™ä»½å¯ä»¥æ­£å¸¸é‹ä½œ
   - é‚„åŸåŠŸèƒ½å»ºè­°ä½¿ç”¨ ClickHouse åŸç”Ÿ BACKUP/RESTORE å‘½ä»¤

6. **å„ªåŒ–è¡¨è¨­è¨ˆæ³¨æ„äº‹é …**
   - Gorilla ç·¨ç¢¼ä¸é©ç”¨æ–¼ Decimal é¡å‹
   - TTL ä½¿ç”¨ DateTime64 éœ€è¦è½‰æ›ç‚º DateTime
   - ç‰©åŒ–è¦–åœ–ä½¿ç”¨ State å‡½æ•¸èšåˆï¼ŒæŸ¥è©¢æ™‚ç”¨ Merge å‡½æ•¸
   - bloom_filter ç´¢å¼•å°å­—ä¸²æŸ¥è©¢æ•ˆèƒ½æå‡é¡¯è‘—

## æ”¯æ´è³‡æº

- [å®˜æ–¹æ–‡æª”](https://clickhouse.com/docs)
- [Docker Hub](https://hub.docker.com/r/clickhouse/clickhouse-server)
- [GitHub Issues](https://github.com/ClickHouse/ClickHouse/issues)
- [ç¤¾ç¾¤è«–å£‡](https://clickhouse.com/community)

## å¯¦æ¸¬é©—è­‰é …ç›®

ç¶“éå®Œæ•´æ¸¬è©¦é©—è­‰çš„åŠŸèƒ½ï¼š
- âœ… Docker Compose éƒ¨ç½²
- âœ… Makefile è‡ªå‹•åŒ–ç®¡ç†
- âœ… åŸºæœ¬è¡¨å»ºç«‹èˆ‡æŸ¥è©¢
- âœ… å„ªåŒ–è¡¨çµæ§‹ï¼ˆtick_data_optimizedï¼‰
- âœ… ç‰©åŒ–è¦–åœ–ï¼ˆ1åˆ†é˜ã€5åˆ†é˜Kç·šï¼‰
- âœ… ç´¢å¼•å„ªåŒ–ï¼ˆbloom_filterã€minmaxï¼‰
- âœ… å‚™ä»½åŠŸèƒ½
- âš ï¸ é‚„åŸåŠŸèƒ½ï¼ˆéœ€è¦æ”¹é€²ï¼‰

---

**æœ€å¾Œæ›´æ–°**: 2025-09-27
**ç‰ˆæœ¬**: ClickHouse 24.8
**ä½œè€…**: DevOps Team
**å¯¦æ¸¬é©—è­‰**: 2025-09-27 æˆåŠŸéƒ¨ç½²æ–¼ Ubuntu ç’°å¢ƒï¼ŒåŒ…å«å„ªåŒ–è¡¨è¨­è¨ˆ