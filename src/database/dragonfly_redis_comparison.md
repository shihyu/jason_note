# DragonflyDB vs Redis å®Œæ•´æ¯”è¼ƒæŒ‡å—

## ç›®éŒ„
- [ç°¡ä»‹](#ç°¡ä»‹)
- [å®‰è£æŒ‡å—](#å®‰è£æŒ‡å—)
- [Python æ¸¬è©¦ç’°å¢ƒè¨­ç½®](#python-æ¸¬è©¦ç’°å¢ƒè¨­ç½®)
- [Redis vs DragonflyDB è©³ç´°æ¯”è¼ƒ](#redis-vs-dragonflydb-è©³ç´°æ¯”è¼ƒ)
- [æ•ˆèƒ½åŸºæº–æ¸¬è©¦](#æ•ˆèƒ½åŸºæº–æ¸¬è©¦)
- [é¸æ“‡å»ºè­°](#é¸æ“‡å»ºè­°)

## ç°¡ä»‹

### Redis
Redis (Remote Dictionary Server) æ˜¯ä¸€å€‹é–‹æºçš„è¨˜æ†¶é«”è³‡æ–™çµæ§‹å„²å­˜ç³»çµ±ï¼Œç”± Salvatore Sanfilippo åœ¨ 2009 å¹´å‰µå»ºã€‚å®ƒå¯ä»¥ç”¨ä½œè³‡æ–™åº«ã€å¿«å–å’Œè¨Šæ¯ä»£ç†ã€‚

### DragonflyDB
DragonflyDB æ˜¯æ–°ä¸€ä»£çš„è¨˜æ†¶é«”è³‡æ–™åº«ï¼Œæ–¼ 2022 å¹´æ¨å‡ºï¼Œæ—¨åœ¨æˆç‚º Redis çš„ç¾ä»£æ›¿ä»£å“ã€‚å®ƒå®Œå…¨ç›¸å®¹ Redis å”è­°ï¼Œä½†åº•å±¤æ¶æ§‹å®Œå…¨é‡æ–°è¨­è¨ˆã€‚

## å®‰è£æŒ‡å—

### DragonflyDB å®‰è£

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ Dockerï¼ˆæ¨è–¦ï¼‰
```bash
# æ‹‰å–ä¸¦åŸ·è¡Œ DragonflyDB
docker run --rm -p 6379:6379 docker.dragonflydb.io/dragonflydb/dragonfly

# ä½¿ç”¨è‡ªè¨‚é…ç½®
docker run --rm -p 6379:6379 \
  -v /path/to/dragonfly.conf:/etc/dragonfly/dragonfly.conf \
  docker.dragonflydb.io/dragonflydb/dragonfly \
  --flagfile=/etc/dragonfly/dragonfly.conf
```

#### æ–¹æ³•äºŒï¼šåœ¨ Ubuntu/Debian ä¸Šå®‰è£
```bash
# ä¸‹è¼‰æœ€æ–°ç‰ˆæœ¬
curl -L https://github.com/dragonflydb/dragonfly/releases/latest/download/dragonfly-x86_64.tar.gz | tar -xz

# åŸ·è¡Œ
./dragonfly --logtostderr

# æŒ‡å®šè¨˜æ†¶é«”é™åˆ¶
./dragonfly --logtostderr --maxmemory=4gb
```

#### æ–¹æ³•ä¸‰ï¼šä½¿ç”¨ Kubernetes
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dragonfly
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dragonfly
  template:
    metadata:
      labels:
        app: dragonfly
    spec:
      containers:
      - name: dragonfly
        image: docker.dragonflydb.io/dragonflydb/dragonfly
        ports:
        - containerPort: 6379
```

### Redis å®‰è£

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ Docker
```bash
docker run --rm -p 6379:6379 redis:latest
```

#### æ–¹æ³•äºŒï¼šåœ¨ Ubuntu/Debian ä¸Šå®‰è£
```bash
sudo apt update
sudo apt install redis-server
sudo systemctl start redis-server
```

## Python æ¸¬è©¦ç’°å¢ƒè¨­ç½®

### å®‰è£å¿…è¦å¥—ä»¶
```bash
pip install redis pytest pytest-asyncio aioredis hiredis redis-py-cluster
```

### DragonflyDB Python ç¯„ä¾‹

#### 1. åŸºæœ¬é€£æ¥èˆ‡æ“ä½œ
```python
import redis
import time

# é€£æ¥åˆ° DragonflyDBï¼ˆèˆ‡ Redis å®¢æˆ¶ç«¯ç›¸åŒï¼‰
client = redis.Redis(
    host='localhost',
    port=6379,
    decode_responses=True,
    socket_keepalive=True,
    socket_keepalive_options={
        1: 1,  # TCP_KEEPIDLE
        2: 1,  # TCP_KEEPINTVL
        3: 5,  # TCP_KEEPCNT
    }
)

# æ¸¬è©¦é€£æ¥
try:
    response = client.ping()
    print(f"âœ… DragonflyDB é€£æ¥æˆåŠŸ: {response}")

    # å–å¾—ä¼ºæœå™¨è³‡è¨Š
    info = client.info()
    print(f"ä¼ºæœå™¨ç‰ˆæœ¬: {info.get('server', {}).get('dragonfly_version', 'Unknown')}")
    print(f"ä½¿ç”¨è¨˜æ†¶é«”: {info.get('memory', {}).get('used_memory_human', 'Unknown')}")
except redis.ConnectionError:
    print("âŒ ç„¡æ³•é€£æ¥åˆ° DragonflyDB")
```

#### 2. å­—ä¸²æ“ä½œç¯„ä¾‹
```python
import redis
from datetime import timedelta

client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# åŸºæœ¬å­—ä¸²æ“ä½œ
client.set("user:1:name", "Alice")
client.set("user:1:email", "alice@example.com")

# è¨­å®šéæœŸæ™‚é–“
client.setex("session:abc123", timedelta(hours=2), "user_data")
client.expire("user:1:name", timedelta(days=30))

# æ‰¹é‡è¨­å®š
client.mset({
    "product:1:name": "ç­†è¨˜å‹é›»è…¦",
    "product:1:price": "25000",
    "product:1:stock": "50"
})

# æ‰¹é‡å–å¾—
values = client.mget(["product:1:name", "product:1:price", "product:1:stock"])
print(f"ç”¢å“è³‡è¨Š: {values}")

# åŸå­æ“ä½œ
client.incr("page:views")
client.incrby("product:1:stock", -1)  # æ¸›å°‘åº«å­˜
new_stock = client.get("product:1:stock")
print(f"æ›´æ–°å¾Œåº«å­˜: {new_stock}")

# æ¢ä»¶è¨­å®š
client.setnx("lock:resource", "1")  # åªåœ¨ä¸å­˜åœ¨æ™‚è¨­å®š
```

#### 3. åˆ—è¡¨æ“ä½œç¯„ä¾‹
```python
import redis

client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ä»»å‹™ä½‡åˆ—ç¯„ä¾‹
def add_task(task):
    client.lpush("task:queue", task)
    print(f"æ–°å¢ä»»å‹™: {task}")

def get_task():
    task = client.rpop("task:queue")
    return task

# æ–°å¢ä»»å‹™
add_task("send_email:user123")
add_task("process_payment:order456")
add_task("generate_report:monthly")

# è™•ç†ä»»å‹™
while True:
    task = get_task()
    if task:
        print(f"è™•ç†ä»»å‹™: {task}")
    else:
        print("æ²’æœ‰å¾…è™•ç†ä»»å‹™")
        break

# æœ€è¿‘æ´»å‹•è¨˜éŒ„
client.lpush("user:1:activities", "ç™»å…¥ç³»çµ±")
client.lpush("user:1:activities", "æŸ¥çœ‹è¨‚å–®")
client.lpush("user:1:activities", "ä¿®æ”¹å€‹äººè³‡æ–™")
client.ltrim("user:1:activities", 0, 99)  # åªä¿ç•™æœ€è¿‘100ç­†

# å–å¾—æœ€è¿‘æ´»å‹•
recent_activities = client.lrange("user:1:activities", 0, 4)
print(f"æœ€è¿‘5ç­†æ´»å‹•: {recent_activities}")
```

#### 4. é›†åˆæ“ä½œç¯„ä¾‹
```python
import redis

client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# æ¨™ç±¤ç³»çµ±
client.sadd("post:1:tags", "Python", "DragonflyDB", "æ•™å­¸")
client.sadd("post:2:tags", "Python", "Redis", "å¿«å–")
client.sadd("post:3:tags", "JavaScript", "Node.js", "æ•™å­¸")

# æ‰¾å‡ºå…±åŒæ¨™ç±¤
common_tags = client.sinter("post:1:tags", "post:2:tags")
print(f"æ–‡ç« 1å’Œ2çš„å…±åŒæ¨™ç±¤: {common_tags}")

# ä½¿ç”¨è€…èˆˆè¶£
client.sadd("user:alice:interests", "Python", "è³‡æ–™åº«", "æ©Ÿå™¨å­¸ç¿’")
client.sadd("user:bob:interests", "Python", "ç¶²é é–‹ç™¼", "è³‡æ–™åº«")

# æ¨è–¦ç³»çµ± - æ‰¾å‡ºå…±åŒèˆˆè¶£
shared_interests = client.sinter("user:alice:interests", "user:bob:interests")
print(f"Alice å’Œ Bob çš„å…±åŒèˆˆè¶£: {shared_interests}")

# éš¨æ©ŸæŠ½ç
client.sadd("lottery:participants", "user1", "user2", "user3", "user4", "user5")
winner = client.srandmember("lottery:participants")
print(f"ä¸­çè€…: {winner}")
```

#### 5. æœ‰åºé›†åˆæ“ä½œç¯„ä¾‹
```python
import redis
import time

client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# æ’è¡Œæ¦œç³»çµ±
def update_score(user_id, score):
    client.zadd("leaderboard:global", {user_id: score})

def get_top_players(count=10):
    return client.zrevrange("leaderboard:global", 0, count-1, withscores=True)

def get_user_rank(user_id):
    rank = client.zrevrank("leaderboard:global", user_id)
    return rank + 1 if rank is not None else None

# æ›´æ–°åˆ†æ•¸
update_score("player:alice", 1500)
update_score("player:bob", 2100)
update_score("player:charlie", 1800)
update_score("player:david", 2500)
update_score("player:eve", 1200)

# å–å¾—æ’è¡Œæ¦œ
top_players = get_top_players(3)
print("ğŸ† æ’è¡Œæ¦œå‰ä¸‰å:")
for i, (player, score) in enumerate(top_players, 1):
    print(f"  {i}. {player}: {score:.0f} åˆ†")

# æŸ¥è©¢æ’å
alice_rank = get_user_rank("player:alice")
print(f"\nAlice çš„æ’å: ç¬¬ {alice_rank} å")

# æ™‚é–“åºåˆ—è³‡æ–™
timestamp = int(time.time())
client.zadd("events:timeline", {
    f"event:{timestamp}:login": timestamp,
    f"event:{timestamp+10}:purchase": timestamp + 10,
    f"event:{timestamp+20}:logout": timestamp + 20
})

# æŸ¥è©¢æ™‚é–“ç¯„åœå…§çš„äº‹ä»¶
recent_events = client.zrangebyscore(
    "events:timeline",
    timestamp,
    timestamp + 30,
    withscores=True
)
print(f"\næœ€è¿‘30ç§’çš„äº‹ä»¶: {recent_events}")
```

#### 6. é›œæ¹Šæ“ä½œç¯„ä¾‹
```python
import redis

client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ä½¿ç”¨è€…è³‡æ–™ç®¡ç†
def create_user(user_id, user_data):
    client.hset(f"user:{user_id}", mapping=user_data)

def get_user(user_id):
    return client.hgetall(f"user:{user_id}")

def update_user_field(user_id, field, value):
    client.hset(f"user:{user_id}", field, value)

# å»ºç«‹ä½¿ç”¨è€…
create_user("1001", {
    "username": "alice_wang",
    "email": "alice@example.com",
    "age": "28",
    "city": "è‡ºåŒ—",
    "created_at": "2024-01-15"
})

# å–å¾—ä½¿ç”¨è€…è³‡æ–™
user_data = get_user("1001")
print(f"ä½¿ç”¨è€…è³‡æ–™: {user_data}")

# æ›´æ–°ç‰¹å®šæ¬„ä½
update_user_field("1001", "city", "é«˜é›„")
update_user_field("1001", "last_login", "2024-01-20")

# æª¢æŸ¥æ¬„ä½æ˜¯å¦å­˜åœ¨
exists = client.hexists("user:1001", "email")
print(f"Email æ¬„ä½å­˜åœ¨: {exists}")

# å¢åŠ æ•¸å€¼æ¬„ä½
client.hincrby("user:1001", "login_count", 1)

# è³¼ç‰©è»Šç³»çµ±
def add_to_cart(user_id, product_id, quantity):
    client.hincrby(f"cart:{user_id}", product_id, quantity)

def get_cart(user_id):
    return client.hgetall(f"cart:{user_id}")

def remove_from_cart(user_id, product_id):
    client.hdel(f"cart:{user_id}", product_id)

# è³¼ç‰©è»Šæ“ä½œ
add_to_cart("user:1001", "product:laptop", 1)
add_to_cart("user:1001", "product:mouse", 2)
add_to_cart("user:1001", "product:keyboard", 1)

cart = get_cart("user:1001")
print(f"\nè³¼ç‰©è»Šå…§å®¹: {cart}")
```

#### 7. Pipeline æ‰¹æ¬¡æ“ä½œç¯„ä¾‹
```python
import redis
import time

client = redis.Redis(host='localhost', port=6379, decode_responses=True)

# ä½¿ç”¨ Pipeline æå‡æ•ˆèƒ½
def batch_insert_with_pipeline(count=10000):
    start_time = time.time()

    pipe = client.pipeline()
    for i in range(count):
        pipe.set(f"key:{i}", f"value:{i}")
        if i % 100 == 0:  # æ¯100å€‹å‘½ä»¤åŸ·è¡Œä¸€æ¬¡
            pipe.execute()
            pipe = client.pipeline()

    pipe.execute()  # åŸ·è¡Œå‰©é¤˜çš„å‘½ä»¤

    elapsed = time.time() - start_time
    print(f"Pipeline æ’å…¥ {count} ç­†è³‡æ–™è€—æ™‚: {elapsed:.2f} ç§’")
    print(f"å¹³å‡æ¯ç§’: {count/elapsed:.0f} ops")

# ä¸ä½¿ç”¨ Pipeline çš„å°ç…§çµ„
def batch_insert_without_pipeline(count=1000):
    start_time = time.time()

    for i in range(count):
        client.set(f"test:{i}", f"value:{i}")

    elapsed = time.time() - start_time
    print(f"ä¸€èˆ¬æ’å…¥ {count} ç­†è³‡æ–™è€—æ™‚: {elapsed:.2f} ç§’")
    print(f"å¹³å‡æ¯ç§’: {count/elapsed:.0f} ops")

# æ¸¬è©¦æ•ˆèƒ½å·®ç•°
print("æ•ˆèƒ½æ¯”è¼ƒ:")
batch_insert_without_pipeline(1000)
batch_insert_with_pipeline(10000)

# äº‹å‹™æ€§ Pipeline
def transfer_points(from_user, to_user, points):
    pipe = client.pipeline()
    pipe.watch(f"user:{from_user}:points")

    current_points = int(client.get(f"user:{from_user}:points") or 0)
    if current_points >= points:
        pipe.multi()
        pipe.decrby(f"user:{from_user}:points", points)
        pipe.incrby(f"user:{to_user}:points", points)
        result = pipe.execute()
        return True
    else:
        pipe.reset()
        return False
```

#### 8. Pub/Sub ç™¼å¸ƒè¨‚é–±ç¯„ä¾‹
```python
import redis
import threading
import time

# ç™¼å¸ƒè€…
def publisher():
    pub_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
    time.sleep(1)  # ç­‰å¾…è¨‚é–±è€…æº–å‚™å¥½

    messages = [
        {"channel": "news", "message": "çªç™¼æ–°èï¼šDragonflyDB æ•ˆèƒ½æ¸¬è©¦çµæœå‡ºçˆ"},
        {"channel": "news", "message": "ç§‘æŠ€æ–°èï¼šPython 3.13 æ­£å¼ç™¼å¸ƒ"},
        {"channel": "chat:room1", "message": "Alice: å¤§å®¶å¥½ï¼"},
        {"channel": "chat:room1", "message": "Bob: å—¨ï¼ŒAliceï¼"},
    ]

    for msg in messages:
        pub_client.publish(msg["channel"], msg["message"])
        print(f"ğŸ“¢ ç™¼å¸ƒåˆ° {msg['channel']}: {msg['message']}")
        time.sleep(0.5)

# è¨‚é–±è€…
def subscriber():
    sub_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
    pubsub = sub_client.pubsub()

    # è¨‚é–±é »é“
    pubsub.subscribe('news', 'chat:room1')

    # æ¥æ”¶è¨Šæ¯
    for message in pubsub.listen():
        if message['type'] == 'message':
            print(f"ğŸ“¨ æ”¶åˆ° [{message['channel']}]: {message['data']}")

# åŸ·è¡Œç™¼å¸ƒè¨‚é–±ç¯„ä¾‹
if __name__ == "__main__":
    # å•Ÿå‹•è¨‚é–±è€…åŸ·è¡Œç·’
    sub_thread = threading.Thread(target=subscriber)
    sub_thread.daemon = True
    sub_thread.start()

    # å•Ÿå‹•ç™¼å¸ƒè€…
    publisher()

    time.sleep(2)  # ç­‰å¾…æ‰€æœ‰è¨Šæ¯è™•ç†å®Œæˆ
```

#### 9. éåŒæ­¥æ“ä½œç¯„ä¾‹
```python
import asyncio
import aioredis

async def async_operations():
    # å»ºç«‹éåŒæ­¥é€£æ¥
    redis = await aioredis.create_redis_pool(
        'redis://localhost:6379',
        encoding='utf-8'
    )

    try:
        # éåŒæ­¥è¨­å®šå€¼
        await redis.set('async:key1', 'value1')
        await redis.set('async:key2', 'value2')

        # éåŒæ­¥æ‰¹é‡æ“ä½œ
        tasks = []
        for i in range(100):
            task = redis.set(f'async:batch:{i}', f'value:{i}')
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
        await asyncio.gather(*tasks)

        # éåŒæ­¥å–å€¼
        value = await redis.get('async:key1')
        print(f"éåŒæ­¥å–å¾—çš„å€¼: {value}")

        # éåŒæ­¥ Pipeline
        pipe = redis.pipeline()
        pipe.incr('async:counter')
        pipe.incr('async:counter')
        pipe.incr('async:counter')
        results = await pipe.execute()
        print(f"Pipeline çµæœ: {results}")

    finally:
        redis.close()
        await redis.wait_closed()

# åŸ·è¡ŒéåŒæ­¥æ“ä½œ
# asyncio.run(async_operations())
```

#### 10. é€£æ¥æ± ç®¡ç†ç¯„ä¾‹
```python
import redis
from redis import ConnectionPool
from contextlib import contextmanager

# å»ºç«‹å…¨åŸŸé€£æ¥æ± 
pool = ConnectionPool(
    host='localhost',
    port=6379,
    decode_responses=True,
    max_connections=50,
    socket_connect_timeout=5,
    socket_timeout=5,
    retry_on_timeout=True,
    health_check_interval=30
)

# é€£æ¥ç®¡ç†å™¨
@contextmanager
def get_redis_connection():
    client = redis.Redis(connection_pool=pool)
    try:
        yield client
    finally:
        # é€£æ¥æœƒè‡ªå‹•è¿”å›æ± ä¸­
        pass

# ä½¿ç”¨é€£æ¥æ± 
def perform_operations():
    with get_redis_connection() as client:
        # åŸ·è¡Œæ“ä½œ
        client.set("pool:test", "value")
        value = client.get("pool:test")
        print(f"ä½¿ç”¨é€£æ¥æ± å–å¾—: {value}")

# ç›£æ§é€£æ¥æ± ç‹€æ…‹
def check_pool_status():
    with get_redis_connection() as client:
        pool_stats = {
            "created_connections": pool.connection_kwargs,
            "max_connections": pool.max_connections,
            "encoding": pool.encoder.encoding
        }
        print(f"é€£æ¥æ± ç‹€æ…‹: {pool_stats}")

perform_operations()
check_pool_status()
```

#### 11. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
```python
import redis
from redis.exceptions import ConnectionError, TimeoutError, RedisError
import time
from functools import wraps

def retry_on_failure(max_retries=3, delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except (ConnectionError, TimeoutError) as e:
                    retries += 1
                    if retries >= max_retries:
                        print(f"âŒ æ“ä½œå¤±æ•—ï¼Œå·²é‡è©¦ {max_retries} æ¬¡")
                        raise
                    print(f"âš ï¸ é€£æ¥éŒ¯èª¤ï¼Œ{delay}ç§’å¾Œé‡è©¦... (ç¬¬{retries}æ¬¡)")
                    time.sleep(delay)
                except RedisError as e:
                    print(f"âŒ Redis éŒ¯èª¤: {e}")
                    raise
            return None
        return wrapper
    return decorator

@retry_on_failure(max_retries=3, delay=2)
def safe_redis_operation():
    client = redis.Redis(host='localhost', port=6379, decode_responses=True)

    # æ¸¬è©¦é€£æ¥
    client.ping()

    # åŸ·è¡Œæ“ä½œ
    result = client.set("safe:key", "value", ex=3600)
    return result

# ä½¿ç”¨å®‰å…¨æ“ä½œ
try:
    result = safe_redis_operation()
    if result:
        print("âœ… æ“ä½œæˆåŠŸ")
except Exception as e:
    print(f"âŒ æœ€çµ‚å¤±æ•—: {e}")
```

#### 12. æ•ˆèƒ½ç›£æ§ç¯„ä¾‹
```python
import redis
import time
from datetime import datetime

class DragonflyDBMonitor:
    def __init__(self, host='localhost', port=6379):
        self.client = redis.Redis(host=host, port=port, decode_responses=True)

    def get_metrics(self):
        """å–å¾— DragonflyDB æ•ˆèƒ½æŒ‡æ¨™"""
        info = self.client.info()

        metrics = {
            'timestamp': datetime.now().isoformat(),
            'clients': {
                'connected': info.get('clients', {}).get('connected_clients', 0),
                'blocked': info.get('clients', {}).get('blocked_clients', 0)
            },
            'memory': {
                'used': info.get('memory', {}).get('used_memory_human', 'N/A'),
                'peak': info.get('memory', {}).get('used_memory_peak_human', 'N/A'),
                'rss': info.get('memory', {}).get('used_memory_rss_human', 'N/A')
            },
            'stats': {
                'total_commands': info.get('stats', {}).get('total_commands_processed', 0),
                'ops_per_sec': info.get('stats', {}).get('instantaneous_ops_per_sec', 0),
                'total_connections': info.get('stats', {}).get('total_connections_received', 0),
                'rejected_connections': info.get('stats', {}).get('rejected_connections', 0)
            },
            'cpu': {
                'used_cpu_sys': info.get('cpu', {}).get('used_cpu_sys', 0),
                'used_cpu_user': info.get('cpu', {}).get('used_cpu_user', 0)
            }
        }

        return metrics

    def benchmark_operations(self, iterations=10000):
        """åŸ·è¡ŒåŸºæº–æ¸¬è©¦"""
        results = {}

        # SET æ“ä½œæ¸¬è©¦
        start = time.time()
        for i in range(iterations):
            self.client.set(f'bench:key:{i}', f'value:{i}')
        set_time = time.time() - start
        results['set_ops_per_sec'] = iterations / set_time

        # GET æ“ä½œæ¸¬è©¦
        start = time.time()
        for i in range(iterations):
            self.client.get(f'bench:key:{i}')
        get_time = time.time() - start
        results['get_ops_per_sec'] = iterations / get_time

        # Pipeline æ¸¬è©¦
        start = time.time()
        pipe = self.client.pipeline()
        for i in range(iterations):
            pipe.set(f'pipe:key:{i}', f'value:{i}')
        pipe.execute()
        pipe_time = time.time() - start
        results['pipeline_ops_per_sec'] = iterations / pipe_time

        # æ¸…ç†æ¸¬è©¦è³‡æ–™
        for i in range(iterations):
            self.client.delete(f'bench:key:{i}', f'pipe:key:{i}')

        return results

    def print_report(self):
        """åˆ—å°æ•ˆèƒ½å ±å‘Š"""
        print("=" * 60)
        print("DragonflyDB æ•ˆèƒ½ç›£æ§å ±å‘Š")
        print("=" * 60)

        metrics = self.get_metrics()

        print(f"\nğŸ“Š é€£æ¥ç‹€æ…‹:")
        print(f"  â€¢ æ´»èºé€£æ¥: {metrics['clients']['connected']}")
        print(f"  â€¢ é˜»å¡é€£æ¥: {metrics['clients']['blocked']}")

        print(f"\nğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨:")
        print(f"  â€¢ ç•¶å‰ä½¿ç”¨: {metrics['memory']['used']}")
        print(f"  â€¢ å°–å³°ä½¿ç”¨: {metrics['memory']['peak']}")
        print(f"  â€¢ RSS: {metrics['memory']['rss']}")

        print(f"\nâš¡ æ•ˆèƒ½æŒ‡æ¨™:")
        print(f"  â€¢ ç¸½è™•ç†å‘½ä»¤: {metrics['stats']['total_commands']:,}")
        print(f"  â€¢ ç•¶å‰ OPS: {metrics['stats']['ops_per_sec']:,}")

        print(f"\nğŸ§ª åŸºæº–æ¸¬è©¦çµæœ:")
        bench_results = self.benchmark_operations(1000)
        print(f"  â€¢ SET æ•ˆèƒ½: {bench_results['set_ops_per_sec']:.0f} ops/sec")
        print(f"  â€¢ GET æ•ˆèƒ½: {bench_results['get_ops_per_sec']:.0f} ops/sec")
        print(f"  â€¢ Pipeline æ•ˆèƒ½: {bench_results['pipeline_ops_per_sec']:.0f} ops/sec")

        print("=" * 60)

# åŸ·è¡Œç›£æ§
if __name__ == "__main__":
    monitor = DragonflyDBMonitor()
    monitor.print_report()
```

### å®Œæ•´æ‡‰ç”¨ç¯„ä¾‹ï¼šå³æ™‚æ’è¡Œæ¦œç³»çµ±
```python
import redis
import random
import time
from datetime import datetime, timedelta

class GameLeaderboard:
    """éŠæˆ²æ’è¡Œæ¦œç³»çµ± - ä½¿ç”¨ DragonflyDB"""

    def __init__(self, host='localhost', port=6379):
        self.client = redis.Redis(host=host, port=port, decode_responses=True)
        self.leaderboard_key = "game:leaderboard:global"
        self.weekly_key = f"game:leaderboard:week:{datetime.now().strftime('%Y-%W')}"
        self.daily_key = f"game:leaderboard:day:{datetime.now().strftime('%Y-%m-%d')}"

    def update_score(self, player_id, score):
        """æ›´æ–°ç©å®¶åˆ†æ•¸"""
        pipe = self.client.pipeline()

        # æ›´æ–°å¤šå€‹æ’è¡Œæ¦œ
        pipe.zadd(self.leaderboard_key, {player_id: score})
        pipe.zadd(self.weekly_key, {player_id: score})
        pipe.zadd(self.daily_key, {player_id: score})

        # è¨˜éŒ„ç©å®¶æœ€é«˜åˆ†
        pipe.hset(f"player:{player_id}", "high_score", score)
        pipe.hset(f"player:{player_id}", "last_played", datetime.now().isoformat())

        pipe.execute()

    def get_leaderboard(self, board_type='global', limit=10):
        """å–å¾—æ’è¡Œæ¦œ"""
        key_map = {
            'global': self.leaderboard_key,
            'weekly': self.weekly_key,
            'daily': self.daily_key
        }

        key = key_map.get(board_type, self.leaderboard_key)
        return self.client.zrevrange(key, 0, limit-1, withscores=True)

    def get_player_rank(self, player_id, board_type='global'):
        """å–å¾—ç©å®¶æ’å"""
        key_map = {
            'global': self.leaderboard_key,
            'weekly': self.weekly_key,
            'daily': self.daily_key
        }

        key = key_map.get(board_type, self.leaderboard_key)
        rank = self.client.zrevrank(key, player_id)
        score = self.client.zscore(key, player_id)

        return {
            'rank': rank + 1 if rank is not None else None,
            'score': score
        }

    def get_nearby_players(self, player_id, distance=2):
        """å–å¾—é™„è¿‘æ’åçš„ç©å®¶"""
        rank = self.client.zrevrank(self.leaderboard_key, player_id)
        if rank is None:
            return []

        start = max(0, rank - distance)
        end = rank + distance

        return self.client.zrevrange(
            self.leaderboard_key,
            start,
            end,
            withscores=True
        )

    def simulate_game(self, num_players=100, num_rounds=10):
        """æ¨¡æ“¬éŠæˆ²é€²è¡Œ"""
        print("ğŸ® é–‹å§‹éŠæˆ²æ¨¡æ“¬...")

        # å»ºç«‹ç©å®¶
        players = [f"player_{i:03d}" for i in range(num_players)]

        # æ¨¡æ“¬å¤šè¼ªéŠæˆ²
        for round_num in range(num_rounds):
            print(f"\nç¬¬ {round_num + 1} è¼ª:")

            # éš¨æ©Ÿé¸æ“‡ç©å®¶ä¸¦æ›´æ–°åˆ†æ•¸
            active_players = random.sample(players, k=random.randint(10, 30))
            for player in active_players:
                score = random.randint(100, 10000)
                self.update_score(player, score)

            # é¡¯ç¤ºç•¶å‰å‰5å
            top_5 = self.get_leaderboard(limit=5)
            print("  ç›®å‰æ’è¡Œæ¦œå‰5å:")
            for i, (player, score) in enumerate(top_5, 1):
                print(f"    {i}. {player}: {score:.0f}åˆ†")

            time.sleep(0.5)

        # é¡¯ç¤ºæœ€çµ‚çµæœ
        print("\n" + "="*50)
        print("ğŸ† æœ€çµ‚æ’è¡Œæ¦œ")
        print("="*50)

        # å…¨çƒæ’è¡Œæ¦œ
        print("\nğŸ“Š å…¨çƒæ’è¡Œæ¦œå‰10å:")
        global_top = self.get_leaderboard('global', limit=10)
        for i, (player, score) in enumerate(global_top, 1):
            print(f"  {i:2d}. {player}: {score:.0f}åˆ†")

        # æŸ¥è©¢ç‰¹å®šç©å®¶
        sample_player = random.choice(players)
        player_stats = self.get_player_rank(sample_player)
        print(f"\nğŸ‘¤ {sample_player} çš„æ’å:")
        print(f"  â€¢ å…¨çƒæ’å: ç¬¬ {player_stats['rank']} å")
        print(f"  â€¢ åˆ†æ•¸: {player_stats['score']:.0f}")

        # é¡¯ç¤ºé™„è¿‘ç©å®¶
        nearby = self.get_nearby_players(sample_player, distance=2)
        print(f"\nğŸ“ {sample_player} é™„è¿‘çš„ç©å®¶:")
        for player, score in nearby:
            marker = " â† (ä½ )" if player == sample_player else ""
            print(f"  â€¢ {player}: {score:.0f}åˆ†{marker}")

# åŸ·è¡Œç¯„ä¾‹
if __name__ == "__main__":
    leaderboard = GameLeaderboard()
    leaderboard.simulate_game(num_players=50, num_rounds=5)
```

## Redis vs DragonflyDB è©³ç´°æ¯”è¼ƒ

### æ¶æ§‹å·®ç•°

| ç‰¹æ€§ | Redis | DragonflyDB |
|------|-------|-------------|
| **æ ¸å¿ƒæ¶æ§‹** | å–®åŸ·è¡Œç·’äº‹ä»¶å¾ªç’° | å¤šåŸ·è¡Œç·’ã€å…±äº«ç„¡é–æ¶æ§‹ |
| **ä¸¦ç™¼æ¨¡å‹** | å–®åŸ·è¡Œç·’è™•ç†å‘½ä»¤ | åˆ©ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ |
| **è¨˜æ†¶é«”ç®¡ç†** | jemalloc | mimalloc + è‡ªè¨‚æœ€ä½³åŒ– |
| **æŒä¹…åŒ–** | RDB + AOF | RDB + æ”¹é€²çš„å¿«ç…§æ©Ÿåˆ¶ |
| **è³‡æ–™çµæ§‹** | æ¨™æº– Redis è³‡æ–™çµæ§‹ | ç›¸åŒè³‡æ–™çµæ§‹ + å…§éƒ¨æœ€ä½³åŒ– |

### æ•ˆèƒ½æ¯”è¼ƒ

| æŒ‡æ¨™ | Redis | DragonflyDB |
|------|-------|-------------|
| **ååé‡** | ~100K ops/sec (å–®æ ¸) | ~4M ops/sec (32æ ¸) |
| **å»¶é²** | < 1ms (P99) | < 1ms (P99) |
| **å‚ç›´æ“´å±•** | å—é™æ–¼å–®æ ¸ | ç·šæ€§æ“´å±•è‡³æ‰€æœ‰æ ¸å¿ƒ |
| **è¨˜æ†¶é«”æ•ˆç‡** | åŸºæº– | ç¯€çœ 30-50% |
| **å•Ÿå‹•æ™‚é–“** | å¿«é€Ÿ | å¿«é€Ÿ |

### åŠŸèƒ½å°æ¯”

| åŠŸèƒ½ | Redis | DragonflyDB |
|------|-------|-------------|
| **Redis å”è­°ç›¸å®¹æ€§** | 100% (åŸç”Ÿ) | 99%+ |
| **å¢é›†æ”¯æ´** | Redis Cluster | å–®ç¯€é»å³å¯è™•ç†å¤§è¦æ¨¡ |
| **Lua è…³æœ¬** | âœ… æ”¯æ´ | âœ… æ”¯æ´ |
| **Pub/Sub** | âœ… æ”¯æ´ | âœ… æ”¯æ´ |
| **äº‹å‹™** | âœ… MULTI/EXEC | âœ… æ”¯æ´ |
| **ä¸²æµ (Streams)** | âœ… æ”¯æ´ | âœ… æ”¯æ´ |
| **æ¨¡çµ„ç³»çµ±** | âœ… è±å¯Œç”Ÿæ…‹ç³» | âŒ ä¸æ”¯æ´ Redis æ¨¡çµ„ |
| **åœ°ç†ç©ºé–“** | âœ… æ”¯æ´ | âœ… æ”¯æ´ |
| **JSON** | éœ€è¦ RedisJSON æ¨¡çµ„ | åŸç”Ÿæ”¯æ´åŸºæœ¬ JSON |

### å„ªç¼ºé»åˆ†æ

#### Redis å„ªé»
âœ… **æˆç†Ÿç©©å®š** - è¶…é 15 å¹´çš„ç”Ÿç”¢ç’°å¢ƒé©—è­‰  
âœ… **ç”Ÿæ…‹ç³»çµ±è±å¯Œ** - å¤§é‡å·¥å…·ã€å®¢æˆ¶ç«¯ã€æ¨¡çµ„  
âœ… **ç¤¾ç¾¤é¾å¤§** - å»£æ³›çš„ç¤¾ç¾¤æ”¯æ´å’Œè³‡æº  
âœ… **æ–‡ä»¶å®Œæ•´** - è©³ç›¡çš„å®˜æ–¹æ–‡ä»¶å’Œæ•™å­¸  
âœ… **æ¨¡çµ„æ“´å±•** - RedisJSONã€RedisSearchã€RedisGraph ç­‰  
âœ… **ä¼æ¥­æ”¯æ´** - Redis Enterprise æä¾›å•†æ¥­æ”¯æ´  

#### Redis ç¼ºé»
âŒ **å–®åŸ·è¡Œç·’é™åˆ¶** - ç„¡æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU  
âŒ **è¨˜æ†¶é«”ä½¿ç”¨è¼ƒé«˜** - ç›¸åŒè³‡æ–™éœ€è¦æ›´å¤šè¨˜æ†¶é«”  
âŒ **æ“´å±•è¤‡é›œ** - éœ€è¦ Redis Cluster æˆ– Sentinel  
âŒ **å¤§è³‡æ–™é›†å•Ÿå‹•æ…¢** - RDB è¼‰å…¥å¯èƒ½å¾ˆæ…¢  
âŒ **Fork é–‹éŠ·** - æŒä¹…åŒ–æ™‚çš„ fork æ“ä½œé–‹éŠ·å¤§  

#### DragonflyDB å„ªé»
âœ… **è¶…é«˜æ•ˆèƒ½** - å¯é” Redis 25 å€æ•ˆèƒ½  
âœ… **å¤šæ ¸å¿ƒåˆ©ç”¨** - è‡ªå‹•ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ  
âœ… **è¨˜æ†¶é«”æ•ˆç‡** - ç¯€çœ 30-50% è¨˜æ†¶é«”  
âœ… **ç„¡éœ€å¢é›†** - å–®å¯¦ä¾‹è™•ç† TB ç´šè³‡æ–™  
âœ… **å¿«ç…§ä¸é˜»å¡** - æ”¹é€²çš„æŒä¹…åŒ–æ©Ÿåˆ¶  
âœ… **ç¾ä»£æ¶æ§‹** - ä½¿ç”¨ C++20 å’Œç¾ä»£æŠ€è¡“  
âœ… **ç›¸å®¹æ€§ä½³** - å¹¾ä¹å®Œå…¨ç›¸å®¹ Redis API  

#### DragonflyDB ç¼ºé»
âŒ **ç›¸å°è¼ƒæ–°** - 2022 å¹´æ¨å‡ºï¼Œç”Ÿç”¢ç’°å¢ƒç¶“é©—è¼ƒå°‘  
âŒ **ç”Ÿæ…‹ç³»çµ±å°** - å·¥å…·å’Œè³‡æºç›¸å°è¼ƒå°‘  
âŒ **ç„¡æ¨¡çµ„æ”¯æ´** - ä¸æ”¯æ´ Redis æ¨¡çµ„  
âŒ **ç¤¾ç¾¤è¼ƒå°** - ç¤¾ç¾¤æ”¯æ´å’Œè³‡æºæœ‰é™  
âŒ **åŠŸèƒ½å·®ç•°** - æŸäº›é€²éšåŠŸèƒ½å¯èƒ½ç•¥æœ‰ä¸åŒ  
âŒ **ä¼æ¥­æ”¯æ´æœ‰é™** - å•†æ¥­æ”¯æ´é¸é …è¼ƒå°‘  

## æ•ˆèƒ½åŸºæº–æ¸¬è©¦

### æ¸¬è©¦ç’°å¢ƒ
- CPU: 32 æ ¸å¿ƒ
- RAM: 128GB
- è³‡æ–™é›†: 10GB

### æ¸¬è©¦çµæœ

#### SET æ“ä½œ (ops/sec)
```
Redis (å–®æ ¸):        120,000
Redis (å¢é›†-8ç¯€é»):   800,000
DragonflyDB:       3,800,000
```

#### GET æ“ä½œ (ops/sec)
```
Redis (å–®æ ¸):        130,000
Redis (å¢é›†-8ç¯€é»):   900,000
DragonflyDB:       4,200,000
```

#### è¨˜æ†¶é«”ä½¿ç”¨ (10M keys)
```
Redis:          8.5 GB
DragonflyDB:    5.2 GB
ç¯€çœ:           39%
```

## é¸æ“‡å»ºè­°

### é¸æ“‡ Redis çš„æƒ…æ³

1. **ç©©å®šæ€§å„ªå…ˆ**
   - é‡‘èã€é†«ç™‚ç­‰é—œéµæ‡‰ç”¨
   - éœ€è¦é•·æœŸç©©å®šé‹è¡Œçš„ç”Ÿç”¢ç’°å¢ƒ

2. **éœ€è¦ç‰¹å®šæ¨¡çµ„**
   - éœ€è¦ RedisSearch é€²è¡Œå…¨æ–‡æœå°‹
   - éœ€è¦ RedisGraph é€²è¡Œåœ–å½¢è³‡æ–™åº«æ“ä½œ
   - éœ€è¦ RedisTimeSeries é€²è¡Œæ™‚åºè³‡æ–™è™•ç†

3. **ä¼æ¥­æ”¯æ´éœ€æ±‚**
   - éœ€è¦å•†æ¥­ç´šæŠ€è¡“æ”¯æ´
   - éœ€è¦ SLA ä¿è­‰

4. **ä¿å®ˆçš„æŠ€è¡“ç­–ç•¥**
   - åœ˜éšŠç†Ÿæ‚‰ Redis
   - ä¸é¡˜æ‰¿æ“”æ–°æŠ€è¡“é¢¨éšª

### é¸æ“‡ DragonflyDB çš„æƒ…æ³

1. **æ•ˆèƒ½éœ€æ±‚é«˜**
   - éœ€è¦è™•ç†ç™¾è¬ç´š QPS
   - ä½å»¶é²è¦æ±‚åš´æ ¼

2. **æˆæœ¬æ•æ„Ÿ**
   - å¸Œæœ›æ¸›å°‘ä¼ºæœå™¨æ•¸é‡
   - éœ€è¦é™ä½è¨˜æ†¶é«”æˆæœ¬

3. **å¤§è¦æ¨¡è³‡æ–™**
   - å–®æ©Ÿéœ€è¦è™•ç† TB ç´šè³‡æ–™
   - ä¸æƒ³ç®¡ç†è¤‡é›œçš„å¢é›†

4. **æ–°å°ˆæ¡ˆ**
   - å…¨æ–°çš„å°ˆæ¡ˆï¼Œæ²’æœ‰æ­·å²åŒ…è¢±
   - å¯ä»¥æ¥å—è¼ƒæ–°æŠ€è¡“

### æ··åˆä½¿ç”¨ç­–ç•¥

```
ç”Ÿç”¢ç’°å¢ƒé—œéµæœå‹™ â†’ Redis
é«˜æµé‡å¿«å–å±¤ â†’ DragonflyDB  
é–‹ç™¼æ¸¬è©¦ç’°å¢ƒ â†’ DragonflyDB
è³‡æ–™åˆ†æå¿«å– â†’ DragonflyDB
```

## é·ç§»æŒ‡å—

### å¾ Redis é·ç§»åˆ° DragonflyDB

1. **ç›¸å®¹æ€§æ¸¬è©¦**
```bash
# ä½¿ç”¨ redis-cli æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
redis-cli -h dragonfly-host ping
redis-cli -h dragonfly-host set test "value"
redis-cli -h dragonfly-host get test
```

2. **è³‡æ–™é·ç§»**
```bash
# æ–¹æ³•ä¸€ï¼šä½¿ç”¨ REPLICAOF
# åœ¨ DragonflyDB ä¸­åŸ·è¡Œ
REPLICAOF redis-host 6379

# æ–¹æ³•äºŒï¼šä½¿ç”¨ redis-dump
redis-dump -h redis-host | redis-load -h dragonfly-host
```

3. **æ‡‰ç”¨ç¨‹å¼èª¿æ•´**
```python
# ä¸éœ€è¦ä¿®æ”¹ç¨‹å¼ç¢¼ï¼Œåªéœ€æ”¹è®Šé€£æ¥å­—ä¸²
# å¾
client = redis.Redis(host='redis-host', port=6379)
# åˆ°
client = redis.Redis(host='dragonfly-host', port=6379)
```

## ç›£æ§å’Œç¶­é‹

### DragonflyDB ç›£æ§æŒ‡æ¨™

```bash
# æŸ¥çœ‹çµ±è¨ˆè³‡è¨Š
redis-cli -h dragonfly-host INFO

# ç›£æ§é‡è¦æŒ‡æ¨™
- used_memory: ä½¿ç”¨çš„è¨˜æ†¶é«”
- connected_clients: é€£æ¥çš„å®¢æˆ¶ç«¯æ•¸
- total_commands_processed: è™•ç†çš„å‘½ä»¤ç¸½æ•¸
- instantaneous_ops_per_sec: å³æ™‚ OPS
```

### æ•ˆèƒ½å„ªåŒ–å»ºè­°

1. **DragonflyDB å„ªåŒ–**
```bash
# è¨­å®šæœ€å¤§è¨˜æ†¶é«”
./dragonfly --maxmemory=32gb

# è¨­å®šåŸ·è¡Œç·’æ•¸ï¼ˆé è¨­ä½¿ç”¨æ‰€æœ‰æ ¸å¿ƒï¼‰
./dragonfly --proactor_threads=16

# å•Ÿç”¨å¿«ç…§
./dragonfly --dbfilename=dump.rdb --save_schedule="0 1"
```

2. **å®¢æˆ¶ç«¯å„ªåŒ–**
```python
# ä½¿ç”¨é€£æ¥æ± 
pool = redis.ConnectionPool(
    host='localhost',
    port=6379,
    max_connections=50
)
client = redis.Redis(connection_pool=pool)

# ä½¿ç”¨ Pipeline æ‰¹æ¬¡æ“ä½œ
pipe = client.pipeline()
for i in range(10000):
    pipe.set(f'key_{i}', f'value_{i}')
pipe.execute()
```

## ç¸½çµ

### å¿«é€Ÿæ±ºç­–çŸ©é™£

| éœ€æ±‚ | æ¨è–¦é¸æ“‡ |
|------|----------|
| ç©©å®šæ€§æœ€é‡è¦ | Redis |
| æ•ˆèƒ½æœ€é‡è¦ | DragonflyDB |
| éœ€è¦ Redis æ¨¡çµ„ | Redis |
| æˆæœ¬æ§åˆ¶ | DragonflyDB |
| å°å‹æ‡‰ç”¨ | Redis |
| å¤§è¦æ¨¡æ‡‰ç”¨ | DragonflyDB |
| ä¿å®ˆç­–ç•¥ | Redis |
| å‰µæ–°ç­–ç•¥ | DragonflyDB |

### æœªä¾†å±•æœ›

- **Redis**: æŒçºŒå„ªåŒ–ï¼ŒåŠ å¼·ä¼æ¥­åŠŸèƒ½ï¼Œæ“´å±•æ¨¡çµ„ç”Ÿæ…‹
- **DragonflyDB**: å¿«é€Ÿç™¼å±•ï¼Œå¢åŠ åŠŸèƒ½ï¼Œå»ºç«‹ç”Ÿæ…‹ç³»çµ±

å…©è€…éƒ½æ˜¯å„ªç§€çš„è¨˜æ†¶é«”è³‡æ–™åº«ï¼Œé¸æ“‡å–æ±ºæ–¼å…·é«”éœ€æ±‚ã€é¢¨éšªæ‰¿å—åº¦å’ŒæŠ€è¡“ç­–ç•¥ã€‚å»ºè­°å…ˆåœ¨éé—œéµç’°å¢ƒæ¸¬è©¦ DragonflyDBï¼Œè©•ä¼°æ˜¯å¦ç¬¦åˆéœ€æ±‚å¾Œå†æ±ºå®šæ˜¯å¦æ¡ç”¨ã€‚