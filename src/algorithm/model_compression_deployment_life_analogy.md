# æ¨¡å‹å£“ç¸®èˆ‡éƒ¨ç½²å®Œæ•´æŒ‡å— - ç”¨ç”Ÿæ´»æ¯”å–»ç†è§£

## ğŸ¯ æ ¸å¿ƒæ¯”å–»ï¼šæ¬å®¶æ‰“åŒ… vs ç›´æ¥æ¬

### å•é¡Œï¼šæ·±åº¦å­¸ç¿’æ¨¡å‹å¤ªå¤§

```
æƒ…å¢ƒï¼šè¨“ç·´å¥½ä¸€å€‹ AI æ¨¡å‹

è¨“ç·´ç’°å¢ƒï¼ˆå¯¦é©—å®¤ï¼‰ï¼š
- GPUï¼š8 å¼µ A100ï¼ˆæ¯å¼µ $15,000ï¼‰
- è¨˜æ†¶é«”ï¼š512 GB
- æ¨¡å‹å¤§å°ï¼š10 GB
- æ¨ç†é€Ÿåº¦ï¼š100 ms

éƒ¨ç½²ç’°å¢ƒï¼ˆæ‰‹æ©Ÿï¼‰ï¼š
- CPUï¼šæ‰‹æ©Ÿæ™¶ç‰‡
- è¨˜æ†¶é«”ï¼š4 GB
- å¯ç”¨ç©ºé–“ï¼š200 MB
- è¦æ±‚é€Ÿåº¦ï¼š<50 ms

å•é¡Œï¼š
âŒ æ¨¡å‹å¤ªå¤§ï¼ˆæ”¾ä¸ä¸‹ï¼‰
âŒ è¨ˆç®—å¤ªæ…¢ï¼ˆç”¨ä¸äº†ï¼‰
âŒ è€—é›»å¤ªå¤šï¼ˆé›»æ± æ’ä¸ä½ï¼‰

æ¯”å–»ï¼š
å°±åƒæŠŠã€Œè±ªå®…ã€æ¬åˆ°ã€Œå¥—æˆ¿ã€
â†’ éœ€è¦ã€Œå£“ç¸®ã€å’Œã€Œå„ªåŒ–ã€
```

---

## ğŸ“š æ¨¡å‹å£“ç¸®çš„å››å¤§æŠ€è¡“

### 1. å‰ªæï¼ˆPruningï¼‰- æ–·æ¨é›¢

**æ¯”å–»**ï¼šæ¬å®¶æ™‚ä¸Ÿæ‰ä¸å¸¸ç”¨çš„æ±è¥¿

```
ç¥ç¶“ç¶²è·¯ï¼š
1000 è¬å€‹åƒæ•¸

ç™¼ç¾ï¼š
- 30% çš„åƒæ•¸ã€Œæ¥è¿‘é›¶ã€ï¼ˆå¹¾ä¹ä¸èµ·ä½œç”¨ï¼‰
- 20% çš„åƒæ•¸ã€Œé‡è¤‡ã€ï¼ˆå¤šé¤˜ï¼‰

å‰ªæï¼š
ç§»é™¤ã€Œä¸é‡è¦ã€çš„åƒæ•¸
â†’ åªä¿ç•™ 50% åƒæ•¸
â†’ æ¨¡å‹è®Šå°ä¸€åŠï¼

é—œéµå•é¡Œï¼š
å¦‚ä½•åˆ¤æ–·ã€Œé‡è¦æ€§ã€ï¼Ÿ
```

**éçµæ§‹åŒ–å‰ªæ**ï¼š

```python
def magnitude_pruning(model, sparsity=0.5):
    """
    æ¬Šé‡å‰ªæ

    æ¯”å–»ï¼š
    ä¸Ÿæ‰ã€Œæ•¸å€¼å°ã€çš„åƒæ•¸
    ï¼ˆèªç‚ºå®ƒå€‘ä¸é‡è¦ï¼‰

    åƒæ•¸ï¼š
        sparsity: ç¨€ç–åº¦ï¼ˆ0.5 = ä¿ç•™ 50%ï¼‰
    """
    for layer in model.layers:
        weights = layer.weights

        # è¨ˆç®—é–¾å€¼ï¼ˆä¿ç•™å‰ 50% å¤§çš„æ¬Šé‡ï¼‰
        threshold = np.percentile(np.abs(weights), sparsity * 100)

        # å‰µå»ºé®ç½©ï¼ˆmaskï¼‰
        mask = np.abs(weights) >= threshold

        # æ‡‰ç”¨é®ç½©ï¼ˆå°æ–¼é–¾å€¼çš„è¨­ç‚º 0ï¼‰
        layer.weights = weights * mask

    return model

# ä½¿ç”¨
compressed_model = magnitude_pruning(model, sparsity=0.5)
# æ¨¡å‹å¤§å°æ¸›å°‘ 50%
```

**çµæ§‹åŒ–å‰ªæ**ï¼š

```python
def channel_pruning(model, prune_ratio=0.3):
    """
    é€šé“å‰ªæ

    æ¯”å–»ï¼š
    ç§»é™¤ã€Œæ•´å€‹é€šé“ã€ï¼ˆä¸€æ•´æ’ç¥ç¶“å…ƒï¼‰
    è€Œä¸æ˜¯å–®å€‹æ¬Šé‡

    å„ªé»ï¼š
    - æ›´è¦å‰‡ï¼ˆGPU å‹å¥½ï¼‰
    - å¯¦éš›åŠ é€Ÿæ›´æ˜é¡¯
    """
    for layer in model.conv_layers:
        # è©•ä¼°æ¯å€‹é€šé“çš„é‡è¦æ€§
        channel_importance = []
        for channel in layer.channels:
            # æ–¹æ³• 1ï¼šL1 ç¯„æ•¸
            importance = np.sum(np.abs(channel.weights))

            # æ–¹æ³• 2ï¼šæ¿€æ´»å€¼çµ±è¨ˆ
            # importance = np.mean(channel.activations)

            channel_importance.append(importance)

        # ç§»é™¤æœ€ä¸é‡è¦çš„ 30% é€šé“
        num_prune = int(len(channel_importance) * prune_ratio)
        prune_indices = np.argsort(channel_importance)[:num_prune]

        # ç§»é™¤é€šé“
        layer.remove_channels(prune_indices)

    return model
```

---

### 2. é‡åŒ–ï¼ˆQuantizationï¼‰- é™ä½ç²¾åº¦

**æ¯”å–»**ï¼šç”¨ã€Œç°¡åŒ–ç‰ˆã€ä»£æ›¿ã€Œå®Œæ•´ç‰ˆã€

```
åŸå§‹æ¨¡å‹ï¼ˆFP32ï¼‰ï¼š
æ¬Šé‡ = 3.14159265358979...ï¼ˆ32 ä½æµ®é»æ•¸ï¼‰
å¤§å°ï¼šæ¯å€‹åƒæ•¸ 4 bytes

é‡åŒ–æ¨¡å‹ï¼ˆINT8ï¼‰ï¼š
æ¬Šé‡ = 3ï¼ˆ8 ä½æ•´æ•¸ï¼‰
å¤§å°ï¼šæ¯å€‹åƒæ•¸ 1 byte
â†’ ç¸®å° 4 å€ï¼

æ¯”å–»ï¼š
åŸæœ¬ï¼šã€Œé€™å€‹è˜‹æœé‡ 123.456789 å…¬å…‹ã€
é‡åŒ–ï¼šã€Œé€™å€‹è˜‹æœé‡ 123 å…¬å…‹ã€
â†’ ç²¾åº¦ç•¥é™ï¼Œä½†ã€Œå¤ ç”¨ã€
```

**é‡åŒ–æ­¥é©Ÿ**ï¼š

```python
def quantize_weights(weights, num_bits=8):
    """
    æ¬Šé‡é‡åŒ–

    æ¯”å–»ï¼š
    æŠŠã€Œé€£çºŒå€¼ã€è½‰æˆã€Œé›¢æ•£å€¼ã€

    æ­¥é©Ÿï¼š
    1. æ‰¾å‡ºæœ€å°å€¼å’Œæœ€å¤§å€¼
    2. åˆ†æˆ 256 å€‹å€é–“ï¼ˆ8-bitï¼‰
    3. æ¯å€‹æ¬Šé‡æ˜ å°„åˆ°æœ€è¿‘çš„å€é–“
    """
    # 1. è¨ˆç®—ç¯„åœ
    w_min = np.min(weights)
    w_max = np.max(weights)

    # 2. è¨ˆç®—ç¸®æ”¾å› å­
    num_levels = 2 ** num_bits  # 8-bit = 256 å€‹ç­‰ç´š
    scale = (w_max - w_min) / (num_levels - 1)

    # 3. é‡åŒ–
    quantized = np.round((weights - w_min) / scale).astype(np.int8)

    # 4. åé‡åŒ–ï¼ˆæ¨ç†æ™‚ï¼‰
    dequantized = quantized * scale + w_min

    return quantized, scale, w_min


# ä½¿ç”¨
original_weights = model.layer1.weights  # FP32
quantized, scale, offset = quantize_weights(original_weights, num_bits=8)

print(f"åŸå§‹å¤§å°: {original_weights.nbytes} bytes")
print(f"é‡åŒ–å¤§å°: {quantized.nbytes} bytes")
print(f"å£“ç¸®æ¯”: {original_weights.nbytes / quantized.nbytes:.1f}x")
```

**é‡åŒ–æ„ŸçŸ¥è¨“ç·´ï¼ˆQATï¼‰**ï¼š

```python
def quantization_aware_training(model, train_data):
    """
    é‡åŒ–æ„ŸçŸ¥è¨“ç·´

    æ¯”å–»ï¼š
    ã€Œé‚Šè¨“ç·´é‚Šé‡åŒ–ã€
    è®“æ¨¡å‹ã€Œé©æ‡‰ã€é‡åŒ–èª¤å·®

    æ­¥é©Ÿï¼š
    1. è¨“ç·´æ™‚ï¼šæ’å…¥ã€Œå‡é‡åŒ–ã€æ“ä½œ
    2. å‰å‘å‚³æ’­ï¼šæ¨¡æ“¬é‡åŒ–æ•ˆæœ
    3. åå‘å‚³æ’­ï¼šæ­£å¸¸è¨ˆç®—æ¢¯åº¦
    4. æ¨¡å‹å­¸æœƒã€Œè£œå„Ÿã€é‡åŒ–èª¤å·®
    """
    for epoch in range(epochs):
        for batch_x, batch_y in train_data:
            # å‰å‘å‚³æ’­ï¼ˆæ’å…¥å‡é‡åŒ–ï¼‰
            h1 = model.layer1(batch_x)
            h1_quantized = fake_quantize(h1, num_bits=8)  # æ¨¡æ“¬é‡åŒ–

            h2 = model.layer2(h1_quantized)
            h2_quantized = fake_quantize(h2, num_bits=8)

            output = model.layer3(h2_quantized)

            # è¨ˆç®—æå¤±ä¸¦æ›´æ–°
            loss = compute_loss(output, batch_y)
            model.backward(loss)

    # è¨“ç·´å¾Œï¼Œç›´æ¥éƒ¨ç½²ï¼ˆç„¡éœ€é‡æ–°å¾®èª¿ï¼‰
    return model


def fake_quantize(tensor, num_bits=8):
    """
    å‡é‡åŒ–ï¼ˆFake Quantizationï¼‰

    æ¯”å–»ï¼š
    é‡åŒ–å¾Œç«‹åˆ»åé‡åŒ–
    ä¿æŒ FP32 æ ¼å¼ï¼Œä½†æ¨¡æ“¬é‡åŒ–èª¤å·®
    """
    # é‡åŒ–
    t_min, t_max = tensor.min(), tensor.max()
    scale = (t_max - t_min) / (2**num_bits - 1)
    quantized = np.round((tensor - t_min) / scale)

    # åé‡åŒ–
    dequantized = quantized * scale + t_min

    return dequantized  # ä»æ˜¯ FP32ï¼Œä½†å¸¶é‡åŒ–èª¤å·®
```

---

### 3. çŸ¥è­˜è’¸é¤¾ï¼ˆKnowledge Distillationï¼‰- è€å¸«æ•™å­¸ç”Ÿ

**æ¯”å–»**ï¼šåå¸«æ•™æ™®é€šå­¸ç”Ÿ

```
å¤§æ¨¡å‹ï¼ˆTeacherï¼‰ï¼š
- 1000 è¬åƒæ•¸
- æº–ç¢ºç‡ 95%
- æ¨ç†æ…¢

å°æ¨¡å‹ï¼ˆStudentï¼‰ï¼š
- 10 è¬åƒæ•¸
- æº–ç¢ºç‡ 85%ï¼ˆè‡ªå·±è¨“ç·´ï¼‰
- æ¨ç†å¿«

çŸ¥è­˜è’¸é¤¾ï¼š
è®“ã€Œå°æ¨¡å‹ã€æ¨¡ä»¿ã€Œå¤§æ¨¡å‹ã€
â†’ å°æ¨¡å‹æº–ç¢ºç‡æå‡åˆ° 92%ï¼

é—œéµï¼š
å­¸ç”Ÿä¸åªå­¸ã€Œç­”æ¡ˆã€ï¼ˆhard labelï¼‰
é‚„å­¸ã€Œæ€è€ƒéç¨‹ã€ï¼ˆsoft labelï¼‰
```

**å¯¦ä½œ**ï¼š

```python
class KnowledgeDistillation:
    def __init__(self, teacher_model, student_model, temperature=3.0):
        """
        çŸ¥è­˜è’¸é¤¾

        æ¯”å–»ï¼š
        è€å¸«ï¼ˆå¤§æ¨¡å‹ï¼‰æ•™å­¸ç”Ÿï¼ˆå°æ¨¡å‹ï¼‰

        åƒæ•¸ï¼š
            teacher_model: æ•™å¸«æ¨¡å‹ï¼ˆå·²è¨“ç·´å¥½ï¼‰
            student_model: å­¸ç”Ÿæ¨¡å‹ï¼ˆè¦è¨“ç·´ï¼‰
            temperature: æº«åº¦åƒæ•¸ï¼ˆæ§åˆ¶ã€Œè»ŸåŒ–ã€ç¨‹åº¦ï¼‰
        """
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature

    def soft_labels(self, logits, temperature):
        """
        è»Ÿæ¨™ç±¤

        æ¯”å–»ï¼š
        è€å¸«ä¸åªèªªã€Œç­”æ¡ˆæ˜¯ Aã€
        é‚„èªªã€ŒA æœ‰ 70% æ©Ÿç‡ï¼ŒB æœ‰ 20%ï¼ŒC æœ‰ 10%ã€
        â†’ æ›´å¤šè³‡è¨Šï¼

        åƒæ•¸ï¼š
            logits: åŸå§‹è¼¸å‡ºï¼ˆæœª softmaxï¼‰
            temperature: æº«åº¦ï¼ˆè¶Šé«˜è¶Šã€Œè»Ÿã€ï¼‰
        """
        # é«˜æº« softmaxï¼ˆåˆ†ä½ˆæ›´å¹³æ»‘ï¼‰
        scaled_logits = logits / temperature
        soft = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits))

        return soft

    def distillation_loss(self, student_logits, teacher_logits, true_labels, alpha=0.5):
        """
        è’¸é¤¾æå¤±

        æ¯”å–»ï¼š
        å­¸ç”Ÿçš„æˆç¸¾ç”±å…©éƒ¨åˆ†çµ„æˆï¼š
        1. æ¨¡ä»¿è€å¸«ï¼ˆ50%ï¼‰
        2. åšå°é¡Œç›®ï¼ˆ50%ï¼‰

        å…¬å¼ï¼š
            Loss = Î± Ã— KL(student, teacher) + (1-Î±) Ã— CE(student, labels)
                   â†‘                          â†‘
                æ¨¡ä»¿è€å¸«                    åšå°é¡Œç›®
        """
        # æå¤± 1ï¼šæ¨¡ä»¿è€å¸«ï¼ˆKL æ•£åº¦ï¼‰
        teacher_soft = self.soft_labels(teacher_logits, self.temperature)
        student_soft = self.soft_labels(student_logits, self.temperature)

        kl_loss = -np.sum(teacher_soft * np.log(student_soft + 1e-8))

        # æå¤± 2ï¼šåšå°é¡Œç›®ï¼ˆäº¤å‰ç†µï¼‰
        ce_loss = cross_entropy(student_logits, true_labels)

        # ç¸½æå¤±
        total_loss = alpha * kl_loss + (1 - alpha) * ce_loss

        return total_loss

    def train_student(self, train_data, epochs=10):
        """
        è¨“ç·´å­¸ç”Ÿæ¨¡å‹

        æµç¨‹ï¼š
        1. æ•™å¸«æ¨¡å‹ï¼šç”Ÿæˆè»Ÿæ¨™ç±¤
        2. å­¸ç”Ÿæ¨¡å‹ï¼šå­¸ç¿’è»Ÿæ¨™ç±¤
        3. è©•ä¼°ï¼šæ¯”è¼ƒå­¸ç”Ÿå’Œæ•™å¸«çš„æ€§èƒ½
        ```"""
        for epoch in range(epochs):
            for batch_x, batch_y in train_data:
                # 1. æ•™å¸«æ¨¡å‹ï¼šç”Ÿæˆè»Ÿæ¨™ç±¤ï¼ˆä¸åå‘å‚³æ’­ï¼‰
                teacher_logits = self.teacher.forward(batch_x)

                # 2. å­¸ç”Ÿæ¨¡å‹ï¼šå‰å‘å‚³æ’­
                student_logits = self.student.forward(batch_x)

                # 3. è¨ˆç®—è’¸é¤¾æå¤±
                loss = self.distillation_loss(
                    student_logits,
                    teacher_logits,
                    batch_y,
                    alpha=0.5
                )

                # 4. åå‘å‚³æ’­ï¼ˆåªæ›´æ–°å­¸ç”Ÿï¼‰
                self.student.backward(loss)

            # è©•ä¼°
            if epoch % 10 == 0:
                accuracy = self.evaluate_student()
                print(f"Epoch {epoch}: Student Accuracy = {accuracy:.2%}")

        return self.student


# ä½¿ç”¨ç¯„ä¾‹
def distill_model():
    """è’¸é¤¾ç¤ºç¯„"""

    # 1. è¨“ç·´å¤§æ¨¡å‹ï¼ˆæ•™å¸«ï¼‰
    teacher = ResNet152()  # 1000 è¬åƒæ•¸
    teacher.train(train_data)
    teacher_accuracy = 95.3%

    # 2. å‰µå»ºå°æ¨¡å‹ï¼ˆå­¸ç”Ÿï¼‰
    student = MobileNet()  # 40 è¬åƒæ•¸

    # 3. çŸ¥è­˜è’¸é¤¾
    distiller = KnowledgeDistillation(teacher, student, temperature=3.0)
    distilled_student = distiller.train_student(train_data)

    # 4. è©•ä¼°
    student_accuracy_before = 85.2%  # ç›´æ¥è¨“ç·´
    student_accuracy_after = 92.1%   # è’¸é¤¾å¾Œ

    print(f"æ•™å¸«æº–ç¢ºç‡: {teacher_accuracy}")
    print(f"å­¸ç”Ÿæº–ç¢ºç‡ï¼ˆç›´æ¥è¨“ç·´ï¼‰: {student_accuracy_before}")
    print(f"å­¸ç”Ÿæº–ç¢ºç‡ï¼ˆè’¸é¤¾å¾Œï¼‰: {student_accuracy_after}")
    print(f"æå‡: +{student_accuracy_after - student_accuracy_before:.1f}%")
```

---

### 4. ä½ç§©åˆ†è§£ï¼ˆLow-Rank Factorizationï¼‰- ç°¡åŒ–é‹ç®—

**æ¯”å–»**ï¼šç”¨ã€Œå¿«æ·æ–¹å¼ã€ä»£æ›¿ã€Œç¹é è·¯ã€

```
åŸå§‹çŸ©é™£ä¹˜æ³•ï¼š
W: 1000 Ã— 1000ï¼ˆ100 è¬å€‹åƒæ•¸ï¼‰
x: 1000 Ã— 1
y = W Ã— xï¼ˆéœ€è¦ 100 è¬æ¬¡ä¹˜æ³•ï¼‰

ä½ç§©åˆ†è§£ï¼š
W â‰ˆ U Ã— V
U: 1000 Ã— 10ï¼ˆ1 è¬å€‹åƒæ•¸ï¼‰
V: 10 Ã— 1000ï¼ˆ1 è¬å€‹åƒæ•¸ï¼‰
ç¸½å…±ï¼š2 è¬å€‹åƒæ•¸ï¼ˆå£“ç¸® 50 å€ï¼ï¼‰

è¨ˆç®—ï¼š
y = U Ã— (V Ã— x)
   = 1000Ã—10 æ¬¡ + 10Ã—1000 æ¬¡
   = 2 è¬æ¬¡ä¹˜æ³•ï¼ˆåŠ é€Ÿ 50 å€ï¼ï¼‰

æ¯”å–»ï¼š
åŸæœ¬ï¼šå¾å°åŒ—ã€Œç¹ä¸€åœˆã€åˆ°é«˜é›„
åˆ†è§£ï¼šå°åŒ— â†’ ä¸­ç¹¼ç«™ â†’ é«˜é›„ï¼ˆæ›´å¿«ï¼‰
```

```python
def low_rank_decomposition(weight_matrix, rank=10):
    """
    ä½ç§©åˆ†è§£ï¼ˆSVDï¼‰

    æ¯”å–»ï¼š
    æŠŠã€Œå¤§çŸ©é™£ã€æ‹†æˆã€Œå…©å€‹å°çŸ©é™£ã€

    åƒæ•¸ï¼š
        weight_matrix: åŸå§‹æ¬Šé‡çŸ©é™£ (M, N)
        rank: ç§©ï¼ˆè¶Šå°å£“ç¸®è¶Šå¤šï¼‰

    è¿”å›ï¼š
        U: (M, rank)
        V: (rank, N)
        ä½¿å¾— W â‰ˆ U Ã— V
    """
    # å¥‡ç•°å€¼åˆ†è§£ï¼ˆSVDï¼‰
    U, S, Vt = np.linalg.svd(weight_matrix, full_matrices=False)

    # åªä¿ç•™å‰ k å€‹å¥‡ç•°å€¼
    U_k = U[:, :rank]
    S_k = np.diag(S[:rank])
    V_k = Vt[:rank, :]

    # é‡æ§‹
    U_compressed = U_k @ np.sqrt(S_k)
    V_compressed = np.sqrt(S_k) @ V_k

    # é©—è­‰è¿‘ä¼¼èª¤å·®
    reconstructed = U_compressed @ V_compressed
    error = np.linalg.norm(weight_matrix - reconstructed) / np.linalg.norm(weight_matrix)
    print(f"é‡æ§‹èª¤å·®: {error:.4f}")

    return U_compressed, V_compressed


# ä½¿ç”¨
# åŸå§‹å±¤ï¼š1000 Ã— 1000
W = model.layer1.weights  # (1000, 1000)

# åˆ†è§£
U, V = low_rank_decomposition(W, rank=10)

# æ›¿æ›
model.layer1 = TwoLayerFactorized(U, V)
# ç¾åœ¨åªéœ€ 2 è¬åƒæ•¸ï¼ˆåŸæœ¬ 100 è¬ï¼‰
```

---

## ğŸš€ æ¨¡å‹éƒ¨ç½²ç­–ç•¥

### 1. ONNXï¼ˆOpen Neural Network Exchangeï¼‰

**ç›®çš„**ï¼šè·¨æ¡†æ¶éƒ¨ç½²

```
å•é¡Œï¼š
è¨“ç·´ï¼šPyTorch
éƒ¨ç½²ï¼šTensorFlow Liteï¼ˆæ‰‹æ©Ÿï¼‰
â†’ ä¸å…¼å®¹ï¼

è§£æ±ºï¼šONNX ä¸­é–“æ ¼å¼
PyTorch â†’ ONNX â†’ TensorFlow Lite
```

```python
import torch
import torch.onnx

def export_to_onnx(pytorch_model, output_path):
    """
    å°å‡º PyTorch æ¨¡å‹ç‚º ONNX

    æ¯”å–»ï¼š
    æŠŠã€ŒPyTorch èªè¨€ã€ç¿»è­¯æˆã€Œé€šç”¨èªè¨€ã€
    ```"""
    # å‰µå»ºç¤ºä¾‹è¼¸å…¥
    dummy_input = torch.randn(1, 3, 224, 224)

    # å°å‡º
    torch.onnx.export(
        pytorch_model,
        dummy_input,
        output_path,
        opset_version=11,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )

    print(f"æ¨¡å‹å·²å°å‡ºåˆ°: {output_path}")


# ä½¿ç”¨
model = torchvision.models.resnet50(pretrained=True)
export_to_onnx(model, "resnet50.onnx")

# åœ¨å…¶ä»–æ¡†æ¶è¼‰å…¥
import onnxruntime
session = onnxruntime.InferenceSession("resnet50.onnx")
```

### 2. TensorRTï¼ˆNVIDIA æ¨ç†åŠ é€Ÿï¼‰

**ç›®çš„**ï¼šGPU æ¨ç†å„ªåŒ–

```python
import tensorrt as trt

def optimize_with_tensorrt(onnx_path, engine_path):
    """
    ç”¨ TensorRT å„ªåŒ–æ¨¡å‹

    æ¯”å–»ï¼š
    é‡å°ã€ŒNVIDIA GPUã€åšå°ˆé–€å„ªåŒ–
    â†’ æ¨ç†é€Ÿåº¦æå‡ 5-10 å€

    å„ªåŒ–æŠ€è¡“ï¼š
    1. å±¤èåˆï¼ˆLayer Fusionï¼‰
    2. ç²¾åº¦æ ¡æº–ï¼ˆINT8 é‡åŒ–ï¼‰
    3. æ ¸å‡½æ•¸å„ªåŒ–
    ```"""
    # å‰µå»º builder
    logger = trt.Logger(trt.Logger.WARNING)
    builder = trt.Builder(logger)
    network = builder.create_network()
    parser = trt.OnnxParser(network, logger)

    # è§£æ ONNX
    with open(onnx_path, 'rb') as model:
        parser.parse(model.read())

    # é…ç½®
    config = builder.create_builder_config()
    config.max_workspace_size = 1 << 30  # 1 GB
    config.set_flag(trt.BuilderFlag.FP16)  # ä½¿ç”¨ FP16

    # æ§‹å»ºå¼•æ“
    engine = builder.build_engine(network, config)

    # ä¿å­˜
    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())

    print(f"TensorRT å¼•æ“å·²ä¿å­˜åˆ°: {engine_path}")

# ä½¿ç”¨
optimize_with_tensorrt("model.onnx", "model.trt")
```

### 3. TensorFlow Liteï¼ˆæ‰‹æ©Ÿéƒ¨ç½²ï¼‰

**ç›®çš„**ï¼šç§»å‹•è¨­å‚™æ¨ç†

```python
import tensorflow as tf

def convert_to_tflite(keras_model, output_path, quantize=True):
    """
    è½‰æ›ç‚º TensorFlow Lite

    æ¯”å–»ï¼š
    æŠŠã€Œå®Œæ•´ç‰ˆã€æ”¹æˆã€Œæ‰‹æ©Ÿç‰ˆã€

    å„ªåŒ–ï¼š
    1. é‡åŒ–ï¼ˆINT8ï¼‰
    2. ç®—å­èåˆ
    3. ç§»é™¤è¨“ç·´ç”¨æ“ä½œ
    """
    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)

    if quantize:
        # è¨“ç·´å¾Œé‡åŒ–
        converter.optimizations = [tf.lite.Optimize.DEFAULT]

        # ä»£è¡¨æ€§æ•¸æ“šé›†ï¼ˆç”¨æ–¼æ ¡æº–ï¼‰
        def representative_dataset():
            for _ in range(100):
                yield [np.random.randn(1, 224, 224, 3).astype(np.float32)]

        converter.representative_dataset = representative_dataset

        # å¼·åˆ¶ INT8
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.inference_input_type = tf.uint8
        converter.inference_output_type = tf.uint8

    # è½‰æ›
    tflite_model = converter.convert()

    # ä¿å­˜
    with open(output_path, 'wb') as f:
        f.write(tflite_model)

    print(f"TFLite æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    print(f"å¤§å°: {len(tflite_model) / 1024:.2f} KB")

# ä½¿ç”¨
model = tf.keras.applications.MobileNetV2()
convert_to_tflite(model, "mobilenet_v2.tflite", quantize=True)
```

---

## ğŸ¯ å¯¦æˆ°ï¼šå®Œæ•´å£“ç¸®æµç¨‹

```python
class ModelCompressor:
    """å®Œæ•´çš„æ¨¡å‹å£“ç¸®æµç¨‹"""

    def __init__(self, model):
        self.model = model

    def compress(self, methods=['prune', 'quantize', 'distill']):
        """
        å®Œæ•´å£“ç¸®æµç¨‹

        æ¯”å–»ï¼š
        æ¬å®¶çš„å®Œæ•´æµç¨‹
        1. æ–·æ¨é›¢ï¼ˆå‰ªæï¼‰
        2. æ‰“åŒ…å£“ç¸®ï¼ˆé‡åŒ–ï¼‰
        3. æ‰¾æ¬å®¶å…¬å¸ï¼ˆè’¸é¤¾ï¼‰
        """
        compressed_model = self.model.copy()

        # 1. å‰ªæ
        if 'prune' in methods:
            print("æ­¥é©Ÿ 1: å‰ªæ...")
            compressed_model = self.prune_model(compressed_model, sparsity=0.5)
            self.evaluate("å‰ªæå¾Œ", compressed_model)

        # 2. é‡åŒ–
        if 'quantize' in methods:
            print("\næ­¥é©Ÿ 2: é‡åŒ–...")
            compressed_model = self.quantize_model(compressed_model, bits=8)
            self.evaluate("é‡åŒ–å¾Œ", compressed_model)

        # 3. çŸ¥è­˜è’¸é¤¾
        if 'distill' in methods:
            print("\næ­¥é©Ÿ 3: çŸ¥è­˜è’¸é¤¾...")
            student_model = self.create_student_model()
            compressed_model = self.distill(self.model, student_model)
            self.evaluate("è’¸é¤¾å¾Œ", compressed_model)

        return compressed_model

    def evaluate(self, stage, model):
        """è©•ä¼°å£“ç¸®æ•ˆæœ"""
        size = model.get_size()
        accuracy = model.evaluate(test_data)
        latency = model.measure_latency()

        print(f"{stage}:")
        print(f"  å¤§å°: {size / 1024:.2f} MB")
        print(f"  æº–ç¢ºç‡: {accuracy:.2%}")
        print(f"  å»¶é²: {latency:.2f} ms")


# ä½¿ç”¨
compressor = ModelCompressor(large_model)
compressed = compressor.compress(methods=['prune', 'quantize', 'distill'])
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
æ­¥é©Ÿ 1: å‰ªæ...
å‰ªæå¾Œ:
  å¤§å°: 50.00 MBï¼ˆåŸæœ¬ 100.00 MBï¼‰
  æº–ç¢ºç‡: 94.2%ï¼ˆåŸæœ¬ 95.0%ï¼‰
  å»¶é²: 80.00 msï¼ˆåŸæœ¬ 100.00 msï¼‰

æ­¥é©Ÿ 2: é‡åŒ–...
é‡åŒ–å¾Œ:
  å¤§å°: 12.50 MBï¼ˆå£“ç¸® 8 å€ï¼‰
  æº–ç¢ºç‡: 93.8%ï¼ˆ-1.2%ï¼‰
  å»¶é²: 30.00 msï¼ˆåŠ é€Ÿ 3.3 å€ï¼‰

æ­¥é©Ÿ 3: çŸ¥è­˜è’¸é¤¾...
è’¸é¤¾å¾Œ:
  å¤§å°: 5.00 MBï¼ˆå£“ç¸® 20 å€ï¼‰
  æº–ç¢ºç‡: 94.5%ï¼ˆåƒ… -0.5%ï¼‰
  å»¶é²: 15.00 msï¼ˆåŠ é€Ÿ 6.7 å€ï¼‰
```

---

## ğŸ“Š å£“ç¸®æŠ€è¡“æ¯”è¼ƒ

| æŠ€è¡“ | å£“ç¸®æ¯” | ç²¾åº¦æå¤± | åŠ é€Ÿæ¯” | é›£åº¦ |
|------|--------|---------|--------|------|
| **å‰ªæ** | 2-5x | å° | 1-2x | ä¸­ |
| **é‡åŒ–** | 4x | å°-ä¸­ | 2-4x | æ˜“ |
| **çŸ¥è­˜è’¸é¤¾** | 10-100x | ä¸­ | 10x+ | é›£ |
| **ä½ç§©åˆ†è§£** | 2-3x | å° | 2-3x | ä¸­ |
| **çµ„åˆä½¿ç”¨** | 20-100x | ä¸­ | 10-50x | é›£ |

---

## ğŸ“ å¯¦å‹™å»ºè­°

### 1. å£“ç¸®ç­–ç•¥é¸æ“‡

```
å ´æ™¯ 1ï¼šé›²ç«¯æ¨ç†ï¼ˆæœ‰ GPUï¼‰
æ¨è–¦ï¼šé‡åŒ–ï¼ˆFP16ï¼‰+ TensorRT
åŸå› ï¼šé«˜æ•ˆèƒ½ï¼Œç•¥é™ç²¾åº¦å¯æ¥å—

å ´æ™¯ 2ï¼šæ‰‹æ©Ÿ APP
æ¨è–¦ï¼šè’¸é¤¾ + é‡åŒ–ï¼ˆINT8ï¼‰+ TFLite
åŸå› ï¼šæ¨¡å‹å°ã€çœé›»

å ´æ™¯ 3ï¼šé‚Šç·£è¨­å‚™ï¼ˆIoTï¼‰
æ¨è–¦ï¼šæ¥µè‡´è’¸é¤¾ + å‰ªæ + 4-bit é‡åŒ–
åŸå› ï¼šè³‡æºæ¥µåº¦å—é™

å ´æ™¯ 4ï¼šè¦æ±‚é«˜ç²¾åº¦
æ¨è–¦ï¼šå‰ªæ + FP16 é‡åŒ–
åŸå› ï¼šç²¾åº¦æå¤±æœ€å°
```

### 2. å£“ç¸®-ç²¾åº¦ Trade-off

```python
# å¯¦é©—ä¸åŒå£“ç¸®ç­‰ç´š
compression_levels = {
    'light': {
        'prune': 0.3,
        'quantize': 'fp16',
        'distill': False,
    },
    'medium': {
        'prune': 0.5,
        'quantize': 'int8',
        'distill': False,
    },
    'aggressive': {
        'prune': 0.7,
        'quantize': 'int8',
        'distill': True,
        'distill_ratio': 0.1,  # å­¸ç”Ÿæ¨¡å‹åªæœ‰ 10% å¤§å°
    }
}

# æ¸¬è©¦
for level, config in compression_levels.items():
    compressed = compress_model(model, config)
    evaluate(compressed, level)
```

### 3. éƒ¨ç½²æª¢æŸ¥æ¸…å–®

```
âœ… æ¨¡å‹å¤§å°ï¼šæ˜¯å¦ç¬¦åˆè¨­å‚™é™åˆ¶ï¼Ÿ
âœ… æ¨ç†é€Ÿåº¦ï¼šæ˜¯å¦æ»¿è¶³å¯¦æ™‚è¦æ±‚ï¼Ÿ
âœ… æº–ç¢ºç‡ï¼šæ˜¯å¦åœ¨å¯æ¥å—ç¯„åœï¼Ÿ
âœ… è¨˜æ†¶é«”ï¼šå³°å€¼è¨˜æ†¶é«”æ˜¯å¦éé«˜ï¼Ÿ
âœ… é›»é‡æ¶ˆè€—ï¼šæ˜¯å¦è€—é›»éå¤šï¼Ÿ
âœ… å…¼å®¹æ€§ï¼šç›®æ¨™å¹³å°æ˜¯å¦æ”¯æŒï¼Ÿ
âœ… ç©©å®šæ€§ï¼šé•·æ™‚é–“é‹è¡Œæ˜¯å¦ç©©å®šï¼Ÿ
```

---

## ğŸ”— ç¸½çµ

### æ¨¡å‹å£“ç¸®æ ¸å¿ƒæ€æƒ³

1. **å‰ªæ**ï¼šç§»é™¤ä¸é‡è¦åƒæ•¸
2. **é‡åŒ–**ï¼šé™ä½æ•¸å€¼ç²¾åº¦
3. **è’¸é¤¾**ï¼šè¨“ç·´å°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹
4. **åˆ†è§£**ï¼šç°¡åŒ–è¨ˆç®—éç¨‹

### ä¸»è¦å„ªå‹¢

- âœ… æ¨¡å‹è®Šå°ï¼ˆé™ä½å­˜å„²éœ€æ±‚ï¼‰
- âœ… æ¨ç†æ›´å¿«ï¼ˆé™ä½å»¶é²ï¼‰
- âœ… çœé›»ï¼ˆå»¶é•·é›»æ± å£½å‘½ï¼‰
- âœ… é™ä½éƒ¨ç½²æˆæœ¬

### ä¸»è¦æŒ‘æˆ°

- âš ï¸ ç²¾åº¦æå¤±ï¼ˆéœ€è¦æ¬Šè¡¡ï¼‰
- âš ï¸ å£“ç¸®å·¥ç¨‹è¤‡é›œ
- âš ï¸ ç¡¬é«”å…¼å®¹æ€§å•é¡Œ
- âš ï¸ èª¿å„ªè€—æ™‚

### å¯¦ç”¨å·¥å…·

- **PyTorch Mobile**ï¼šPyTorch æ¨¡å‹éƒ¨ç½²
- **TensorFlow Lite**ï¼šTF æ¨¡å‹éƒ¨ç½²
- **ONNX Runtime**ï¼šè·¨å¹³å°æ¨ç†
- **TensorRT**ï¼šNVIDIA GPU å„ªåŒ–
- **OpenVINO**ï¼šIntel CPU å„ªåŒ–

### æœªä¾†æ–¹å‘

- ç¥ç¶“æ¶æ§‹æœç´¢ï¼ˆNASï¼‰è‡ªå‹•æ‰¾å°æ¨¡å‹
- ç¡¬é«”æ„ŸçŸ¥å£“ç¸®ï¼ˆé‡å°ç‰¹å®šæ™¶ç‰‡å„ªåŒ–ï¼‰
- å‹•æ…‹ç¶²è·¯ï¼ˆæ ¹æ“šè¼¸å…¥èª¿æ•´è¨ˆç®—é‡ï¼‰
- æ··åˆç²¾åº¦è¨“ç·´èˆ‡æ¨ç†

---

*æœ€å¾Œæ›´æ–°: 2025-11-26*
