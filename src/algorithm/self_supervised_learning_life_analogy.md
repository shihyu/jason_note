# è‡ªç›£ç£å­¸ç¿’å®Œæ•´æŒ‡å— - ç”¨ç”Ÿæ´»æ¯”å–»ç†è§£

## ğŸ¯ æ ¸å¿ƒæ¯”å–»ï¼šè‡ªå­¸ vs ä¸Šèª²

### ä¸‰ç¨®å­¸ç¿’æ–¹å¼å°æ¯”

```
ç›£ç£å­¸ç¿’ï¼ˆSupervised Learningï¼‰ï¼š
è€å¸«æ•™ä½ ã€Œæ¨™æº–ç­”æ¡ˆã€

ä¾‹å­ï¼š
è€å¸«ï¼šã€Œé€™æ˜¯è²“ã€ï¼ˆçµ¦æ¨™ç±¤ï¼‰
å­¸ç”Ÿï¼šã€Œæˆ‘è¨˜ä½äº†ã€

å•é¡Œï¼šéœ€è¦å¤§é‡ã€Œæ¨™è¨»æ•¸æ“šã€ï¼ˆæ˜‚è²´ï¼‰
```

```
ç„¡ç›£ç£å­¸ç¿’ï¼ˆUnsupervised Learningï¼‰ï¼š
è‡ªå·±æ‰¾è¦å¾‹ï¼Œæ²’æœ‰ç­”æ¡ˆ

ä¾‹å­ï¼š
å­¸ç”Ÿçœ‹ 1000 å¼µåœ–ç‰‡
ç™¼ç¾ï¼šã€Œæœ‰äº›åœ–ç‰‡å¾ˆåƒï¼ˆéƒ½æ˜¯å‹•ç‰©ï¼‰ã€

å•é¡Œï¼šå­¸åˆ°çš„æ±è¥¿ä¸ä¸€å®šæœ‰ç”¨
```

```
è‡ªç›£ç£å­¸ç¿’ï¼ˆSelf-Supervised Learningï¼‰ï¼š
è‡ªå·±å‡ºé¡Œã€è‡ªå·±ç­”é¡Œ

ä¾‹å­ï¼š
å­¸ç”Ÿçœ‹ä¸€å¥è©±ï¼šã€Œæˆ‘æ„›___ã€
è‡ªå·±å‰µé€ ä»»å‹™ï¼šã€Œå¡«ç©ºé¡Œï¼Œç­”æ¡ˆæ˜¯ã€ä½ ã€ã€
â†’ é€šéã€Œè‡ªå·±å‰µé€ çš„ä»»å‹™ã€å­¸ç¿’

å„ªé»ï¼š
âœ… ä¸éœ€è¦äººå·¥æ¨™è¨»ï¼ˆçœéŒ¢ï¼‰
âœ… å­¸åˆ°çš„ç‰¹å¾µæ›´é€šç”¨ï¼ˆæœ‰ç”¨ï¼‰
âœ… å¯ä»¥ä½¿ç”¨æµ·é‡æ•¸æ“š
```

---

## ğŸ“š ç”Ÿæ´»åŒ–æ¡ˆä¾‹ 1ï¼šæ‹¼åœ–éŠæˆ²

### æƒ…å¢ƒï¼šå­¸ç¿’åœ–åƒç‰¹å¾µ

```
ç›£ç£å­¸ç¿’ï¼š
è€å¸«å‘Šè¨´ä½ ã€Œé€™æ˜¯è²“ã€ã€Œé€™æ˜¯ç‹—ã€
â†’ éœ€è¦ 10 è¬å¼µæ¨™è¨»åœ–ç‰‡

è‡ªç›£ç£å­¸ç¿’ï¼š
è‡ªå·±ç©ã€Œæ‹¼åœ–éŠæˆ²ã€
â†’ ä¸éœ€è¦æ¨™è¨»ï¼

æ­¥é©Ÿï¼š
1. æ‹¿ä¸€å¼µåœ–ç‰‡ï¼ˆæ²’æœ‰æ¨™ç±¤ï¼‰
2. åˆ‡æˆ 9 å¡Šæ‹¼åœ–
3. æ‰“äº‚é †åº
4. ä»»å‹™ï¼šæŠŠæ‹¼åœ–ã€Œé‚„åŸã€

å­¸åˆ°ä»€éº¼ï¼Ÿ
- ç‰©é«”çš„é‚Šç·£
- é¡è‰²çš„é€£çºŒæ€§
- ç´‹ç†ç‰¹å¾µ
â†’ é›–ç„¶æ²’æœ‰æ¨™ç±¤ï¼Œä½†å­¸åˆ°äº†ã€Œåœ–åƒç†è§£ã€ï¼
```

### é·ç§»å­¸ç¿’

```
è¨“ç·´ï¼šç”¨ 100 è¬å¼µã€Œç„¡æ¨™è¨»ã€åœ–ç‰‡ç©æ‹¼åœ–
â†’ å­¸æœƒç†è§£åœ–åƒ

å¾®èª¿ï¼šç”¨ 1000 å¼µã€Œæœ‰æ¨™è¨»ã€åœ–ç‰‡åšåˆ†é¡
â†’ å¿«é€Ÿå­¸æœƒè²“ç‹—åˆ†é¡

æ¯”å–»ï¼š
å…ˆã€Œè‡ªå­¸ã€åŸºç¤çŸ¥è­˜ï¼ˆå¤§é‡æ•¸æ“šï¼‰
å†ã€Œä¸Šèª²ã€å­¸å°ˆæ¥­æŠ€èƒ½ï¼ˆå°‘é‡æ¨™è¨»æ•¸æ“šï¼‰
â†’ äº‹åŠåŠŸå€ï¼
```

---

## ğŸ—ï¸ è‡ªç›£ç£å­¸ç¿’çš„æ ¸å¿ƒæŠ€å·§

### 1. Pretext Taskï¼ˆå‰ç½®ä»»å‹™ï¼‰

**æ¦‚å¿µ**ï¼šè¨­è¨ˆã€Œè‡ªå‹•ç”Ÿæˆæ¨™ç±¤ã€çš„ä»»å‹™

```
ä»»å‹™ 1ï¼šåœ–åƒæ—‹è½‰é æ¸¬
1. éš¨æ©Ÿæ—‹è½‰åœ–ç‰‡ï¼ˆ0Â°, 90Â°, 180Â°, 270Â°ï¼‰
2. ä»»å‹™ï¼šé æ¸¬ã€Œæ—‹è½‰äº†å¹¾åº¦ï¼Ÿã€
3. æ¨™ç±¤ï¼šè‡ªå‹•ç”Ÿæˆï¼ˆæ—‹è½‰è§’åº¦ï¼‰

å­¸åˆ°ä»€éº¼ï¼Ÿ
- ç‰©é«”çš„æ–¹å‘
- ç©ºé–“é—œä¿‚

ä»»å‹™ 2ï¼šæ‹¼åœ–æ’åº
1. åˆ‡æˆ 9 å¡Šï¼Œæ‰“äº‚
2. ä»»å‹™ï¼šã€Œç¬¬ 5 å¡Šåœ¨å“ªï¼Ÿã€
3. æ¨™ç±¤ï¼šåŸå§‹ä½ç½®ï¼ˆè‡ªå‹•çŸ¥é“ï¼‰

å­¸åˆ°ä»€éº¼ï¼Ÿ
- ç‰©é«”çš„çµæ§‹
- éƒ¨åˆ†èˆ‡æ•´é«”çš„é—œä¿‚

ä»»å‹™ 3ï¼šé¡è‰²åŒ–
1. æŠŠå½©è‰²åœ–ç‰‡è®Šç°éš
2. ä»»å‹™ï¼šã€Œé‚„åŸé¡è‰²ã€
3. æ¨™ç±¤ï¼šåŸå§‹é¡è‰²ï¼ˆè‡ªå‹•çŸ¥é“ï¼‰

å­¸åˆ°ä»€éº¼ï¼Ÿ
- ç‰©é«”çš„èªç¾©ï¼ˆå¤©ç©ºæ˜¯è—è‰²ã€è‰åœ°æ˜¯ç¶ è‰²ï¼‰
```

### 2. å°æ¯”å­¸ç¿’ï¼ˆContrastive Learningï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šç›¸ä¼¼çš„è¦æ¥è¿‘ï¼Œä¸ç›¸ä¼¼çš„è¦é é›¢

```
æ¯”å–»ï¼šæ•´ç†ç…§ç‰‡

ä½ æœ‰ 1000 å¼µç…§ç‰‡ï¼š
- ã€Œæˆ‘åœ¨æµ·é‚Šã€çš„ç…§ç‰‡ â†’ æ‡‰è©²å¾ˆç›¸ä¼¼
- ã€Œæˆ‘åœ¨æµ·é‚Šã€vsã€Œè²“å’ªã€â†’ æ‡‰è©²ä¸ç›¸ä¼¼

å°æ¯”å­¸ç¿’ï¼š
1. åŒä¸€å¼µåœ–çš„ã€Œä¸åŒè¦–è§’ã€â†’ æ‹‰è¿‘ï¼ˆæ­£æ¨£æœ¬å°ï¼‰
2. ä¸åŒåœ–ç‰‡ â†’ æ¨é ï¼ˆè² æ¨£æœ¬å°ï¼‰

æ•¸å­¸ï¼š
ç›¸ä¼¼åº¦(åœ–1-å¢å¼·ç‰ˆ, åœ–1-å¦ä¸€å¢å¼·ç‰ˆ) â†’ é«˜
ç›¸ä¼¼åº¦(åœ–1, åœ–2) â†’ ä½
```

**æ•¸æ“šå¢å¼·**ï¼š

```python
def create_positive_pair(image):
    """
    å‰µå»ºæ­£æ¨£æœ¬å°

    æ¯”å–»ï¼š
    åŒä¸€å¼µç…§ç‰‡çš„ã€Œå…©å€‹ç‰ˆæœ¬ã€
    ï¼ˆæœ¬è³ªç›¸åŒï¼Œçœ‹èµ·ä¾†ä¸åŒï¼‰

    æ–¹æ³•ï¼š
    - è£åˆ‡ä¸åŒå€åŸŸ
    - æ”¹è®Šé¡è‰²
    - ç¿»è½‰
    - åŠ å™ªéŸ³
    """
    # å¢å¼·ç‰ˆæœ¬ 1
    aug1 = random_crop(image)
    aug1 = color_jitter(aug1)
    aug1 = horizontal_flip(aug1)

    # å¢å¼·ç‰ˆæœ¬ 2ï¼ˆä¸åŒçš„éš¨æ©Ÿæ“ä½œï¼‰
    aug2 = random_crop(image)
    aug2 = color_jitter(aug2)
    aug2 = rotation(aug2)

    return aug1, aug2

# ä½¿ç”¨
img = load_image("cat.jpg")
positive_pair = create_positive_pair(img)
# positive_pair[0] å’Œ positive_pair[1] æ‡‰è©²ã€Œç›¸ä¼¼ã€
```

---

## ğŸ’» ç¶“å…¸ç®—æ³•å¯¦ä½œ

### 1. SimCLRï¼ˆSimple Contrastive Learningï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šæœ€å¤§åŒ–æ­£æ¨£æœ¬å°çš„ç›¸ä¼¼åº¦

```python
import numpy as np

class SimCLR:
    def __init__(self, encoder, projection_dim=128):
        """
        SimCLR å°æ¯”å­¸ç¿’

        æ¯”å–»ï¼š
        è¨“ç·´æ¨¡å‹ã€Œèªå‡ºã€åŒä¸€å¼µåœ–çš„ä¸åŒç‰ˆæœ¬

        åƒæ•¸ï¼š
            encoder: ç·¨ç¢¼å™¨ï¼ˆå¦‚ ResNetï¼‰
            projection_dim: æŠ•å½±é ­ç¶­åº¦
        """
        self.encoder = encoder
        self.projection_head = self.build_projection_head(projection_dim)

    def build_projection_head(self, dim):
        """
        æŠ•å½±é ­ï¼šæŠŠç‰¹å¾µæŠ•å½±åˆ°ã€Œå°æ¯”ç©ºé–“ã€

        æ¯”å–»ï¼š
        æŠŠè¤‡é›œçš„åœ–åƒç‰¹å¾µ
        å£“ç¸®æˆã€Œç°¡å–®çš„æŒ‡ç´‹ã€ï¼ˆ128 ç¶­å‘é‡ï¼‰
        """
        class ProjectionHead:
            def __init__(self, input_dim, output_dim):
                self.w1 = np.random.randn(input_dim, 256) * 0.01
                self.w2 = np.random.randn(256, output_dim) * 0.01

            def forward(self, x):
                h = np.maximum(0, x.dot(self.w1))  # ReLU
                z = h.dot(self.w2)
                # L2 æ­¸ä¸€åŒ–ï¼ˆè®“å‘é‡åœ¨å–®ä½çƒä¸Šï¼‰
                z = z / (np.linalg.norm(z) + 1e-8)
                return z

        return ProjectionHead(2048, dim)  # å‡è¨­ encoder è¼¸å‡º 2048 ç¶­

    def compute_similarity(self, z_i, z_j):
        """
        è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰

        æ¯”å–»ï¼š
        å…©å€‹ã€ŒæŒ‡ç´‹ã€æœ‰å¤šåƒï¼Ÿ

        å…¬å¼ï¼š
            sim(z_i, z_j) = z_i Â· z_j / (||z_i|| Ã— ||z_j||)

        å€¼åŸŸï¼š-1 åˆ° 1
        1 = å®Œå…¨ç›¸åŒ
        0 = ç„¡é—œ
        -1 = å®Œå…¨ç›¸å
        """
        return np.dot(z_i, z_j)

    def nt_xent_loss(self, z_i, z_j, temperature=0.5):
        """
        NT-Xent æå¤±ï¼ˆå°æ¯”æå¤±ï¼‰

        æ¯”å–»ï¼š
        ã€Œæ­£æ¨£æœ¬å°ã€æ‡‰è©²ç›¸ä¼¼ï¼ˆé«˜åˆ†ï¼‰
        ã€Œè² æ¨£æœ¬å°ã€æ‡‰è©²ä¸ç›¸ä¼¼ï¼ˆä½åˆ†ï¼‰

        åƒæ•¸ï¼š
            z_i, z_j: æ­£æ¨£æœ¬å°çš„è¡¨ç¤º
            temperature: æº«åº¦åƒæ•¸ï¼ˆæ§åˆ¶ã€ŒæŸ”å’Œåº¦ã€ï¼‰
        """
        # è¨ˆç®—èˆ‡æ‰€æœ‰æ¨£æœ¬çš„ç›¸ä¼¼åº¦
        batch_size = len(z_i)

        # æ­£æ¨£æœ¬ç›¸ä¼¼åº¦
        pos_sim = self.compute_similarity(z_i, z_j) / temperature

        # è² æ¨£æœ¬ç›¸ä¼¼åº¦ï¼ˆèˆ‡å…¶ä»–æ‰€æœ‰æ¨£æœ¬ï¼‰
        neg_sims = []
        for k in range(batch_size):
            if k != i:  # æ’é™¤è‡ªå·±
                neg_sim = self.compute_similarity(z_i, z_j[k]) / temperature
                neg_sims.append(neg_sim)

        # NT-Xent æå¤±
        # loss = -log(exp(pos_sim) / (exp(pos_sim) + Î£exp(neg_sims)))
        numerator = np.exp(pos_sim)
        denominator = numerator + np.sum(np.exp(neg_sims))
        loss = -np.log(numerator / denominator)

        return loss

    def train_step(self, batch_images):
        """
        è¨“ç·´ä¸€æ­¥

        æ¯”å–»ï¼š
        1. æ‹¿ä¸€æ‰¹åœ–ç‰‡
        2. æ¯å¼µåœ–è£½ä½œã€Œå…©å€‹å¢å¼·ç‰ˆæœ¬ã€
        3. è¨“ç·´æ¨¡å‹ã€Œèªå‡ºã€å®ƒå€‘æ˜¯åŒä¸€å¼µ
        """
        losses = []

        for image in batch_images:
            # 1. æ•¸æ“šå¢å¼·ï¼ˆå‰µå»ºæ­£æ¨£æœ¬å°ï¼‰
            aug1, aug2 = create_positive_pair(image)

            # 2. ç·¨ç¢¼
            h_i = self.encoder.forward(aug1)
            h_j = self.encoder.forward(aug2)

            # 3. æŠ•å½±
            z_i = self.projection_head.forward(h_i)
            z_j = self.projection_head.forward(h_j)

            # 4. è¨ˆç®—æå¤±
            loss = self.nt_xent_loss(z_i, z_j)
            losses.append(loss)

        return np.mean(losses)

    def extract_features(self, image):
        """
        æå–ç‰¹å¾µï¼ˆè¨“ç·´å¾Œä½¿ç”¨ï¼‰

        æ¯”å–»ï¼š
        æŠŠåœ–ç‰‡è½‰æˆã€ŒæŒ‡ç´‹ã€
        ç”¨æ–¼ä¸‹æ¸¸ä»»å‹™ï¼ˆåˆ†é¡ã€æª¢ç´¢ç­‰ï¼‰
        """
        h = self.encoder.forward(image)
        return h  # åªç”¨ç·¨ç¢¼å™¨ï¼Œä¸ç”¨æŠ•å½±é ­


# ä½¿ç”¨ç¯„ä¾‹
def train_simclr():
    """è¨“ç·´ SimCLR"""

    # æº–å‚™æ•¸æ“šï¼ˆç„¡æ¨™è¨»ï¼ï¼‰
    unlabeled_images = load_unlabeled_images()  # 100 è¬å¼µåœ–ç‰‡

    # å‰µå»º SimCLR
    encoder = ResNet50()  # ç·¨ç¢¼å™¨
    simclr = SimCLR(encoder)

    # è¨“ç·´
    for epoch in range(100):
        for batch in get_batches(unlabeled_images, batch_size=256):
            loss = simclr.train_step(batch)

            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

    # å¾®èª¿ï¼ˆç”¨å°‘é‡æ¨™è¨»æ•¸æ“šï¼‰
    labeled_images, labels = load_labeled_images()  # 1000 å¼µ

    # å‡çµç·¨ç¢¼å™¨ï¼Œåªè¨“ç·´åˆ†é¡é ­
    classifier = FullyConnected(input_dim=2048, output_dim=10)
    fine_tune(simclr.encoder, classifier, labeled_images, labels)

train_simclr()
```

---

### 2. BERTï¼ˆMasked Language Modelï¼‰

**ä»»å‹™**ï¼šå¡«ç©ºé¡Œ

```
åŸå§‹å¥å­ï¼š
ã€Œæˆ‘æ„›åƒè˜‹æœã€

é®è”½ç‰ˆæœ¬ï¼š
ã€Œæˆ‘æ„›åƒ[MASK]ã€

ä»»å‹™ï¼š
é æ¸¬ [MASK] æ˜¯ä»€éº¼
â†’ ç­”æ¡ˆï¼šè˜‹æœ

æ¨™ç±¤ï¼š
è‡ªå‹•ç”Ÿæˆï¼ˆåŸå§‹è©ï¼‰
```

**å¯¦ä½œ**ï¼š

```python
class BERT:
    def __init__(self, vocab_size, hidden_dim):
        """
        BERT è‡ªç›£ç£å­¸ç¿’

        æ¯”å–»ï¼š
        ç©ã€Œå¡«ç©ºéŠæˆ²ã€å­¸ç¿’èªè¨€

        åƒæ•¸ï¼š
            vocab_size: è©å½™é‡
            hidden_dim: éš±è—å±¤ç¶­åº¦
        """
        self.vocab_size = vocab_size
        self.transformer = Transformer(
            vocab_size=vocab_size,
            d_model=hidden_dim,
            num_layers=12
        )

    def mask_tokens(self, tokens, mask_prob=0.15):
        """
        éš¨æ©Ÿé®è”½å–®è©

        æ¯”å–»ï¼š
        éš¨æ©Ÿå¡—é»‘ 15% çš„å­—

        ç­–ç•¥ï¼š
        - 80%ï¼šæ›¿æ›æˆ [MASK]
        - 10%ï¼šæ›¿æ›æˆéš¨æ©Ÿè©
        - 10%ï¼šä¿æŒåŸæ¨£

        ç‚ºä»€éº¼é€™æ¨£ï¼Ÿ
        é¿å…æ¨¡å‹ã€Œä¾è³´ã€[MASK] æ¨™è¨˜
        ```"""
        masked_tokens = tokens.copy()
        labels = tokens.copy()

        for i in range(len(tokens)):
            if np.random.rand() < mask_prob:
                rand = np.random.rand()

                if rand < 0.8:
                    # 80%ï¼šé®è”½
                    masked_tokens[i] = MASK_TOKEN
                elif rand < 0.9:
                    # 10%ï¼šéš¨æ©Ÿè©
                    masked_tokens[i] = np.random.randint(self.vocab_size)
                # 10%ï¼šä¿æŒåŸæ¨£ï¼ˆä¸åšè™•ç†ï¼‰

            else:
                # ä¸é®è”½çš„è©ï¼Œæ¨™ç±¤è¨­ç‚º -1ï¼ˆä¸è¨ˆç®—æå¤±ï¼‰
                labels[i] = -1

        return masked_tokens, labels

    def forward(self, tokens):
        """
        å‰å‘å‚³æ’­

        æ¯”å–»ï¼š
        çœ‹ã€Œå¡—é»‘ã€çš„å¥å­
        çŒœæ¸¬è¢«å¡—é»‘çš„å­—æ˜¯ä»€éº¼
        """
        # é€šé Transformer
        hidden_states = self.transformer.forward(tokens)

        # é æ¸¬æ¯å€‹ä½ç½®çš„è©
        predictions = self.predict_tokens(hidden_states)

        return predictions

    def train_step(self, sentences):
        """
        è¨“ç·´ä¸€æ­¥

        æ¯”å–»ï¼š
        1. æ‹¿ä¸€æ‰¹å¥å­
        2. éš¨æ©Ÿå¡—é»‘ 15% çš„å­—
        3. è¨“ç·´æ¨¡å‹çŒœæ¸¬è¢«å¡—é»‘çš„å­—
        """
        total_loss = 0

        for sentence in sentences:
            # 1. é®è”½
            masked_tokens, labels = self.mask_tokens(sentence)

            # 2. é æ¸¬
            predictions = self.forward(masked_tokens)

            # 3. è¨ˆç®—æå¤±ï¼ˆåªè¨ˆç®—è¢«é®è”½çš„è©ï¼‰
            loss = 0
            for i, label in enumerate(labels):
                if label != -1:  # è¢«é®è”½çš„è©
                    loss += cross_entropy(predictions[i], label)

            total_loss += loss

        return total_loss / len(sentences)


# ä½¿ç”¨ç¯„ä¾‹
def pretrain_bert():
    """é è¨“ç·´ BERT"""

    # æº–å‚™æ•¸æ“šï¼ˆç„¡æ¨™è¨»ï¼ï¼‰
    unlabeled_text = load_wikipedia()  # ç¶­åŸºç™¾ç§‘å…¨æ–‡

    # å‰µå»º BERT
    bert = BERT(vocab_size=30000, hidden_dim=768)

    # é è¨“ç·´
    for epoch in range(100):
        for batch in get_batches(unlabeled_text, batch_size=256):
            loss = bert.train_step(batch)

            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")

    # å¾®èª¿ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰
    labeled_reviews, sentiments = load_movie_reviews()  # 1000 æ¢è©•è«–

    fine_tune_classifier(bert, labeled_reviews, sentiments)

pretrain_bert()
```

---

### 3. MoCoï¼ˆMomentum Contrastï¼‰

**å‰µæ–°**ï¼šç”¨ã€ŒéšŠåˆ—ã€å­˜å„²è² æ¨£æœ¬

```
SimCLR çš„å•é¡Œï¼š
éœ€è¦ã€Œå¤§æ‰¹æ¬¡ã€ï¼ˆbatch size = 4096ï¼‰
â†’ éœ€è¦å¾ˆå¤š GPUï¼ˆæ˜‚è²´ï¼‰

MoCo çš„è§£æ±ºï¼š
ç”¨ã€ŒéšŠåˆ—ã€å­˜å„²ã€Œéå»çš„ã€è² æ¨£æœ¬
â†’ å¯ä»¥ç”¨å°æ‰¹æ¬¡ï¼ˆbatch size = 256ï¼‰

æ¯”å–»ï¼š
SimCLR = æ¯æ¬¡è€ƒè©¦ï¼Œè€ƒå®˜éƒ½æ˜¯ã€Œæ–°é¢å­”ã€
MoCo = è€ƒå®˜æ˜¯ã€Œéå»å¹¾æ¬¡ã€è€ƒè©¦çš„è€ƒå®˜
â†’ ä¸éœ€è¦åŒæ™‚è«‹å¾ˆå¤šè€ƒå®˜
```

```python
class MoCo:
    def __init__(self, encoder, queue_size=65536):
        """
        MoCo å°æ¯”å­¸ç¿’

        æ¯”å–»ï¼š
        ç¶­è­·ä¸€å€‹ã€Œè¨˜æ†¶éšŠåˆ—ã€å­˜å„²éå»çš„æ¨£æœ¬

        åƒæ•¸ï¼š
            encoder: ç·¨ç¢¼å™¨
            queue_size: éšŠåˆ—å¤§å°ï¼ˆè² æ¨£æœ¬æ•¸é‡ï¼‰
        """
        # Query ç·¨ç¢¼å™¨ï¼ˆæ­£å¸¸æ›´æ–°ï¼‰
        self.encoder_q = encoder

        # Key ç·¨ç¢¼å™¨ï¼ˆå‹•é‡æ›´æ–°ï¼‰
        self.encoder_k = encoder.copy()

        # éšŠåˆ—ï¼ˆå­˜å„²éå»çš„è² æ¨£æœ¬ï¼‰
        self.queue = np.zeros((queue_size, 128))
        self.queue_ptr = 0

        # å‹•é‡ä¿‚æ•¸
        self.momentum = 0.999

    def momentum_update(self):
        """
        å‹•é‡æ›´æ–° Key ç·¨ç¢¼å™¨

        æ¯”å–»ï¼š
        Key ç·¨ç¢¼å™¨ã€Œæ…¢æ…¢ã€è·Ÿéš¨ Query ç·¨ç¢¼å™¨

        å…¬å¼ï¼š
            Î¸_k = m Ã— Î¸_k + (1-m) Ã— Î¸_q
            â†‘     â†‘         â†‘
          Key    ä¿ç•™99.9%  å€Ÿé‘‘0.1%
        """
        for param_q, param_k in zip(
            self.encoder_q.parameters(),
            self.encoder_k.parameters()
        ):
            param_k = self.momentum * param_k + (1 - self.momentum) * param_q

    def enqueue_dequeue(self, keys):
        """
        æ›´æ–°éšŠåˆ—

        æ¯”å–»ï¼š
        ã€Œå…ˆé€²å…ˆå‡ºã€
        æ–°æ¨£æœ¬é€²ä¾†ï¼ŒèˆŠæ¨£æœ¬å‡ºå»
        """
        batch_size = len(keys)

        # åŠ å…¥éšŠåˆ—
        self.queue[self.queue_ptr:self.queue_ptr + batch_size] = keys

        # æ›´æ–°æŒ‡é‡
        self.queue_ptr = (self.queue_ptr + batch_size) % len(self.queue)

    def train_step(self, batch_images):
        """è¨“ç·´ä¸€æ­¥"""

        losses = []

        for image in batch_images:
            # 1. å‰µå»ºæ­£æ¨£æœ¬å°
            query, key = create_positive_pair(image)

            # 2. ç·¨ç¢¼
            q = self.encoder_q.forward(query)  # Query
            k = self.encoder_k.forward(key)    # Keyï¼ˆä¸åå‘å‚³æ’­ï¼‰

            # 3. è¨ˆç®—ç›¸ä¼¼åº¦
            # æ­£æ¨£æœ¬ï¼šq å’Œ k
            pos_sim = np.dot(q, k)

            # è² æ¨£æœ¬ï¼šq å’ŒéšŠåˆ—ä¸­çš„æ‰€æœ‰æ¨£æœ¬
            neg_sims = q.dot(self.queue.T)

            # 4. å°æ¯”æå¤±
            logits = np.concatenate([[pos_sim], neg_sims])
            labels = 0  # ç¬¬ 0 å€‹æ˜¯æ­£æ¨£æœ¬
            loss = cross_entropy(logits, labels)

            losses.append(loss)

            # 5. æ›´æ–°éšŠåˆ—
            self.enqueue_dequeue([k])

        # 6. å‹•é‡æ›´æ–° Key ç·¨ç¢¼å™¨
        self.momentum_update()

        return np.mean(losses)
```

---

## ğŸ¯ è‡ªç›£ç£å­¸ç¿’åœ¨ NLP

### 1. Word2Vec

**ä»»å‹™**ï¼šæ ¹æ“šä¸Šä¸‹æ–‡é æ¸¬è©

```
å¥å­ï¼šã€Œæˆ‘æ„›åƒ___å’Œé¦™è•‰ã€

ä»»å‹™ï¼š
å¡«ç©ºï¼Œç­”æ¡ˆå¯èƒ½æ˜¯ã€Œè˜‹æœã€

å­¸åˆ°ä»€éº¼ï¼Ÿ
ã€Œè˜‹æœã€å’Œã€Œé¦™è•‰ã€åœ¨èªç¾©ä¸Šç›¸è¿‘
ï¼ˆå› ç‚ºå®ƒå€‘å‡ºç¾åœ¨ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ï¼‰
```

### 2. GPTï¼ˆè‡ªè¿´æ­¸èªè¨€æ¨¡å‹ï¼‰

**ä»»å‹™**ï¼šé æ¸¬ä¸‹ä¸€å€‹è©

```
è¼¸å…¥ï¼šã€Œä»Šå¤©å¤©æ°£ã€
ä»»å‹™ï¼šé æ¸¬ä¸‹ä¸€å€‹è©
å¯èƒ½ç­”æ¡ˆï¼šã€Œå¾ˆå¥½ã€ã€Œä¸éŒ¯ã€ã€Œç³Ÿç³•ã€

æ¨™ç±¤ï¼š
ä¸‹ä¸€å€‹è©ï¼ˆè‡ªå‹•çŸ¥é“ï¼‰
```

### 3. T5ï¼ˆText-to-Textï¼‰

**ä»»å‹™**ï¼šå„ç¨®æ–‡æœ¬è½‰æ›

```
å¡«ç©ºï¼š
è¼¸å…¥ï¼šã€Œæˆ‘æ„›åƒ<X>ã€
è¼¸å‡ºï¼šã€Œè˜‹æœã€

ç¿»è­¯ï¼š
è¼¸å…¥ï¼šã€Œtranslate English to Chinese: I love youã€
è¼¸å‡ºï¼šã€Œæˆ‘æ„›ä½ ã€

æ‘˜è¦ï¼š
è¼¸å…¥ï¼šã€Œsummarize: [é•·æ–‡ç« ]ã€
è¼¸å‡ºï¼šã€Œ[æ‘˜è¦]ã€

â†’ çµ±ä¸€æˆã€Œæ–‡æœ¬åˆ°æ–‡æœ¬ã€ä»»å‹™
â†’ å¯ä»¥ç”¨åŒä¸€å€‹æ¨¡å‹
```

---

## ğŸš€ è‡ªç›£ç£å­¸ç¿’åœ¨ CV

### ä»»å‹™é¡å‹

```
1. Pretext Tasksï¼ˆå‰ç½®ä»»å‹™ï¼‰
   - æ—‹è½‰é æ¸¬
   - æ‹¼åœ–é‚„åŸ
   - é¡è‰²åŒ–

2. Contrastive Learningï¼ˆå°æ¯”å­¸ç¿’ï¼‰
   - SimCLR
   - MoCo
   - BYOL

3. Masked Image Modelingï¼ˆé®è”½åœ–åƒå»ºæ¨¡ï¼‰
   - MAEï¼ˆMasked Autoencoderï¼‰
   - BEiTï¼ˆé¡ä¼¼ BERTï¼‰

4. Self-Distillationï¼ˆè‡ªè’¸é¤¾ï¼‰
   - DINO
   - EsViT
```

---

## ğŸ“Š è‡ªç›£ç£ vs ç›£ç£ vs ç„¡ç›£ç£

| ç‰¹æ€§ | ç›£ç£å­¸ç¿’ | ç„¡ç›£ç£å­¸ç¿’ | è‡ªç›£ç£å­¸ç¿’ |
|------|---------|-----------|----------|
| **éœ€è¦æ¨™è¨»** | âœ… æ˜¯ï¼ˆå¤§é‡ï¼‰ | âŒ å¦ | âŒ å¦ |
| **å­¸ç¿’ç›®æ¨™** | æ˜ç¢ºï¼ˆæ¨™ç±¤ï¼‰ | æ¨¡ç³Šï¼ˆèšé¡ï¼‰ | æ˜ç¢ºï¼ˆè‡ªå‹•ä»»å‹™ï¼‰ |
| **æ•¸æ“šéœ€æ±‚** | å°‘ï¼ˆ1è¬ï¼‰ | å¤§ï¼ˆ10è¬ï¼‰ | å¤§ï¼ˆ100è¬ï¼‰ |
| **æ•ˆæœ** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **æˆæœ¬** | é«˜ï¼ˆæ¨™è¨»ï¼‰ | ä½ | ä½ |
| **æ³›åŒ–èƒ½åŠ›** | ä¸­ | ä¸­ | é«˜ |

---

## ğŸ“ å¯¦å‹™å»ºè­°

### 1. æ•¸æ“šå¢å¼·ç­–ç•¥

```python
# åœ–åƒå¢å¼·
augmentations = [
    RandomCrop(size=224),
    ColorJitter(brightness=0.4, contrast=0.4),
    RandomHorizontalFlip(p=0.5),
    RandomRotation(degrees=15),
    GaussianBlur(kernel_size=3),
]

# æ–‡æœ¬å¢å¼·
def augment_text(sentence):
    """æ–‡æœ¬å¢å¼·ç­–ç•¥"""

    # 1. å›è­¯ï¼ˆBack Translationï¼‰
    # ä¸­æ–‡ â†’ è‹±æ–‡ â†’ ä¸­æ–‡
    en = translate_to_english(sentence)
    augmented = translate_to_chinese(en)

    # 2. åŒç¾©è©æ›¿æ›
    # ã€Œå¥½ã€â†’ã€Œæ£’ã€ã€Œå„ªç§€ã€
    augmented = replace_with_synonym(sentence)

    # 3. éš¨æ©Ÿæ’å…¥
    augmented = random_insert_word(sentence)

    return augmented
```

### 2. æº«åº¦åƒæ•¸èª¿å„ª

```python
# æº«åº¦åƒæ•¸ï¼ˆtemperatureï¼‰çš„å½±éŸ¿

temperature = 0.1  # ä½æº«
â†’ ç›¸ä¼¼åº¦åˆ†æ•¸ã€Œé›†ä¸­ã€
â†’ å°æ¯”æ›´æ˜é¡¯
â†’ è¨“ç·´æ›´é›£ï¼ˆå®¹æ˜“éæ“¬åˆï¼‰

temperature = 1.0  # é«˜æº«
â†’ ç›¸ä¼¼åº¦åˆ†æ•¸ã€Œåˆ†æ•£ã€
â†’ å°æ¯”è¼ƒæŸ”å’Œ
â†’ è¨“ç·´æ›´ç©©å®š

æ¨è–¦ï¼štemperature = 0.5ï¼ˆä¸­ç­‰ï¼‰
```

### 3. è² æ¨£æœ¬é¸æ“‡

```python
# ç­–ç•¥ 1ï¼šéš¨æ©Ÿè² æ¨£æœ¬
# å¾ batch ä¸­éš¨æ©Ÿé¸æ“‡
negative_samples = random.sample(batch, k=256)

# ç­–ç•¥ 2ï¼šå›°é›£è² æ¨£æœ¬ï¼ˆHard Negativesï¼‰
# é¸æ“‡ã€Œæœ€åƒã€ä½†ã€Œä¸æ˜¯ã€çš„æ¨£æœ¬
similarities = compute_similarities(query, all_samples)
hard_negatives = top_k_similar(similarities, k=64)

# ç­–ç•¥ 3ï¼šæ··åˆç­–ç•¥
negatives = random_negatives + hard_negatives
```

---

## ğŸ”— ç¸½çµ

### è‡ªç›£ç£å­¸ç¿’æ ¸å¿ƒæ€æƒ³

1. **è‡ªå‹•ç”Ÿæˆæ¨™ç±¤**ï¼šä¸éœ€è¦äººå·¥æ¨™è¨»
2. **Pretext Task**ï¼šè¨­è¨ˆå‰ç½®ä»»å‹™
3. **é·ç§»å­¸ç¿’**ï¼šé è¨“ç·´ + å¾®èª¿

### ä¸»è¦å„ªå‹¢

- âœ… ä¸éœ€è¦æ¨™è¨»æ•¸æ“šï¼ˆçœæˆæœ¬ï¼‰
- âœ… å¯ä½¿ç”¨æµ·é‡æ•¸æ“š
- âœ… å­¸åˆ°é€šç”¨ç‰¹å¾µï¼ˆæ³›åŒ–å¥½ï¼‰
- âœ… å°‘é‡æ¨™è¨»å³å¯å¾®èª¿

### ä¸»è¦æŒ‘æˆ°

- âš ï¸ éœ€è¦å¤§ç®—åŠ›ï¼ˆé è¨“ç·´æ…¢ï¼‰
- âš ï¸ å‰ç½®ä»»å‹™è¨­è¨ˆéœ€æŠ€å·§
- âš ï¸ ä¸æ˜¯æ‰€æœ‰é ˜åŸŸéƒ½é©ç”¨

### ä¸»è¦æ‡‰ç”¨

- **CV**ï¼šåœ–åƒåˆ†é¡ã€ç›®æ¨™æª¢æ¸¬
- **NLP**ï¼šBERTã€GPT ç³»åˆ—
- **å¤šæ¨¡æ…‹**ï¼šCLIPï¼ˆæ–‡æœ¬-åœ–åƒï¼‰
- **æ¨è–¦ç³»çµ±**ï¼šç”¨æˆ¶è¡Œç‚ºå»ºæ¨¡

### æœªä¾†æ–¹å‘

- å¤šæ¨¡æ…‹è‡ªç›£ç£å­¸ç¿’
- å°æ¨£æœ¬è‡ªç›£ç£å­¸ç¿’
- åœ¨ç·šè‡ªç›£ç£å­¸ç¿’
- å¯è§£é‡‹è‡ªç›£ç£å­¸ç¿’

---

*æœ€å¾Œæ›´æ–°: 2025-11-26*
