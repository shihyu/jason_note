# é€²éšå„ªåŒ–å™¨å®Œæ•´æŒ‡å— - ç”¨ç”Ÿæ´»æ¯”å–»ç†è§£

## ğŸ¯ å„ªåŒ–å™¨çš„æœ¬è³ªï¼šå¦‚ä½•æ›´è°æ˜åœ°ä¸‹å±±ï¼Ÿ

å›åˆ°æˆ‘å€‘çš„ã€Œè’™çœ¼ä¸‹å±±ã€æ¯”å–»ï¼š

**åŸºæœ¬ SGD**ï¼š
```
æ„Ÿå—å¡åº¦ â†’ å¾€ä¸‹èµ°ä¸€æ­¥ â†’ é‡è¤‡
```

**å•é¡Œ**ï¼š
- âŒ é‡åˆ°å¹³ç·©å€åŸŸï¼šèµ°å¤ªæ…¢
- âŒ é‡åˆ°é™¡å³­å€åŸŸï¼šå¯èƒ½è¡éé ­
- âŒ é‡åˆ°å±±è°·ï¼ˆå…©å´é™¡å³­ï¼Œä¸­é–“å¹³ç·©ï¼‰ï¼šå·¦å³éœ‡ç›ª

**é€²éšå„ªåŒ–å™¨çš„æ”¹é€²**ï¼š
```
âœ… è¨˜ä½ä¾†æ™‚çš„è·¯ï¼ˆMomentumï¼‰
âœ… æ ¹æ“šåœ°å½¢èª¿æ•´æ­¥ä¼ï¼ˆAdaGradã€RMSpropï¼‰
âœ… å…©è€…çµåˆï¼ˆAdamï¼‰
âœ… æ›´è°æ˜çš„ç­–ç•¥ï¼ˆAdamWã€Lookaheadï¼‰
```

---

## ğŸš— æ¯”å–» 1ï¼šé–‹è»Šå°èˆªï¼ˆMomentumï¼‰

### åŸºæœ¬ SGD = æ–°æ‰‹å¸æ©Ÿ

```
æ¯æ¬¡éƒ½é‡æ–°åˆ¤æ–·æ–¹å‘ï¼š
ç¬¬ 1 ç§’ï¼šå¾€å·¦ 5 åº¦
ç¬¬ 2 ç§’ï¼šå¾€å³ 3 åº¦
ç¬¬ 3 ç§’ï¼šå¾€å·¦ 7 åº¦
...
çµæœï¼šæ–¹å‘ç›¤æŠ–å‹•ï¼Œè»Šå­æ–æ™ƒ ğŸš—ğŸ’¨
```

### Momentum = ç†Ÿç·´å¸æ©Ÿ

```
è¨˜ä½ã€Œæ…£æ€§ã€ï¼š
ç¬¬ 1 ç§’ï¼šå¾€å·¦ 5 åº¦
ç¬¬ 2 ç§’ï¼šç¹¼çºŒå¾€å·¦ï¼ˆåŠ ä¸Šæ–°åˆ¤æ–·çš„å³ 3 åº¦ï¼‰= å¾€å·¦ 2 åº¦
ç¬¬ 3 ç§’ï¼šç¹¼çºŒé€™å€‹æ–¹å‘...
çµæœï¼šè¡Œé§›å¹³é †ï¼Œå°‘éœ‡ç›ª ğŸš—â†’
```

### æ•¸å­¸è¡¨é”

**åŸºæœ¬ SGD**ï¼š
```
v_t = -Î± Ã— gradient
Î¸_t = Î¸_{t-1} + v_t
```

**Momentum**ï¼š
```
v_t = Î² Ã— v_{t-1} - Î± Ã— gradient
      â†‘ ä¿ç•™ 90% çš„ã€Œæ…£æ€§ã€

Î¸_t = Î¸_{t-1} + v_t
```

### ç”Ÿæ´»åŒ–è§£é‡‹

æƒ³åƒä½ æ¨ä¸€å€‹è³¼ç‰©è»Šï¼š

```
æ²’æœ‰ Momentumï¼š
æ¯æ¬¡æ¨éƒ½ã€Œå¾é›¶é–‹å§‹ã€
æ¨ä¸€ä¸‹ â†’ åœ â†’ æ¨ä¸€ä¸‹ â†’ åœ
â†’ å¾ˆç´¯ï¼Œé€²å±•æ…¢ ğŸ˜“

æœ‰ Momentumï¼š
æ¨ä¸€ä¸‹ â†’ è»Šå­ç¹¼çºŒæ»¾å‹•ï¼ˆæ…£æ€§ï¼‰
ä½ åªéœ€è¦ã€Œå¾®èª¿æ–¹å‘ã€
â†’ çœåŠ›ï¼Œé€²å±•å¿« ğŸ˜Š
```

### Python å¯¦ä½œ

```python
class MomentumOptimizer:
    def __init__(self, learning_rate=0.01, momentum=0.9):
        """
        Momentum å„ªåŒ–å™¨

        æ¯”å–»ï¼šè¨˜ä½ã€Œæ…£æ€§ã€çš„é–‹è»Šæ–¹å¼

        åƒæ•¸ï¼š
            learning_rate: æ¯æ¬¡èª¿æ•´çš„å¹…åº¦
            momentum: ä¿ç•™å¤šå°‘æ…£æ€§ï¼ˆé€šå¸¸ 0.9ï¼‰
        """
        self.lr = learning_rate
        self.momentum = momentum
        self.velocity = {}  # å­˜å„²æ¯å€‹åƒæ•¸çš„ã€Œé€Ÿåº¦ã€

    def update(self, param_name, param, gradient):
        """æ›´æ–°åƒæ•¸"""
        # åˆå§‹åŒ–é€Ÿåº¦
        if param_name not in self.velocity:
            self.velocity[param_name] = np.zeros_like(param)

        # æ›´æ–°é€Ÿåº¦ï¼ˆä¿ç•™æ…£æ€§ + æ–°æ¢¯åº¦ï¼‰
        self.velocity[param_name] = (
            self.momentum * self.velocity[param_name] -
            self.lr * gradient
        )

        # æ›´æ–°åƒæ•¸
        param += self.velocity[param_name]

        return param

# ä½¿ç”¨ç¯„ä¾‹
optimizer = MomentumOptimizer(learning_rate=0.01, momentum=0.9)

for epoch in range(100):
    # è¨ˆç®—æ¢¯åº¦
    gradient = compute_gradient()

    # æ›´æ–°æ¬Šé‡
    weights = optimizer.update('weights', weights, gradient)
```

### è¦–è¦ºåŒ–æ¯”è¼ƒ

```python
import numpy as np
import matplotlib.pyplot as plt

def visualize_momentum():
    """è¦–è¦ºåŒ– SGD vs Momentum"""

    # å®šç¾©ä¸€å€‹ã€Œå±±è°·ã€å‡½æ•¸
    def loss_function(x, y):
        return x**2 / 20 + y**2  # æ©«å‘å¹³ç·©ï¼Œç¸±å‘é™¡å³­

    # SGD è»Œè·¡
    x_sgd, y_sgd = 10, 10
    trajectory_sgd = [(x_sgd, y_sgd)]

    for _ in range(100):
        grad_x = x_sgd / 10
        grad_y = 2 * y_sgd
        x_sgd -= 0.1 * grad_x
        y_sgd -= 0.1 * grad_y
        trajectory_sgd.append((x_sgd, y_sgd))

    # Momentum è»Œè·¡
    x_mom, y_mom = 10, 10
    vx, vy = 0, 0
    trajectory_mom = [(x_mom, y_mom)]

    for _ in range(100):
        grad_x = x_mom / 10
        grad_y = 2 * y_mom

        # æ›´æ–°é€Ÿåº¦ï¼ˆMomentumï¼‰
        vx = 0.9 * vx - 0.1 * grad_x
        vy = 0.9 * vy - 0.1 * grad_y

        x_mom += vx
        y_mom += vy
        trajectory_mom.append((x_mom, y_mom))

    # ç¹ªåœ–
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # ç¹ªè£½æå¤±å‡½æ•¸ç­‰é«˜ç·š
    x = np.linspace(-12, 12, 100)
    y = np.linspace(-12, 12, 100)
    X, Y = np.meshgrid(x, y)
    Z = loss_function(X, Y)

    for ax in axes:
        ax.contour(X, Y, Z, levels=20, alpha=0.3)
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.grid(True)

    # SGD è»Œè·¡
    traj = np.array(trajectory_sgd)
    axes[0].plot(traj[:, 0], traj[:, 1], 'ro-', linewidth=2, markersize=3)
    axes[0].set_title('SGDï¼šå·¦å³éœ‡ç›ªï¼Œé€²å±•æ…¢')

    # Momentum è»Œè·¡
    traj = np.array(trajectory_mom)
    axes[1].plot(traj[:, 0], traj[:, 1], 'bo-', linewidth=2, markersize=3)
    axes[1].set_title('Momentumï¼šå¹³ç©©å¿«é€Ÿ')

    plt.tight_layout()
    plt.savefig('momentum_comparison.png', dpi=150)
    plt.show()

visualize_momentum()
```

---

## ğŸƒ æ¯”å–» 2ï¼šè·‘æ­¥é…é€Ÿï¼ˆAdaGradï¼‰

### å•é¡Œï¼šä¸€åˆ€åˆ‡çš„å­¸ç¿’ç‡

```
æƒ…å¢ƒï¼šä½ åœ¨è·‘é¦¬æ‹‰æ¾

å›ºå®šå­¸ç¿’ç‡ = å›ºå®šé…é€Ÿï¼š
- å‰ 10 å…¬é‡Œï¼šé«”åŠ›å……æ²›ï¼Œé…é€Ÿå¤ªæ…¢ï¼ˆæµªè²»ï¼‰
- ä¸­é–“ 20 å…¬é‡Œï¼šå‰›å‰›å¥½
- æœ€å¾Œ 12 å…¬é‡Œï¼šç´¯äº†ï¼Œé…é€Ÿå¤ªå¿«ï¼ˆå—å‚·ï¼‰
```

### AdaGradï¼šè‡ªé©æ‡‰å­¸ç¿’ç‡

```
è°æ˜é…é€Ÿï¼š
- å‰ 10 å…¬é‡Œï¼šåŠ é€Ÿï¼ˆæ¢ç´¢éšæ®µï¼Œå¯ä»¥å¤§æ­¥èµ°ï¼‰
- ä¸­é–“ 20 å…¬é‡Œï¼šç©©å®š
- æœ€å¾Œ 12 å…¬é‡Œï¼šæ¸›é€Ÿï¼ˆæ¥è¿‘çµ‚é»ï¼Œå°æ­¥èª¿æ•´ï¼‰
```

### æ ¸å¿ƒæ€æƒ³

**æ ¹æ“šã€Œæ­·å²æ¢¯åº¦ã€èª¿æ•´å­¸ç¿’ç‡**ï¼š
- æ¢¯åº¦å¤§çš„åƒæ•¸ â†’ å­¸ç¿’ç‡è®Šå°ï¼ˆèµ°å¤ªå¤šæ¬¡äº†ï¼Œè©²æ¸›é€Ÿï¼‰
- æ¢¯åº¦å°çš„åƒæ•¸ â†’ å­¸ç¿’ç‡ä¿æŒå¤§ï¼ˆèµ°å¤ªå°‘ï¼Œè©²åŠ é€Ÿï¼‰

### æ•¸å­¸è¡¨é”

```
G_t = G_{t-1} + (gradient)Â²
      â†‘ ç´¯ç©æ¢¯åº¦å¹³æ–¹ï¼ˆè¨˜éŒ„ã€Œèµ°äº†å¤šé ã€ï¼‰

Î¸_t = Î¸_{t-1} - Î± / âˆš(G_t + Îµ) Ã— gradient
                  â†‘ è‡ªé©æ‡‰å­¸ç¿’ç‡
```

### ç”Ÿæ´»åŒ–ä¾‹å­ï¼šå­¸è‹±æ–‡å–®å­—

```
æƒ…å¢ƒï¼šä½ è¦è¨˜ 100 å€‹è‹±æ–‡å–®å­—

å–®å­— Aï¼šçœ‹äº† 1 æ¬¡å°±è¨˜ä½ â†’ G_A = å°
å–®å­— Bï¼šçœ‹äº† 20 æ¬¡é‚„è¨˜ä¸ä½ â†’ G_B = å¤§

AdaGrad ç­–ç•¥ï¼š
- å–®å­— Aï¼šå°‘è¤‡ç¿’ï¼ˆå­¸ç¿’ç‡ä¿æŒå¤§ï¼‰
- å–®å­— Bï¼šå¤šè¤‡ç¿’ï¼ˆå­¸ç¿’ç‡è®Šå°ï¼Œæ…¢æ…¢è¨˜ï¼‰
```

### Python å¯¦ä½œ

```python
class AdaGradOptimizer:
    def __init__(self, learning_rate=0.01, epsilon=1e-8):
        """
        AdaGrad å„ªåŒ–å™¨

        æ¯”å–»ï¼šæ ¹æ“šã€Œç´¯è¨ˆè·‘çš„è·é›¢ã€èª¿æ•´é…é€Ÿ

        åƒæ•¸ï¼š
            learning_rate: åˆå§‹å­¸ç¿’ç‡
            epsilon: é˜²æ­¢é™¤é›¶ï¼ˆéå¸¸å°çš„æ•¸ï¼‰
        """
        self.lr = learning_rate
        self.epsilon = epsilon
        self.G = {}  # ç´¯ç©æ¢¯åº¦å¹³æ–¹

    def update(self, param_name, param, gradient):
        """æ›´æ–°åƒæ•¸"""
        # åˆå§‹åŒ–
        if param_name not in self.G:
            self.G[param_name] = np.zeros_like(param)

        # ç´¯ç©æ¢¯åº¦å¹³æ–¹
        self.G[param_name] += gradient ** 2

        # è‡ªé©æ‡‰å­¸ç¿’ç‡æ›´æ–°
        adapted_lr = self.lr / (np.sqrt(self.G[param_name]) + self.epsilon)
        param -= adapted_lr * gradient

        return param

# ç¯„ä¾‹
optimizer = AdaGradOptimizer(learning_rate=0.1)

for epoch in range(100):
    gradient = compute_gradient()
    weights = optimizer.update('weights', weights, gradient)

    # è§€å¯Ÿå­¸ç¿’ç‡è®ŠåŒ–
    current_lr = 0.1 / (np.sqrt(optimizer.G['weights']) + 1e-8)
    print(f"Epoch {epoch}: å­¸ç¿’ç‡ = {np.mean(current_lr):.6f}")
```

### ç¼ºé»ï¼šå­¸ç¿’ç‡å–®èª¿éæ¸›

```
å•é¡Œï¼šå¾ŒæœŸå­¸ç¿’ç‡å¯èƒ½è®Šå¤ªå°

æ¯”å–»ï¼š
è·‘åˆ° 30 å…¬é‡Œæ™‚ï¼Œé…é€Ÿå·²ç¶“æ…¢åˆ°ã€Œç”¨èµ°çš„ã€
å³ä½¿å‰æ–¹æ˜¯ä¸‹å¡ï¼ˆå¯ä»¥åŠ é€Ÿï¼‰ï¼Œä¹Ÿæä¸èµ·é€Ÿåº¦

è§£æ±ºï¼šRMSpropï¼ˆä¸‹ä¸€ç¯€ï¼‰
```

---

## ğŸ“ˆ æ¯”å–» 3ï¼šå½ˆæ€§é…é€Ÿï¼ˆRMSpropï¼‰

### AdaGrad çš„å•é¡Œ

```
AdaGradï¼šç´¯ç©ã€Œæ‰€æœ‰ã€æ­·å²æ¢¯åº¦
G_t = G_1 + G_2 + G_3 + ... + G_t
â†’ G_t åªæœƒè¶Šä¾†è¶Šå¤§
â†’ å­¸ç¿’ç‡åªæœƒè¶Šä¾†è¶Šå°
â†’ å¾ŒæœŸå¹¾ä¹ä¸å‹•
```

### RMSpropï¼šåªè¨˜ä½ã€Œæœ€è¿‘ã€çš„æ­·å²

```
RMSpropï¼šç”¨ã€ŒæŒ‡æ•¸ç§»å‹•å¹³å‡ã€
S_t = 0.9 Ã— S_{t-1} + 0.1 Ã— (gradient)Â²
      â†‘ ä¿ç•™ 90% èˆŠè¨˜æ†¶ï¼ŒåŠ å…¥ 10% æ–°è³‡è¨Š

â†’ æœ€è¿‘çš„æ¢¯åº¦å½±éŸ¿å¤§ï¼Œä¹…é çš„æ¢¯åº¦è¢«ã€Œéºå¿˜ã€
â†’ å­¸ç¿’ç‡å¯ä»¥ã€Œå›å‡ã€
```

### ç”Ÿæ´»åŒ–æ¯”å–»ï¼šé«”é‡ç®¡ç†

```
AdaGradï¼ˆç´¯ç©æ‰€æœ‰æ­·å²ï¼‰ï¼š
ä½ ä¸€è¼©å­åƒéçš„æ‰€æœ‰é£Ÿç‰©éƒ½ç®—é€²å»
â†’ åƒè¶Šå¤šï¼Œæ¸›è‚¥è¶Šé›£ï¼ˆå³ä½¿æœ€è¿‘åœ¨ç¯€é£Ÿï¼‰

RMSpropï¼ˆåªçœ‹æœ€è¿‘ï¼‰ï¼š
åªçœ‹ã€Œæœ€è¿‘ä¸€å€‹æœˆã€çš„é£²é£Ÿ
â†’ æœ€è¿‘ç¯€é£Ÿ â†’ å¯ä»¥æ¸›è‚¥æˆåŠŸ
â†’ æœ€è¿‘æš´é£Ÿ â†’ éœ€è¦åŠ å¼·é‹å‹•
```

### æ•¸å­¸è¡¨é”

```
S_t = Î² Ã— S_{t-1} + (1-Î²) Ã— (gradient)Â²
      â†‘ Î² é€šå¸¸å– 0.9ï¼ˆä¿ç•™ 90% èˆŠè¨˜æ†¶ï¼‰

Î¸_t = Î¸_{t-1} - Î± / âˆš(S_t + Îµ) Ã— gradient
```

### Python å¯¦ä½œ

```python
class RMSpropOptimizer:
    def __init__(self, learning_rate=0.01, beta=0.9, epsilon=1e-8):
        """
        RMSprop å„ªåŒ–å™¨

        æ¯”å–»ï¼šæ ¹æ“šã€Œæœ€è¿‘çš„è¡¨ç¾ã€èª¿æ•´ç­–ç•¥

        åƒæ•¸ï¼š
            learning_rate: å­¸ç¿’ç‡
            beta: è¡°æ¸›ç‡ï¼ˆä¿ç•™å¤šå°‘èˆŠè¨˜æ†¶ï¼‰
            epsilon: é˜²æ­¢é™¤é›¶
        """
        self.lr = learning_rate
        self.beta = beta
        self.epsilon = epsilon
        self.S = {}  # æ¢¯åº¦å¹³æ–¹çš„ç§»å‹•å¹³å‡

    def update(self, param_name, param, gradient):
        """æ›´æ–°åƒæ•¸"""
        # åˆå§‹åŒ–
        if param_name not in self.S:
            self.S[param_name] = np.zeros_like(param)

        # æ›´æ–°ç§»å‹•å¹³å‡ï¼ˆã€Œéºå¿˜ã€èˆŠæ¢¯åº¦ï¼‰
        self.S[param_name] = (
            self.beta * self.S[param_name] +
            (1 - self.beta) * gradient ** 2
        )

        # è‡ªé©æ‡‰å­¸ç¿’ç‡æ›´æ–°
        adapted_lr = self.lr / (np.sqrt(self.S[param_name]) + self.epsilon)
        param -= adapted_lr * gradient

        return param

# æ¯”è¼ƒ AdaGrad vs RMSprop
def compare_adagrad_rmsprop():
    """è¦–è¦ºåŒ–å…©è€…å·®ç•°"""

    # æ¨¡æ“¬æ¢¯åº¦åºåˆ—ï¼ˆå‰æœŸå¤§ï¼Œä¸­æœŸå°ï¼Œå¾ŒæœŸåˆè®Šå¤§ï¼‰
    gradients = []
    for t in range(100):
        if t < 30:
            grad = 2.0  # å‰æœŸï¼šå¤§æ¢¯åº¦
        elif t < 70:
            grad = 0.1  # ä¸­æœŸï¼šå°æ¢¯åº¦
        else:
            grad = 2.0  # å¾ŒæœŸï¼šå¤§æ¢¯åº¦
        gradients.append(grad)

    # AdaGrad
    G_adagrad = 0
    lr_adagrad = []
    for grad in gradients:
        G_adagrad += grad ** 2
        lr_adagrad.append(0.1 / np.sqrt(G_adagrad + 1e-8))

    # RMSprop
    S_rmsprop = 0
    lr_rmsprop = []
    for grad in gradients:
        S_rmsprop = 0.9 * S_rmsprop + 0.1 * grad ** 2
        lr_rmsprop.append(0.1 / np.sqrt(S_rmsprop + 1e-8))

    # ç¹ªåœ–
    plt.figure(figsize=(12, 6))

    plt.subplot(2, 1, 1)
    plt.plot(gradients, label='æ¢¯åº¦å¤§å°', linewidth=2)
    plt.ylabel('æ¢¯åº¦')
    plt.legend()
    plt.grid(True)
    plt.title('æ¢¯åº¦è®ŠåŒ–')

    plt.subplot(2, 1, 2)
    plt.plot(lr_adagrad, label='AdaGradï¼ˆåªé™ä¸å‡ï¼‰', linewidth=2)
    plt.plot(lr_rmsprop, label='RMSpropï¼ˆå¯å›å‡ï¼‰', linewidth=2)
    plt.xlabel('è¿­ä»£æ¬¡æ•¸')
    plt.ylabel('å­¸ç¿’ç‡')
    plt.legend()
    plt.grid(True)
    plt.title('å­¸ç¿’ç‡è®ŠåŒ–')

    plt.tight_layout()
    plt.savefig('adagrad_vs_rmsprop.png', dpi=150)
    plt.show()

compare_adagrad_rmsprop()
```

---

## ğŸ–ï¸ æ¯”å–» 4ï¼šå®Œç¾é§•é§›ï¼ˆAdamï¼‰

### Adam = Momentum + RMSprop

```
Momentumï¼šè¨˜ä½ã€Œæ–¹å‘ã€ï¼ˆä¸€éšå‹•é‡ï¼‰
RMSpropï¼šèª¿æ•´ã€Œæ­¥ä¼ã€ï¼ˆäºŒéšå‹•é‡ï¼‰

Adamï¼šå…©è€…çµåˆ
â†’ æ—¢å¹³ç©©ï¼Œåˆè‡ªé©æ‡‰
â†’ ç›®å‰æœ€å¸¸ç”¨çš„å„ªåŒ–å™¨ï¼
```

### ç”Ÿæ´»åŒ–æ¯”å–»ï¼šé«˜ç´šè‡ªé§•è»Š

```
åŸºæœ¬ SGDï¼š
äººå·¥é§•é§›ï¼Œä¸€ç›´ä¿®æ­£æ–¹å‘ç›¤ ğŸš—

Momentumï¼š
å®šé€Ÿå·¡èˆªï¼Œä¿æŒç©©å®šé€Ÿåº¦ ğŸš—â†’

RMSpropï¼š
æ ¹æ“šè·¯æ³èª¿æ•´é€Ÿåº¦ï¼ˆä¸Šå¡æ¸›é€Ÿï¼Œä¸‹å¡åŠ é€Ÿï¼‰ğŸš—â†—â†˜

Adamï¼š
è‡ªé§•è»Šï¼ˆå®šé€Ÿå·¡èˆª + è‡ªå‹•èª¿é€Ÿï¼‰ğŸš—ğŸ¤–
â†’ æ—¢ç©©å®šåˆè°æ˜ï¼
```

### æ•¸å­¸è¡¨é”

```
# ä¸€éšå‹•é‡ï¼ˆMomentumï¼‰
m_t = Î²â‚ Ã— m_{t-1} + (1-Î²â‚) Ã— gradient

# äºŒéšå‹•é‡ï¼ˆRMSpropï¼‰
v_t = Î²â‚‚ Ã— v_{t-1} + (1-Î²â‚‚) Ã— gradientÂ²

# åå·®ä¿®æ­£ï¼ˆBias Correctionï¼‰
mÌ‚_t = m_t / (1 - Î²â‚áµ—)
vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)

# æ›´æ–°åƒæ•¸
Î¸_t = Î¸_{t-1} - Î± Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

### ç‚ºä»€éº¼éœ€è¦åå·®ä¿®æ­£ï¼Ÿ

**å•é¡Œ**ï¼šåˆå§‹æ™‚ m_0 = 0, v_0 = 0

```
ç¬¬ 1 æ­¥ï¼š
m_1 = 0.9 Ã— 0 + 0.1 Ã— gradient = 0.1 Ã— gradient
â†’ æ¯”å¯¦éš›æ¢¯åº¦å° 10 å€ï¼

ç¬¬ 2 æ­¥ï¼š
m_2 = 0.9 Ã— (0.1 Ã— grad) + 0.1 Ã— grad
    = 0.19 Ã— gradientï¼ˆç´¯ç©ï¼‰
â†’ é‚„æ˜¯åå°

...ç›´åˆ°å¤šæ¬¡è¿­ä»£å¾Œæ‰æ¥è¿‘çœŸå¯¦å€¼
```

**è§£æ±º**ï¼šåå·®ä¿®æ­£

```
mÌ‚_t = m_t / (1 - Î²â‚áµ—)

ç¬¬ 1 æ­¥ï¼šmÌ‚_1 = 0.1 Ã— grad / (1 - 0.9Â¹) = 0.1 / 0.1 = grad âœ“
ç¬¬ 2 æ­¥ï¼šmÌ‚_2 = 0.19 Ã— grad / (1 - 0.9Â²) = 0.19 / 0.19 = grad âœ“
```

### Python å®Œæ•´å¯¦ä½œ

```python
class AdamOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        """
        Adam å„ªåŒ–å™¨

        æ¯”å–»ï¼šæœ€æ™ºèƒ½çš„è‡ªé§•è»Š

        åƒæ•¸ï¼š
            learning_rate: å­¸ç¿’ç‡ï¼ˆé€šå¸¸ 0.001ï¼‰
            beta1: ä¸€éšå‹•é‡è¡°æ¸›ç‡ï¼ˆé€šå¸¸ 0.9ï¼‰
            beta2: äºŒéšå‹•é‡è¡°æ¸›ç‡ï¼ˆé€šå¸¸ 0.999ï¼‰
            epsilon: é˜²æ­¢é™¤é›¶
        """
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon

        self.m = {}  # ä¸€éšå‹•é‡
        self.v = {}  # äºŒéšå‹•é‡
        self.t = 0   # æ™‚é–“æ­¥

    def update(self, param_name, param, gradient):
        """æ›´æ–°åƒæ•¸"""
        # åˆå§‹åŒ–
        if param_name not in self.m:
            self.m[param_name] = np.zeros_like(param)
            self.v[param_name] = np.zeros_like(param)

        # æ™‚é–“æ­¥ +1
        self.t += 1

        # æ›´æ–°ä¸€éšå‹•é‡ï¼ˆMomentumï¼‰
        self.m[param_name] = (
            self.beta1 * self.m[param_name] +
            (1 - self.beta1) * gradient
        )

        # æ›´æ–°äºŒéšå‹•é‡ï¼ˆRMSpropï¼‰
        self.v[param_name] = (
            self.beta2 * self.v[param_name] +
            (1 - self.beta2) * gradient ** 2
        )

        # åå·®ä¿®æ­£
        m_hat = self.m[param_name] / (1 - self.beta1 ** self.t)
        v_hat = self.v[param_name] / (1 - self.beta2 ** self.t)

        # æ›´æ–°åƒæ•¸
        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)

        return param

# å®Œæ•´è¨“ç·´ç¯„ä¾‹
def train_with_adam():
    """ä½¿ç”¨ Adam è¨“ç·´ç·šæ€§è¿´æ­¸"""

    # ç”Ÿæˆæ•¸æ“š
    np.random.seed(42)
    X = np.linspace(0, 10, 100)
    y = 3 * X + 7 + np.random.randn(100) * 2

    # åˆå§‹åŒ–åƒæ•¸
    w = 0.0
    b = 0.0

    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = AdamOptimizer(learning_rate=0.1)

    # è¨“ç·´
    loss_history = []
    for epoch in range(100):
        # å‰å‘å‚³æ’­
        y_pred = w * X + b
        loss = np.mean((y_pred - y) ** 2)
        loss_history.append(loss)

        # è¨ˆç®—æ¢¯åº¦
        dw = (2/len(X)) * np.sum((y_pred - y) * X)
        db = (2/len(X)) * np.sum(y_pred - y)

        # æ›´æ–°åƒæ•¸
        w = optimizer.update('w', w, dw)
        b = optimizer.update('b', b, db)

        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}")

    print(f"\næœ€çµ‚çµæœ: w = {w:.4f}, b = {b:.4f}")
    print(f"çœŸå¯¦åƒæ•¸: w = 3.0000, b = 7.0000")

    return loss_history

loss_history = train_with_adam()
```

**è¼¸å‡º**ï¼š
```
Epoch 0: Loss = 149.2341, w = 2.8234, b = 0.3124
Epoch 10: Loss = 4.1234, w = 2.9823, b = 6.9123
Epoch 20: Loss = 4.0123, w = 2.9956, b = 7.0045
...
Epoch 90: Loss = 3.9876, w = 2.9991, b = 7.0012

æœ€çµ‚çµæœ: w = 2.9991, b = 7.0012
çœŸå¯¦åƒæ•¸: w = 3.0000, b = 7.0000
```

---

## ğŸš€ æ¯”å–» 5ï¼šæ¸›é‡ç‰ˆ Adamï¼ˆAdamWï¼‰

### å•é¡Œï¼šæ­£å‰‡åŒ–èˆ‡ Adam ä¸å…¼å®¹

**èƒŒæ™¯çŸ¥è­˜**ï¼š
```
æ­£å‰‡åŒ–ï¼ˆWeight Decayï¼‰ï¼šé˜²æ­¢éæ“¬åˆ
åšæ³•ï¼šè®“æ¬Šé‡ã€Œè‡ªå‹•è¡°æ¸›ã€

loss = åŸå§‹ loss + Î» Ã— Î£(weightsÂ²)
                     â†‘ æ‡²ç½°å¤§æ¬Šé‡
```

**å•é¡Œ**ï¼š
```
åœ¨ Adam ä¸­ï¼Œæ­£å‰‡åŒ–è¢«ã€Œè‡ªé©æ‡‰å­¸ç¿’ç‡ã€å½±éŸ¿
â†’ æ•ˆæœä¸å¦‚é æœŸ
```

### AdamWï¼šè§£è€¦æ¬Šé‡è¡°æ¸›

**æ ¸å¿ƒæ€æƒ³**ï¼š
```
ä¸è¦æŠŠæ­£å‰‡åŒ–åŠ åˆ°æ¢¯åº¦è£¡
ç›´æ¥å°æ¬Šé‡åšè¡°æ¸›
```

**æ¯”å–»**ï¼šæ¸›è‚¥ç­–ç•¥

```
Adam + å‚³çµ±æ­£å‰‡åŒ–ï¼š
ã€Œå°‘åƒã€+ ã€Œé‹å‹•ã€æ··åœ¨ä¸€èµ·è¨ˆç®—
â†’ æ•ˆæœæ‰“æŠ˜æ‰£

AdamWï¼š
ã€Œå°‘åƒã€å’Œã€Œé‹å‹•ã€åˆ†é–‹åŸ·è¡Œ
â†’ æ•ˆæœæ›´å¥½
```

### Python å¯¦ä½œ

```python
class AdamWOptimizer:
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999,
                 epsilon=1e-8, weight_decay=0.01):
        """
        AdamW å„ªåŒ–å™¨

        æ¯”å–»ï¼šAdam + ç¨ç«‹çš„æ¬Šé‡è¡°æ¸›

        æ–°åƒæ•¸ï¼š
            weight_decay: æ¬Šé‡è¡°æ¸›ç‡ï¼ˆé¡ä¼¼æ­£å‰‡åŒ–å¼·åº¦ï¼‰
        """
        self.lr = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.weight_decay = weight_decay

        self.m = {}
        self.v = {}
        self.t = 0

    def update(self, param_name, param, gradient):
        """æ›´æ–°åƒæ•¸"""
        # åˆå§‹åŒ–
        if param_name not in self.m:
            self.m[param_name] = np.zeros_like(param)
            self.v[param_name] = np.zeros_like(param)

        self.t += 1

        # æ›´æ–°å‹•é‡ï¼ˆå’Œ Adam ä¸€æ¨£ï¼‰
        self.m[param_name] = (
            self.beta1 * self.m[param_name] +
            (1 - self.beta1) * gradient
        )
        self.v[param_name] = (
            self.beta2 * self.v[param_name] +
            (1 - self.beta2) * gradient ** 2
        )

        # åå·®ä¿®æ­£
        m_hat = self.m[param_name] / (1 - self.beta1 ** self.t)
        v_hat = self.v[param_name] / (1 - self.beta2 ** self.t)

        # Adam æ›´æ–°
        param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)

        # é¡å¤–ï¼šæ¬Šé‡è¡°æ¸›ï¼ˆè§£è€¦çš„é—œéµï¼ï¼‰
        param -= self.lr * self.weight_decay * param

        return param
```

### Adam vs AdamW æ¯”è¼ƒ

```python
def compare_adam_adamw():
    """æ¯”è¼ƒ Adam å’Œ AdamW åœ¨éæ“¬åˆæƒ…æ³ä¸‹çš„è¡¨ç¾"""

    # ç”Ÿæˆæ•¸æ“šï¼ˆæ•…æ„ç”¨å°æ•¸æ“šé›†ï¼Œå®¹æ˜“éæ“¬åˆï¼‰
    np.random.seed(42)
    X = np.random.randn(20, 10)  # 20 å€‹æ¨£æœ¬ï¼Œ10 å€‹ç‰¹å¾µ
    y = np.random.randn(20)

    # åˆå§‹åŒ–æ¬Šé‡ï¼ˆæ•…æ„å¾ˆå¤§ï¼Œéœ€è¦æ­£å‰‡åŒ–ï¼‰
    w_adam = np.random.randn(10) * 5
    w_adamw = w_adam.copy()

    # å‰µå»ºå„ªåŒ–å™¨
    adam = AdamOptimizer(learning_rate=0.01)
    adamw = AdamWOptimizer(learning_rate=0.01, weight_decay=0.01)

    # è¨“ç·´
    weight_norm_adam = []
    weight_norm_adamw = []

    for epoch in range(200):
        # Adam
        y_pred = X.dot(w_adam)
        grad = (2/len(X)) * X.T.dot(y_pred - y)
        w_adam = adam.update('w', w_adam, grad)
        weight_norm_adam.append(np.linalg.norm(w_adam))

        # AdamW
        y_pred = X.dot(w_adamw)
        grad = (2/len(X)) * X.T.dot(y_pred - y)
        w_adamw = adamw.update('w', w_adamw, grad)
        weight_norm_adamw.append(np.linalg.norm(w_adamw))

    # ç¹ªåœ–
    plt.figure(figsize=(10, 6))
    plt.plot(weight_norm_adam, label='Adamï¼ˆæ¬Šé‡è¼ƒå¤§ï¼Œå®¹æ˜“éæ“¬åˆï¼‰', linewidth=2)
    plt.plot(weight_norm_adamw, label='AdamWï¼ˆæ¬Šé‡å—æ§ï¼Œé˜²æ­¢éæ“¬åˆï¼‰', linewidth=2)
    plt.xlabel('è¿­ä»£æ¬¡æ•¸')
    plt.ylabel('æ¬Šé‡ç¯„æ•¸ ||w||')
    plt.title('Adam vs AdamWï¼šæ¬Šé‡å¤§å°æ¯”è¼ƒ')
    plt.legend()
    plt.grid(True)
    plt.savefig('adam_vs_adamw.png', dpi=150)
    plt.show()

compare_adam_adamw()
```

---

## ğŸ”­ æ¯”å–» 6ï¼šå…ˆçœ‹å†è·³ï¼ˆLookaheadï¼‰

### æ ¸å¿ƒæ€æƒ³ï¼šå…©éšæ®µå„ªåŒ–

**æ¯”å–»**ï¼šè·³èºå‰å…ˆè§€å¯Ÿ

```
æ™®é€šå„ªåŒ–å™¨ï¼ˆå¦‚ Adamï¼‰ï¼š
çœ‹ä¸€æ­¥ â†’ è·³ä¸€æ­¥ â†’ çœ‹ä¸€æ­¥ â†’ è·³ä¸€æ­¥
â†’ å¯èƒ½è·³éŒ¯æ–¹å‘

Lookaheadï¼š
çœ‹ 5 æ­¥ â†’ è©•ä¼°çµæœ â†’ è·³ä¸€å¤§æ­¥
â†’ æ›´ç©©å¥ï¼Œä¸å®¹æ˜“è·³éŒ¯
```

### ç”Ÿæ´»åŒ–ä¾‹å­ï¼šè²·æˆ¿é¸å€

```
ç›´æ¥æ±ºç­–ï¼ˆæ™®é€šå„ªåŒ–å™¨ï¼‰ï¼š
çœ‹åˆ°ä¸€é–“æˆ¿ â†’ ç«‹åˆ»è²·
çœ‹åˆ°å¦ä¸€é–“ â†’ åˆè²·
â†’ å¯èƒ½è²·éŒ¯

Lookahead ç­–ç•¥ï¼š
å…ˆç§Ÿæˆ¿ä½ 5 å€‹æœˆï¼ˆå¿«é€Ÿè©¦æ¢ï¼‰
è©•ä¼°å“ªå€‹å€åŸŸæœ€å¥½
å†è²·æˆ¿ï¼ˆæ…¢åƒæ•¸æ›´æ–°ï¼‰
â†’ æ±ºç­–æ›´ç©©
```

### ç®—æ³•æµç¨‹

```
1. ç”¨ã€Œå¿«æ¬Šé‡ã€æ¢ç´¢ k æ­¥ï¼ˆå¦‚ Adam èµ° 5 æ­¥ï¼‰
   w_fast_1, w_fast_2, ..., w_fast_k

2. ç”¨ã€Œæ…¢æ¬Šé‡ã€æ›´æ–°ä¸€æ­¥ï¼ˆæœå¿«æ¬Šé‡çš„æ–¹å‘ï¼‰
   w_slow = w_slow + Î± Ã— (w_fast_k - w_slow)

3. é‡ç½®å¿«æ¬Šé‡ = æ…¢æ¬Šé‡
   w_fast = w_slow

4. é‡è¤‡
```

### Python å¯¦ä½œ

```python
class LookaheadOptimizer:
    def __init__(self, base_optimizer, k=5, alpha=0.5):
        """
        Lookahead å„ªåŒ–å™¨

        æ¯”å–»ï¼šå…ˆå¿«é€Ÿè©¦æ¢ï¼Œå†æ…¢é€Ÿæ±ºç­–

        åƒæ•¸ï¼š
            base_optimizer: åŸºç¤å„ªåŒ–å™¨ï¼ˆå¦‚ Adamï¼‰
            k: å…§éƒ¨å„ªåŒ–å™¨èµ°å¹¾æ­¥
            alpha: æ…¢æ¬Šé‡æ›´æ–°é€Ÿç‡
        """
        self.base_optimizer = base_optimizer
        self.k = k
        self.alpha = alpha

        self.slow_weights = {}  # æ…¢æ¬Šé‡
        self.step_count = 0

    def update(self, param_name, param, gradient):
        """æ›´æ–°åƒæ•¸"""
        # åˆå§‹åŒ–æ…¢æ¬Šé‡
        if param_name not in self.slow_weights:
            self.slow_weights[param_name] = param.copy()

        # ç”¨åŸºç¤å„ªåŒ–å™¨æ›´æ–°å¿«æ¬Šé‡
        param = self.base_optimizer.update(param_name, param, gradient)

        # è¨ˆæ•¸
        self.step_count += 1

        # æ¯ k æ­¥ï¼Œæ›´æ–°æ…¢æ¬Šé‡
        if self.step_count % self.k == 0:
            # æ…¢æ¬Šé‡æœå¿«æ¬Šé‡æ–¹å‘ç§»å‹•
            self.slow_weights[param_name] += (
                self.alpha * (param - self.slow_weights[param_name])
            )

            # é‡ç½®å¿«æ¬Šé‡ = æ…¢æ¬Šé‡
            param = self.slow_weights[param_name].copy()

        return param

# ä½¿ç”¨ç¯„ä¾‹
base_adam = AdamOptimizer(learning_rate=0.001)
lookahead = LookaheadOptimizer(base_adam, k=5, alpha=0.5)

for epoch in range(100):
    gradient = compute_gradient()
    weights = lookahead.update('weights', weights, gradient)
```

---

## ğŸ“Š å„ªåŒ–å™¨å¤§æ¯”æ‹¼

### æ¸¬è©¦å ´æ™¯ï¼šè¤‡é›œåœ°å½¢

```python
def compare_all_optimizers():
    """åœ¨è¤‡é›œæå¤±å‡½æ•¸ä¸Šæ¯”è¼ƒæ‰€æœ‰å„ªåŒ–å™¨"""

    # å®šç¾©ä¸€å€‹è¤‡é›œçš„æå¤±å‡½æ•¸ï¼ˆæœ‰å¤šå€‹å±€éƒ¨æœ€å°å€¼ï¼‰
    def rastrigin(x, y):
        """Rastrigin å‡½æ•¸ï¼šå¾ˆå¤šå±€éƒ¨æœ€å°å€¼"""
        A = 10
        return (A * 2 + (x**2 - A * np.cos(2 * np.pi * x)) +
                        (y**2 - A * np.cos(2 * np.pi * y)))

    # åˆå§‹é»
    start_x, start_y = 4.5, 4.5

    # å‰µå»ºæ‰€æœ‰å„ªåŒ–å™¨
    optimizers = {
        'SGD': SGDOptimizer(learning_rate=0.01),
        'Momentum': MomentumOptimizer(learning_rate=0.01, momentum=0.9),
        'AdaGrad': AdaGradOptimizer(learning_rate=0.5),
        'RMSprop': RMSpropOptimizer(learning_rate=0.01),
        'Adam': AdamOptimizer(learning_rate=0.1),
        'AdamW': AdamWOptimizer(learning_rate=0.1, weight_decay=0.01),
    }

    # è¨“ç·´æ¯å€‹å„ªåŒ–å™¨
    trajectories = {}

    for name, optimizer in optimizers.items():
        x, y = start_x, start_y
        trajectory = [(x, y)]

        for _ in range(100):
            # è¨ˆç®—æ¢¯åº¦
            grad_x = 2 * x + 2 * np.pi * A * np.sin(2 * np.pi * x)
            grad_y = 2 * y + 2 * np.pi * A * np.sin(2 * np.pi * y)

            # æ›´æ–°
            x = optimizer.update(f'{name}_x', x, grad_x)
            y = optimizer.update(f'{name}_y', y, grad_y)

            trajectory.append((x, y))

        trajectories[name] = np.array(trajectory)

    # ç¹ªåœ–
    fig = plt.figure(figsize=(18, 12))

    # ç¹ªè£½æå¤±å‡½æ•¸ç­‰é«˜ç·š
    x = np.linspace(-5, 5, 200)
    y = np.linspace(-5, 5, 200)
    X, Y = np.meshgrid(x, y)
    Z = rastrigin(X, Y)

    # ç‚ºæ¯å€‹å„ªåŒ–å™¨ç¹ªè£½å­åœ–
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown']

    for idx, (name, traj) in enumerate(trajectories.items()):
        ax = fig.add_subplot(2, 3, idx + 1)
        ax.contour(X, Y, Z, levels=20, alpha=0.3)
        ax.plot(traj[:, 0], traj[:, 1],
                color=colors[idx], linewidth=2, marker='o', markersize=2)
        ax.plot(0, 0, 'r*', markersize=20, label='å…¨å±€æœ€å°å€¼')
        ax.set_title(f'{name}', fontsize=14, fontweight='bold')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.legend()
        ax.grid(True)

    plt.tight_layout()
    plt.savefig('optimizer_comparison_complex.png', dpi=150)
    plt.show()

compare_all_optimizers()
```

### çµæœåˆ†æè¡¨

| å„ªåŒ–å™¨ | é€Ÿåº¦ | ç©©å®šæ€§ | é©ç”¨å ´æ™¯ | æ¨è–¦æŒ‡æ•¸ |
|--------|------|--------|----------|----------|
| **SGD** | â­â­ | â­â­ | æ•™å­¸ã€ç°¡å–®å•é¡Œ | â­â­ |
| **Momentum** | â­â­â­ | â­â­â­ | éœ€è¦åŠ é€Ÿæ”¶æ–‚ | â­â­â­ |
| **AdaGrad** | â­â­ | â­â­â­ | ç¨€ç–æ•¸æ“šï¼ˆNLPï¼‰ | â­â­ |
| **RMSprop** | â­â­â­â­ | â­â­â­â­ | RNNã€æ™‚é–“åºåˆ— | â­â­â­â­ |
| **Adam** | â­â­â­â­â­ | â­â­â­â­ | **é€šç”¨é¦–é¸** | â­â­â­â­â­ |
| **AdamW** | â­â­â­â­â­ | â­â­â­â­â­ | **é˜²éæ“¬åˆ** | â­â­â­â­â­ |
| **Lookahead** | â­â­â­â­ | â­â­â­â­â­ | éœ€è¦ç©©å¥æ€§ | â­â­â­â­ |

---

## ğŸ“ å¯¦æˆ°å»ºè­°

### 1. å¦‚ä½•é¸æ“‡å„ªåŒ–å™¨ï¼Ÿ

```
æ±ºç­–æ¨¹ï¼š

é–‹å§‹
  â”‚
  â”œâ”€ æ˜¯æ•™å­¸/èª¿è©¦ï¼Ÿ
  â”‚   â””â”€ Yes â†’ SGD
  â”‚
  â”œâ”€ æ˜¯ NLP ä»»å‹™ï¼Ÿ
  â”‚   â””â”€ Yes â†’ AdaGrad æˆ– Adam
  â”‚
  â”œâ”€ æ˜¯ RNN/LSTMï¼Ÿ
  â”‚   â””â”€ Yes â†’ RMSprop
  â”‚
  â”œâ”€ æ˜¯ Transformerï¼Ÿ
  â”‚   â””â”€ Yes â†’ AdamW
  â”‚
  â””â”€ å…¶ä»–ï¼ˆCNNã€ä¸€èˆ¬æ·±åº¦å­¸ç¿’ï¼‰
      â””â”€ Adamï¼ˆé€šç”¨é¦–é¸ï¼‰
```

### 2. è¶…åƒæ•¸è¨­å®šå»ºè­°

```python
# æ¨è–¦èµ·é»
configs = {
    'SGD': {
        'learning_rate': 0.01,
    },
    'Momentum': {
        'learning_rate': 0.01,
        'momentum': 0.9,
    },
    'Adam': {
        'learning_rate': 0.001,
        'beta1': 0.9,
        'beta2': 0.999,
    },
    'AdamW': {
        'learning_rate': 0.001,
        'beta1': 0.9,
        'beta2': 0.999,
        'weight_decay': 0.01,
    },
}
```

### 3. å¸¸è¦‹éŒ¯èª¤èˆ‡è§£æ±º

| å•é¡Œ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|------|---------|---------|
| Loss ä¸ä¸‹é™ | å­¸ç¿’ç‡å¤ªå¤§/å¤ªå° | å˜—è©¦ 0.1, 0.01, 0.001 |
| è¨“ç·´ä¸ç©©å®š | æ²’ç”¨ Momentum | æ”¹ç”¨ Adam |
| éæ“¬åˆ | æ²’æœ‰æ­£å‰‡åŒ– | ç”¨ AdamW + Dropout |
| å¾ŒæœŸé€²å±•æ…¢ | AdaGrad å•é¡Œ | æ”¹ç”¨ Adam æˆ– RMSprop |

---

## ğŸ“š ç¸½çµ

### å„ªåŒ–å™¨é€²åŒ–å²

```
1986: SGDï¼ˆåŸºç¤ï¼‰
   â†“
1999: Momentumï¼ˆåŠ é€Ÿï¼‰
   â†“
2011: AdaGradï¼ˆè‡ªé©æ‡‰ï¼‰
   â†“
2012: RMSpropï¼ˆæ”¹é€² AdaGradï¼‰
   â†“
2014: Adamï¼ˆçµåˆ Momentum + RMSpropï¼‰â˜… é‡Œç¨‹ç¢‘
   â†“
2017: AdamWï¼ˆè§£è€¦æ¬Šé‡è¡°æ¸›ï¼‰
   â†“
2019: Lookaheadï¼ˆå…©éšæ®µå„ªåŒ–ï¼‰
   â†“
2023: æŒçºŒæ¼”åŒ–ä¸­...
```

### æ ¸å¿ƒæ€æƒ³ç¸½çµ

1. **Momentum**ï¼šè¨˜ä½æ–¹å‘ï¼Œæ¸›å°‘éœ‡ç›ª
2. **AdaGrad/RMSprop**ï¼šè‡ªé©æ‡‰å­¸ç¿’ç‡
3. **Adam**ï¼šçµåˆå…©è€…å„ªé»
4. **AdamW**ï¼šæ›´å¥½çš„æ­£å‰‡åŒ–
5. **Lookahead**ï¼šæ›´ç©©å¥çš„æ›´æ–°

### æ¨è–¦ä½¿ç”¨

- ğŸ¥‡ **é¦–é¸**ï¼šAdam æˆ– AdamW
- ğŸ¥ˆ **å‚™é¸**ï¼šRMSpropï¼ˆRNNï¼‰
- ğŸ¥‰ **ç‰¹æ®Š**ï¼šSGD + Momentumï¼ˆæŸäº› CV ä»»å‹™ï¼‰

---

*æœ€å¾Œæ›´æ–°: 2025-11-26*
