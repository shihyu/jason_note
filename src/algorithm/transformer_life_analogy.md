# Transformer å®Œæ•´æŒ‡å— - ç”¨ç”Ÿæ´»æ¯”å–»ç†è§£

## ğŸ¯ æ ¸å¿ƒæ¯”å–»ï¼šåœ–æ›¸é¤¨ vs é–±è®€å°èªª

### RNN/LSTM çš„å•é¡Œï¼šå¿…é ˆæŒ‰é †åºè®€

```
RNN/LSTM çœ‹å°èªªï¼š
ç¬¬ 1 é  â†’ ç¬¬ 2 é  â†’ ç¬¬ 3 é  â†’ ... â†’ ç¬¬ 100 é 
        â†“         â†“         â†“
      è¨˜æ†¶1     è¨˜æ†¶2     è¨˜æ†¶3

å•é¡Œï¼š
âŒ å¿…é ˆæŒ‰é †åºè®€ï¼ˆç„¡æ³•è·³é ï¼‰
âŒ è®€å®Œç¬¬ 1 é æ‰èƒ½è®€ç¬¬ 2 é ï¼ˆç„¡æ³•ä¸¦è¡Œï¼‰
âŒ ç¬¬ 1 é çš„è³‡è¨Šå¯èƒ½è¢«éºå¿˜ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰
```

### Transformerï¼šå¯ä»¥åŒæ™‚çœ‹æ‰€æœ‰é é¢

```
Transformer çœ‹å°èªªï¼š
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ç¬¬1é â”‚ç¬¬2é â”‚ç¬¬3é â”‚...  â”‚  åŒæ™‚çœ‹
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
   â†“     â†“     â†“     â†“
   â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜
         â†“
    ã€Œæ³¨æ„åŠ›æ©Ÿåˆ¶ã€
    è‡ªå‹•æ‰¾å‡ºé‡è¦é—œè¯

å„ªé»ï¼š
âœ… ä¸¦è¡Œè™•ç†ï¼ˆGPU å‹å¥½ï¼Œè¶…å¿«ï¼‰
âœ… é•·è·é›¢ä¾è³´ä¸è¡°æ¸›ï¼ˆç¬¬ 1 é å’Œç¬¬ 100 é åŒæ¨£æ¸…æ™°ï¼‰
âœ… è‡ªå‹•æ‰¾å‡ºé—œè¯ï¼ˆä¸éœ€è¦äººå·¥è¨­è¨ˆï¼‰
```

---

## ğŸ“š ç”Ÿæ´»åŒ–æ¡ˆä¾‹ 1ï¼šæœå°‹å¼•æ“

### æ¯”å–»ï¼šGoogle æœå°‹

```
ä½ åœ¨ Google æœå°‹ï¼šã€Œ2024 å¥§é‹ é‡‘ç‰Œã€

å‚³çµ±æ–¹æ³•ï¼ˆRNNï¼‰ï¼š
é€å­—è™•ç† â†’ ã€Œ2024ã€â†’ã€Œå¥§é‹ã€â†’ã€Œé‡‘ç‰Œã€
â†’ æ…¢ï¼Œè€Œä¸”å¯èƒ½å¿˜è¨˜ã€Œ2024ã€

Transformer æ–¹æ³•ï¼š
1. åŒæ™‚çœ‹ä¸‰å€‹è©
2. è‡ªå‹•ç†è§£ã€Œ2024ã€ä¿®é£¾ã€Œå¥§é‹ã€
3. ã€Œé‡‘ç‰Œã€æ˜¯æŸ¥è©¢é‡é»
4. å¿«é€Ÿæ‰¾åˆ°æœ€ç›¸é—œçš„çµæœ

é€™å°±æ˜¯ã€Œæ³¨æ„åŠ›æ©Ÿåˆ¶ã€ï¼
```

### æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼šé—œè¯æ€§è¨ˆç®—

**æƒ…å¢ƒ**ï¼šç†è§£å¥å­ã€ŒThe animal didn't cross the street because it was too tiredã€

```
å•é¡Œï¼šã€Œitã€æŒ‡çš„æ˜¯ä»€éº¼ï¼Ÿ

Transformer çš„åšæ³•ï¼š
1. è¨ˆç®—ã€Œitã€å’Œæ¯å€‹è©çš„ã€Œé—œè¯åˆ†æ•¸ã€

   it â†â†’ The      : 0.02ï¼ˆä½ï¼‰
   it â†â†’ animal   : 0.87ï¼ˆé«˜ï¼ï¼‰
   it â†â†’ didn't   : 0.01
   it â†â†’ cross    : 0.03
   it â†â†’ street   : 0.15
   it â†â†’ because  : 0.05
   it â†â†’ was      : 0.08
   it â†â†’ too      : 0.04
   it â†â†’ tired    : 0.45ï¼ˆé«˜ï¼‰

2. çµè«–ï¼šã€Œitã€æœ€ç›¸é—œçš„æ˜¯ã€Œanimalã€
   â†’ it = animalï¼ˆå‹•ç‰©å¤ªç´¯äº†ï¼‰
```

---

## ğŸ” æ³¨æ„åŠ›æ©Ÿåˆ¶è©³è§£

### æ ¸å¿ƒæ¦‚å¿µï¼šQueryã€Keyã€Value

**æ¯”å–»**ï¼šYouTube æ¨è–¦ç³»çµ±

```
ä½ ï¼ˆQueryï¼‰ï¼šã€Œæˆ‘æƒ³çœ‹æœ‰è¶£çš„è²“å’ªå½±ç‰‡ã€
    â†“ æ¯”å°
YouTube å½±ç‰‡åº«ï¼ˆKeysï¼‰ï¼š
- å½±ç‰‡ 1ï¼ˆKeyï¼‰ï¼šã€Œè²“å’ªè·³èˆã€     â†’ ç›¸é—œåº¦ 0.95ï¼ˆé«˜ï¼ï¼‰
- å½±ç‰‡ 2ï¼ˆKeyï¼‰ï¼šã€Œç‹—ç‹—ç©çƒã€     â†’ ç›¸é—œåº¦ 0.30
- å½±ç‰‡ 3ï¼ˆKeyï¼‰ï¼šã€Œè²“å’ªç¡è¦ºã€     â†’ ç›¸é—œåº¦ 0.88ï¼ˆé«˜ï¼‰
- å½±ç‰‡ 4ï¼ˆKeyï¼‰ï¼šã€Œæ–°èå ±å°ã€     â†’ ç›¸é—œåº¦ 0.02

æ¨è–¦çµ¦ä½ ï¼ˆValuesï¼‰ï¼š
ä¸»è¦æ¨è–¦å½±ç‰‡ 1ï¼ˆè²“å’ªè·³èˆï¼‰
æ¬¡è¦æ¨è–¦å½±ç‰‡ 3ï¼ˆè²“å’ªç¡è¦ºï¼‰

é€™å°±æ˜¯ã€Œæ³¨æ„åŠ›ã€ï¼š
Queryï¼ˆéœ€æ±‚ï¼‰ Ã— Keysï¼ˆå€™é¸ï¼‰ â†’ æ‰¾å‡ºæœ€ç›¸é—œçš„ Values
```

### æ•¸å­¸è¡¨é”ï¼ˆç”Ÿæ´»åŒ–ï¼‰

```python
# 1. è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
scores = Query Ã— Keys^T
         â†‘
    ã€Œæˆ‘æƒ³çœ‹è²“å’ªã€å’Œã€Œå½±ç‰‡æ¨™é¡Œã€çš„ç›¸ä¼¼åº¦

# 2. æ­¸ä¸€åŒ–ï¼ˆè½‰æˆæ©Ÿç‡ï¼‰
attention_weights = softmax(scores / âˆšd_k)
                           â†‘
                    é™¤ä»¥ âˆšd_k é¿å…åˆ†æ•¸éå¤§

# 3. åŠ æ¬Šå¹³å‡
output = attention_weights Ã— Values
         â†‘
    æ ¹æ“šç›¸é—œåº¦ï¼Œçµ„åˆå½±ç‰‡å…§å®¹
```

### è¦–è¦ºåŒ–

```
     Query              Keys               Values
    ã€Œæˆ‘æƒ³çœ‹è²“ã€         ã€Œæ¨™é¡Œã€            ã€Œå½±ç‰‡å…§å®¹ã€
        â”‚                â”‚                    â”‚
        â””â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
             â”‚                                â”‚
             â†“                                â”‚
        [0.95, 0.30, 0.88, 0.02]              â”‚
             â”‚ (ç›¸é—œåº¦)                        â”‚
             â”‚                                â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Ã—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                    æ¨è–¦çµæœï¼ˆåŠ æ¬Šçµ„åˆï¼‰
```

---

## ğŸ—ï¸ Transformer å®Œæ•´æ¶æ§‹

### æ•´é«”çµæ§‹ï¼šç·¨ç¢¼å™¨-è§£ç¢¼å™¨

```
è¼¸å…¥å¥å­ï¼ˆè‹±æ–‡ï¼‰          è¼¸å‡ºå¥å­ï¼ˆä¸­æ–‡ï¼‰
    â†“                          â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç·¨ç¢¼å™¨  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ è§£ç¢¼å™¨  â”‚
â”‚(Encoder)â”‚  (å‚³éèªç¾©)   â”‚(Decoder)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ç†è§£è¼¸å…¥                  ç”Ÿæˆè¼¸å‡º
```

### ç·¨ç¢¼å™¨ï¼ˆEncoderï¼‰ï¼šç†è§£è¼¸å…¥

**æ¯”å–»**ï¼šé–±è®€ç†è§£å°ˆå®¶

```
è¼¸å…¥ï¼šã€ŒI love youã€

ç·¨ç¢¼å™¨åšçš„äº‹ï¼š
1. è©åµŒå…¥ï¼ˆWord Embeddingï¼‰
   I â†’ [0.2, 0.5, 0.1, ...]
   love â†’ [0.8, 0.3, 0.9, ...]
   you â†’ [0.1, 0.7, 0.4, ...]

2. ä½ç½®ç·¨ç¢¼ï¼ˆPositional Encodingï¼‰
   åŠ ä¸Šã€Œä½ç½®è³‡è¨Šã€
   Iï¼ˆä½ç½®1ï¼‰ loveï¼ˆä½ç½®2ï¼‰ youï¼ˆä½ç½®3ï¼‰

3. å¤šé ­è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰
   ã€ŒIã€çœ‹å‘æ‰€æœ‰è© â†’ ç†è§£ã€Œæˆ‘æ˜¯ä¸»èªã€
   ã€Œloveã€çœ‹å‘æ‰€æœ‰è© â†’ ç†è§£ã€Œæ„›æ˜¯å‹•è©ã€
   ã€Œyouã€çœ‹å‘æ‰€æœ‰è© â†’ ç†è§£ã€Œä½ æ˜¯è³“èªã€

4. å‰é¥‹ç¶²è·¯ï¼ˆFeed-Forwardï¼‰
   é€²ä¸€æ­¥è™•ç†è³‡è¨Š

5. é‡è¤‡ N æ¬¡ï¼ˆé€šå¸¸ 6 å±¤ï¼‰
   æ¯å±¤ç†è§£æ›´æ·±å…¥

è¼¸å‡ºï¼šå¥å­çš„ã€Œæ·±åº¦ç†è§£ã€ï¼ˆå‘é‡è¡¨ç¤ºï¼‰
```

### è§£ç¢¼å™¨ï¼ˆDecoderï¼‰ï¼šç”Ÿæˆè¼¸å‡º

**æ¯”å–»**ï¼šå¯«ä½œå°ˆå®¶

```
ä»»å‹™ï¼šæŠŠã€ŒI love youã€ç¿»è­¯æˆä¸­æ–‡

è§£ç¢¼å™¨åšçš„äº‹ï¼š
1. æ¥æ”¶ç·¨ç¢¼å™¨çš„ã€Œç†è§£ã€

2. ç”Ÿæˆç¬¬ 1 å€‹å­—ï¼š
   çœ‹åˆ°ã€Œ<START>ã€
   + ç·¨ç¢¼å™¨çš„ç†è§£
   â†’ é æ¸¬ï¼šã€Œæˆ‘ã€ï¼ˆ90% ä¿¡å¿ƒï¼‰

3. ç”Ÿæˆç¬¬ 2 å€‹å­—ï¼š
   çœ‹åˆ°ã€Œ<START> æˆ‘ã€
   + ç·¨ç¢¼å™¨çš„ç†è§£
   â†’ é æ¸¬ï¼šã€Œæ„›ã€ï¼ˆ85% ä¿¡å¿ƒï¼‰

4. ç”Ÿæˆç¬¬ 3 å€‹å­—ï¼š
   çœ‹åˆ°ã€Œ<START> æˆ‘ æ„›ã€
   + ç·¨ç¢¼å™¨çš„ç†è§£
   â†’ é æ¸¬ï¼šã€Œä½ ã€ï¼ˆ92% ä¿¡å¿ƒï¼‰

5. ç”ŸæˆçµæŸæ¨™è¨˜ï¼š
   çœ‹åˆ°ã€Œ<START> æˆ‘ æ„› ä½ ã€
   â†’ é æ¸¬ï¼šã€Œ<END>ã€

æœ€çµ‚è¼¸å‡ºï¼šã€Œæˆ‘æ„›ä½ ã€
```

---

## ğŸ’» æ ¸å¿ƒçµ„ä»¶å¯¦ä½œ

### 1. è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆSelf-Attentionï¼‰

```python
import numpy as np

class SelfAttention:
    def __init__(self, d_model):
        """
        è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶

        æ¯”å–»ï¼šè®“å¥å­ä¸­çš„æ¯å€‹è©äº’ç›¸ã€Œçœ‹ã€å°æ–¹

        åƒæ•¸ï¼š
            d_model: å‘é‡ç¶­åº¦
        """
        self.d_model = d_model

        # Queryã€Keyã€Value çš„è½‰æ›çŸ©é™£
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01

    def softmax(self, x):
        """Softmax"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    def forward(self, X):
        """
        å‰å‘å‚³æ’­

        æ¯”å–»ï¼š
        X = å¥å­ä¸­çš„æ‰€æœ‰è©å‘é‡
        æ¯å€‹è©éƒ½è¦ã€Œçœ‹ã€å…¶ä»–æ‰€æœ‰è©

        åƒæ•¸ï¼š
            X: è¼¸å…¥åºåˆ— (seq_len, d_model)

        è¿”å›ï¼š
            output: åŠ å…¥æ³¨æ„åŠ›å¾Œçš„è¡¨ç¤º
        """
        # 1. è¨ˆç®— Queryã€Keyã€Value
        Q = X.dot(self.W_q)  # (seq_len, d_model)
        K = X.dot(self.W_k)  # (seq_len, d_model)
        V = X.dot(self.W_v)  # (seq_len, d_model)

        # 2. è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        # Q Ã— K^Tï¼šæ¯å€‹è©å’Œå…¶ä»–è©çš„ç›¸é—œåº¦
        scores = Q.dot(K.T)  # (seq_len, seq_len)

        # 3. ç¸®æ”¾ï¼ˆé¿å…æ¢¯åº¦æ¶ˆå¤±ï¼‰
        scores = scores / np.sqrt(self.d_model)

        # 4. Softmaxï¼ˆè½‰æˆæ©Ÿç‡åˆ†ä½ˆï¼‰
        attention_weights = self.softmax(scores)  # (seq_len, seq_len)

        # 5. åŠ æ¬Šå¹³å‡ Values
        output = attention_weights.dot(V)  # (seq_len, d_model)

        return output, attention_weights


# æ¸¬è©¦
def test_self_attention():
    """æ¸¬è©¦è‡ªæ³¨æ„åŠ›"""

    # å‡è¨­å¥å­ï¼šã€ŒI love youã€ï¼ˆ3 å€‹è©ï¼‰
    # æ¯å€‹è©ç”¨ 4 ç¶­å‘é‡è¡¨ç¤º
    sentence = np.array([
        [0.1, 0.2, 0.3, 0.4],  # I
        [0.5, 0.6, 0.7, 0.8],  # love
        [0.9, 1.0, 1.1, 1.2]   # you
    ])

    # å‰µå»ºè‡ªæ³¨æ„åŠ›å±¤
    attention = SelfAttention(d_model=4)

    # è¨ˆç®—æ³¨æ„åŠ›
    output, weights = attention.forward(sentence)

    print("æ³¨æ„åŠ›æ¬Šé‡ï¼š")
    print("(æ¯ä¸€è¡Œä»£è¡¨ä¸€å€‹è©ã€Œçœ‹ã€å…¶ä»–è©çš„æ³¨æ„åŠ›)")
    print(weights)
    print("\nè§£é‡‹ï¼š")
    print("ç¬¬ 1 è¡Œï¼š'I' å° [I, love, you] çš„æ³¨æ„åŠ›")
    print("ç¬¬ 2 è¡Œï¼š'love' å° [I, love, you] çš„æ³¨æ„åŠ›")
    print("ç¬¬ 3 è¡Œï¼š'you' å° [I, love, you] çš„æ³¨æ„åŠ›")

test_self_attention()
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
æ³¨æ„åŠ›æ¬Šé‡ï¼š
[[0.33  0.34  0.33]   â† 'I' çš„æ³¨æ„åŠ›åˆ†ä½ˆ
 [0.32  0.35  0.33]   â† 'love' çš„æ³¨æ„åŠ›åˆ†ä½ˆ
 [0.31  0.34  0.35]]  â† 'you' çš„æ³¨æ„åŠ›åˆ†ä½ˆ

è§£é‡‹ï¼š
'you' å°è‡ªå·±çš„æ³¨æ„åŠ›æœ€é«˜ï¼ˆ0.35ï¼‰
é€™è¡¨ç¤ºåœ¨ç†è§£ 'you' æ™‚ï¼Œå®ƒè‡ªèº«çš„è³‡è¨Šæœ€é‡è¦
```

### 2. å¤šé ­æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

**æ¯”å–»**ï¼šå¤šè§’åº¦ç†è§£

```
å–®é ­æ³¨æ„åŠ›ï¼ˆ1 å€‹å°ˆå®¶ï¼‰ï¼š
åªå¾ã€Œèªç¾©ç›¸ä¼¼åº¦ã€è§’åº¦ç†è§£

å¤šé ­æ³¨æ„åŠ›ï¼ˆ8 å€‹å°ˆå®¶ï¼‰ï¼š
å°ˆå®¶ 1ï¼šçœ‹ã€Œèªç¾©ç›¸ä¼¼åº¦ã€
å°ˆå®¶ 2ï¼šçœ‹ã€Œèªæ³•çµæ§‹ã€ï¼ˆä¸»è¬‚è³“ï¼‰
å°ˆå®¶ 3ï¼šçœ‹ã€Œæƒ…æ„Ÿè‰²å½©ã€
å°ˆå®¶ 4ï¼šçœ‹ã€Œæ™‚æ…‹é—œä¿‚ã€
...
å°ˆå®¶ 8ï¼šçœ‹ã€Œå…¶ä»–ç‰¹å¾µã€

æœ€å¾Œï¼šç¶œåˆæ‰€æœ‰å°ˆå®¶æ„è¦‹
```

```python
class MultiHeadAttention:
    def __init__(self, d_model, num_heads):
        """
        å¤šé ­æ³¨æ„åŠ›

        æ¯”å–»ï¼šå¤šå€‹å°ˆå®¶å¾ä¸åŒè§’åº¦ç†è§£

        åƒæ•¸ï¼š
            d_model: æ¨¡å‹ç¶­åº¦ï¼ˆå¦‚ 512ï¼‰
            num_heads: é ­æ•¸ï¼ˆå¦‚ 8ï¼‰
        """
        self.num_heads = num_heads
        self.d_model = d_model

        # æ¯å€‹é ­çš„ç¶­åº¦
        self.d_k = d_model // num_heads

        # å‰µå»ºå¤šå€‹æ³¨æ„åŠ›é ­
        self.heads = [SelfAttention(self.d_k) for _ in range(num_heads)]

        # è¼¸å‡ºæŠ•å½±
        self.W_o = np.random.randn(d_model, d_model) * 0.01

    def forward(self, X):
        """
        å‰å‘å‚³æ’­

        æ¯”å–»ï¼š
        1. æŠŠè¼¸å…¥åˆ†çµ¦å„å€‹å°ˆå®¶
        2. æ¯å€‹å°ˆå®¶ç¨ç«‹åˆ†æ
        3. ç¶œåˆæ‰€æœ‰å°ˆå®¶æ„è¦‹
        """
        # 1. åˆ†å‰²è¼¸å…¥çµ¦å„å€‹é ­
        # (seq_len, d_model) â†’ (seq_len, num_heads, d_k)
        seq_len = X.shape[0]
        X_split = X.reshape(seq_len, self.num_heads, self.d_k)

        # 2. æ¯å€‹é ­ç¨ç«‹è¨ˆç®—æ³¨æ„åŠ›
        head_outputs = []
        for i, head in enumerate(self.heads):
            X_head = X_split[:, i, :]  # (seq_len, d_k)
            output, _ = head.forward(X_head)
            head_outputs.append(output)

        # 3. æ‹¼æ¥æ‰€æœ‰é ­çš„è¼¸å‡º
        concat = np.concatenate(head_outputs, axis=-1)  # (seq_len, d_model)

        # 4. ç·šæ€§æŠ•å½±
        output = concat.dot(self.W_o)

        return output

# æ¯”å–»ç¸½çµï¼š
# å–®é ­ = 1 å€‹å°ˆå®¶å¾ 1 å€‹è§’åº¦çœ‹
# å¤šé ­ = 8 å€‹å°ˆå®¶å¾ 8 å€‹è§’åº¦çœ‹ â†’ ç†è§£æ›´å…¨é¢
```

### 3. ä½ç½®ç·¨ç¢¼ï¼ˆPositional Encodingï¼‰

**å•é¡Œ**ï¼šTransformer ç„¡æ³•å€åˆ†é †åº

```
ã€Œæˆ‘æ„›ä½ ã€å’Œã€Œä½ æ„›æˆ‘ã€
åœ¨ Transformer ä¸­æ˜¯ã€ŒåŒä¸€å€‹ã€è©è¢‹
â†’ éœ€è¦åŠ å…¥ã€Œä½ç½®è³‡è¨Šã€
```

**è§£æ±º**ï¼šä½ç½®ç·¨ç¢¼

```python
def positional_encoding(max_len, d_model):
    """
    ä½ç½®ç·¨ç¢¼

    æ¯”å–»ï¼šçµ¦æ¯å€‹ä½ç½®ä¸€å€‹ã€Œèº«ä»½è­‰ã€

    åƒæ•¸ï¼š
        max_len: æœ€å¤§åºåˆ—é•·åº¦
        d_model: æ¨¡å‹ç¶­åº¦

    è¿”å›ï¼š
        PE: ä½ç½®ç·¨ç¢¼çŸ©é™£ (max_len, d_model)
    """
    PE = np.zeros((max_len, d_model))

    for pos in range(max_len):
        for i in range(0, d_model, 2):
            # ä½¿ç”¨æ­£å¼¦å’Œé¤˜å¼¦å‡½æ•¸
            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))
            if i + 1 < d_model:
                PE[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))

    return PE

# ä½¿ç”¨
pos_enc = positional_encoding(max_len=100, d_model=512)

# åŠ åˆ°è©åµŒå…¥ä¸Š
word_embedding = get_word_embedding("hello")  # (512,)
position = 5  # ç¬¬ 5 å€‹ä½ç½®
final_embedding = word_embedding + pos_enc[position]

# ç¾åœ¨ Transformer çŸ¥é“é€™å€‹è©åœ¨ç¬¬ 5 å€‹ä½ç½®äº†ï¼
```

**ç‚ºä»€éº¼ç”¨æ­£å¼¦/é¤˜å¼¦ï¼Ÿ**

```
å„ªé»ï¼š
1. å€¼åŸŸå›ºå®šï¼ˆ-1 åˆ° 1ï¼‰
2. å¯ä»¥è™•ç†ä»»æ„é•·åº¦çš„åºåˆ—
3. ç›¸å°ä½ç½®é—œä¿‚æ¸…æ™°
   ï¼ˆä½ç½® 5 å’Œä½ç½® 6 çš„ç·¨ç¢¼å¾ˆç›¸ä¼¼ï¼‰
```

### 4. å‰é¥‹ç¶²è·¯ï¼ˆFeed-Forward Networkï¼‰

**æ¯”å–»**ï¼šæ·±åº¦æ€è€ƒ

```
æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼šã€Œæˆ‘çœ‹åˆ°äº†ä»€éº¼ã€ï¼ˆè§€å¯Ÿï¼‰
å‰é¥‹ç¶²è·¯ï¼šã€Œé€™ä»£è¡¨ä»€éº¼æ„æ€ã€ï¼ˆæ€è€ƒï¼‰
```

```python
class FeedForward:
    def __init__(self, d_model, d_ff):
        """
        å‰é¥‹ç¶²è·¯

        æ¯”å–»ï¼šæ·±åº¦æ€è€ƒå±¤

        åƒæ•¸ï¼š
            d_model: è¼¸å…¥è¼¸å‡ºç¶­åº¦ï¼ˆå¦‚ 512ï¼‰
            d_ff: éš±è—å±¤ç¶­åº¦ï¼ˆå¦‚ 2048ï¼Œé€šå¸¸æ˜¯ d_model çš„ 4 å€ï¼‰
        """
        self.W1 = np.random.randn(d_model, d_ff) * 0.01
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * 0.01
        self.b2 = np.zeros(d_model)

    def relu(self, x):
        """ReLU æ¿€æ´»å‡½æ•¸"""
        return np.maximum(0, x)

    def forward(self, x):
        """
        å‰å‘å‚³æ’­

        æ¯”å–»ï¼š
        è¼¸å…¥ â†’ æ“´å±•æ€è€ƒï¼ˆå‡ç¶­åˆ° 2048ï¼‰â†’ æ•´åˆçµè«–ï¼ˆé™ç¶­å› 512ï¼‰
        """
        # ç¬¬ä¸€å±¤ï¼šæ“´å±•
        hidden = self.relu(x.dot(self.W1) + self.b1)

        # ç¬¬äºŒå±¤ï¼šæ•´åˆ
        output = hidden.dot(self.W2) + self.b2

        return output
```

---

## ğŸ¨ å®Œæ•´ Transformer å¯¦ä½œ

### ç·¨ç¢¼å™¨å±¤ï¼ˆEncoder Layerï¼‰

```python
class EncoderLayer:
    def __init__(self, d_model, num_heads, d_ff):
        """
        Transformer ç·¨ç¢¼å™¨å±¤

        çµ„ä»¶ï¼š
        1. å¤šé ­è‡ªæ³¨æ„åŠ›
        2. æ®˜å·®é€£æ¥ + å±¤æ¨™æº–åŒ–
        3. å‰é¥‹ç¶²è·¯
        4. æ®˜å·®é€£æ¥ + å±¤æ¨™æº–åŒ–
        """
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff)

    def layer_norm(self, x):
        """å±¤æ¨™æº–åŒ–"""
        mean = np.mean(x, axis=-1, keepdims=True)
        std = np.std(x, axis=-1, keepdims=True)
        return (x - mean) / (std + 1e-8)

    def forward(self, x):
        """
        å‰å‘å‚³æ’­

        æ¯”å–»ï¼š
        1. è‡ªæ³¨æ„åŠ›ï¼šã€Œçœ‹çœ‹å¥å­ä¸­å…¶ä»–è©ã€
        2. å‰é¥‹ç¶²è·¯ï¼šã€Œæ·±åº¦æ€è€ƒã€
        3. æ®˜å·®é€£æ¥ï¼šã€Œä¿ç•™åŸå§‹è³‡è¨Šã€
        """
        # 1. å¤šé ­è‡ªæ³¨æ„åŠ›
        attn_output = self.attention.forward(x)

        # 2. æ®˜å·®é€£æ¥ + å±¤æ¨™æº–åŒ–
        x = self.layer_norm(x + attn_output)

        # 3. å‰é¥‹ç¶²è·¯
        ff_output = self.feed_forward.forward(x)

        # 4. æ®˜å·®é€£æ¥ + å±¤æ¨™æº–åŒ–
        x = self.layer_norm(x + ff_output)

        return x
```

### è§£ç¢¼å™¨å±¤ï¼ˆDecoder Layerï¼‰

```python
class DecoderLayer:
    def __init__(self, d_model, num_heads, d_ff):
        """
        Transformer è§£ç¢¼å™¨å±¤

        çµ„ä»¶ï¼š
        1. æ©ç¢¼è‡ªæ³¨æ„åŠ›ï¼ˆMasked Self-Attentionï¼‰
        2. äº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰
        3. å‰é¥‹ç¶²è·¯
        """
        self.masked_attention = MultiHeadAttention(d_model, num_heads)
        self.cross_attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff)

    def forward(self, x, encoder_output, mask=None):
        """
        å‰å‘å‚³æ’­

        æ¯”å–»ï¼š
        1. çœ‹è‡ªå·±ç”Ÿæˆçš„å…§å®¹ï¼ˆä¸èƒ½å·çœ‹æœªä¾†ï¼‰
        2. çœ‹ç·¨ç¢¼å™¨çš„ç†è§£ï¼ˆè¼¸å…¥å¥å­çš„æ„æ€ï¼‰
        3. æ€è€ƒä¸‹ä¸€å€‹è©
        """
        # 1. æ©ç¢¼è‡ªæ³¨æ„åŠ›ï¼ˆåªèƒ½çœ‹ã€Œå·²ç”Ÿæˆã€çš„éƒ¨åˆ†ï¼‰
        masked_attn = self.masked_attention.forward(x)
        x = x + masked_attn

        # 2. äº¤å‰æ³¨æ„åŠ›ï¼ˆçœ‹ç·¨ç¢¼å™¨çš„è¼¸å‡ºï¼‰
        cross_attn = self.cross_attention.forward(encoder_output)
        x = x + cross_attn

        # 3. å‰é¥‹ç¶²è·¯
        ff_output = self.feed_forward.forward(x)
        x = x + ff_output

        return x
```

### å®Œæ•´ Transformer

```python
class Transformer:
    def __init__(self, src_vocab_size, tgt_vocab_size,
                 d_model=512, num_heads=8, num_layers=6, d_ff=2048):
        """
        å®Œæ•´ Transformer

        æ¯”å–»ï¼š
        ç·¨ç¢¼å™¨ = é–±è®€ç†è§£å°ˆå®¶ï¼ˆç†è§£è¼¸å…¥ï¼‰
        è§£ç¢¼å™¨ = å¯«ä½œå°ˆå®¶ï¼ˆç”Ÿæˆè¼¸å‡ºï¼‰

        åƒæ•¸ï¼š
            src_vocab_size: æºèªè¨€è©å½™é‡ï¼ˆå¦‚è‹±æ–‡ 10000 è©ï¼‰
            tgt_vocab_size: ç›®æ¨™èªè¨€è©å½™é‡ï¼ˆå¦‚ä¸­æ–‡ 5000 è©ï¼‰
            d_model: æ¨¡å‹ç¶­åº¦
            num_heads: æ³¨æ„åŠ›é ­æ•¸
            num_layers: ç·¨ç¢¼å™¨/è§£ç¢¼å™¨å±¤æ•¸
            d_ff: å‰é¥‹ç¶²è·¯éš±è—å±¤ç¶­åº¦
        """
        # è©åµŒå…¥å±¤
        self.src_embedding = np.random.randn(src_vocab_size, d_model) * 0.01
        self.tgt_embedding = np.random.randn(tgt_vocab_size, d_model) * 0.01

        # ä½ç½®ç·¨ç¢¼
        self.pos_encoding = positional_encoding(max_len=5000, d_model=d_model)

        # ç·¨ç¢¼å™¨ï¼ˆ6 å±¤ï¼‰
        self.encoder_layers = [
            EncoderLayer(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ]

        # è§£ç¢¼å™¨ï¼ˆ6 å±¤ï¼‰
        self.decoder_layers = [
            DecoderLayer(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ]

        # è¼¸å‡ºå±¤
        self.output_layer = np.random.randn(d_model, tgt_vocab_size) * 0.01

    def encode(self, src_tokens):
        """
        ç·¨ç¢¼ï¼šç†è§£è¼¸å…¥å¥å­

        æ¯”å–»ï¼šé–±è®€ä¸¦ç†è§£è‹±æ–‡å¥å­
        """
        # 1. è©åµŒå…¥
        x = self.src_embedding[src_tokens]

        # 2. åŠ ä¸Šä½ç½®ç·¨ç¢¼
        x = x + self.pos_encoding[:len(src_tokens)]

        # 3. é€šéç·¨ç¢¼å™¨å±¤
        for layer in self.encoder_layers:
            x = layer.forward(x)

        return x

    def decode(self, tgt_tokens, encoder_output):
        """
        è§£ç¢¼ï¼šç”Ÿæˆè¼¸å‡ºå¥å­

        æ¯”å–»ï¼šæ ¹æ“šè‹±æ–‡ç†è§£ï¼Œå¯«å‡ºä¸­æ–‡
        """
        # 1. è©åµŒå…¥
        x = self.tgt_embedding[tgt_tokens]

        # 2. åŠ ä¸Šä½ç½®ç·¨ç¢¼
        x = x + self.pos_encoding[:len(tgt_tokens)]

        # 3. é€šéè§£ç¢¼å™¨å±¤
        for layer in self.decoder_layers:
            x = layer.forward(x, encoder_output)

        # 4. è¼¸å‡ºå±¤ï¼ˆé æ¸¬ä¸‹ä¸€å€‹è©ï¼‰
        logits = x.dot(self.output_layer)

        return logits

    def translate(self, src_sentence, max_len=50):
        """
        ç¿»è­¯å¥å­

        æ¯”å–»ï¼š
        è¼¸å…¥è‹±æ–‡ â†’ ç†è§£ â†’ ç”Ÿæˆä¸­æ–‡
        """
        # 1. ç·¨ç¢¼è¼¸å…¥
        encoder_output = self.encode(src_sentence)

        # 2. é€æ­¥è§£ç¢¼ç”Ÿæˆ
        tgt_tokens = [START_TOKEN]  # å¾ <START> é–‹å§‹

        for _ in range(max_len):
            # è§£ç¢¼
            logits = self.decode(tgt_tokens, encoder_output)

            # é æ¸¬ä¸‹ä¸€å€‹è©
            next_token = np.argmax(logits[-1])

            # å¦‚æœæ˜¯çµæŸæ¨™è¨˜ï¼Œåœæ­¢
            if next_token == END_TOKEN:
                break

            tgt_tokens.append(next_token)

        return tgt_tokens[1:]  # å»æ‰ <START>

# ä½¿ç”¨ç¯„ä¾‹
transformer = Transformer(
    src_vocab_size=10000,  # è‹±æ–‡è©å½™
    tgt_vocab_size=5000,   # ä¸­æ–‡è©å½™
)

# ç¿»è­¯ã€ŒI love youã€â†’ã€Œæˆ‘æ„›ä½ ã€
english = [45, 892, 234]  # å‡è¨­é€™æ˜¯ "I love you" çš„ token IDs
chinese = transformer.translate(english)
print(f"ç¿»è­¯çµæœ: {chinese}")
```

---

## ğŸš€ Transformer çš„æ‡‰ç”¨

### 1. GPTï¼ˆç”Ÿæˆå¼é è¨“ç·´ Transformerï¼‰

**æ¶æ§‹**ï¼šåªç”¨è§£ç¢¼å™¨ï¼ˆDecoder-onlyï¼‰

```
ä»»å‹™ï¼šæ–‡æœ¬ç”Ÿæˆ

è¼¸å…¥ï¼šã€Œä»Šå¤©å¤©æ°£ã€
è¼¸å‡ºï¼šã€Œå¾ˆå¥½ã€

è¼¸å…¥ï¼šã€Œä»Šå¤©å¤©æ°£å¾ˆå¥½ã€
è¼¸å‡ºï¼šã€Œï¼Œé©åˆå‡ºéŠã€

â†’ æŒçºŒç”Ÿæˆæ–‡æœ¬
```

**æ¯”å–»**ï¼šæ¥é¾éŠæˆ²

```
ä½ èªªï¼šã€Œå¾å‰æœ‰åº§å±±ã€
GPTï¼šã€Œå±±ä¸Šæœ‰åº§å»Ÿã€
ä½ èªªï¼šã€Œå¾å‰æœ‰åº§å±±ï¼Œå±±ä¸Šæœ‰åº§å»Ÿã€
GPTï¼šã€Œå»Ÿè£¡æœ‰å€‹è€å’Œå°šã€
...
```

### 2. BERTï¼ˆé›™å‘ç·¨ç¢¼å™¨è¡¨ç¤ºï¼‰

**æ¶æ§‹**ï¼šåªç”¨ç·¨ç¢¼å™¨ï¼ˆEncoder-onlyï¼‰

```
ä»»å‹™ï¼šç†è§£æ–‡æœ¬ï¼ˆä¸ç”Ÿæˆï¼‰

æ‡‰ç”¨ï¼š
- æƒ…æ„Ÿåˆ†æï¼šã€Œé€™éƒ¨é›»å½±å¾ˆæ£’ã€â†’ æ­£é¢
- å•ç­”ç³»çµ±ï¼šã€Œå°åŒ—åœ¨å“ªï¼Ÿã€â†’ ã€Œå°ç£åŒ—éƒ¨ã€
- æ–‡æœ¬åˆ†é¡ï¼šã€Œè˜‹æœç™¼å¸ƒæ–°æ‰‹æ©Ÿã€â†’ ç§‘æŠ€é¡
```

**æ¯”å–»**ï¼šé–±è®€ç†è§£å°ˆå®¶

```
BERT åƒå­¸ç”Ÿåšé–±è®€ç†è§£ï¼š
1. è®€å®Œæ•´ç¯‡æ–‡ç« 
2. ç†è§£æ–‡ç« æ„æ€
3. å›ç­”å•é¡Œ
ï¼ˆä½†ä¸æœƒã€Œå¯«ä½œæ–‡ã€ï¼‰
```

### 3. T5ï¼ˆText-to-Text Transfer Transformerï¼‰

**æ¶æ§‹**ï¼šå®Œæ•´ Transformerï¼ˆEncoder-Decoderï¼‰

```
ä»»å‹™ï¼šè¬èƒ½æ–‡æœ¬è½‰æ›

ç¿»è­¯ï¼šã€Œtranslate English to Chinese: I love youã€
     â†’ ã€Œæˆ‘æ„›ä½ ã€

æ‘˜è¦ï¼šã€Œsummarize: [é•·æ–‡ç« ]ã€
     â†’ ã€Œ[æ‘˜è¦]ã€

å•ç­”ï¼šã€Œquestion: What is AI? context: [æ–‡ç« ]ã€
     â†’ ã€Œäººå·¥æ™ºæ…§æ˜¯...ã€
```

**æ¯”å–»**ï¼šç‘å£«åˆ€ï¼ˆè¬èƒ½å·¥å…·ï¼‰

---

## ğŸ“Š Transformer vs RNN/LSTM

| ç‰¹æ€§ | RNN/LSTM | Transformer |
|------|----------|-------------|
| **ä¸¦è¡Œæ€§** | âŒ å¿…é ˆé †åºè™•ç† | âœ… å®Œå…¨ä¸¦è¡Œ |
| **è¨“ç·´é€Ÿåº¦** | â­â­ æ…¢ | â­â­â­â­â­ å¿« |
| **é•·è·é›¢ä¾è³´** | â­â­ æœƒè¡°æ¸› | â­â­â­â­â­ ä¸è¡°æ¸› |
| **è¨˜æ†¶é«”éœ€æ±‚** | â­â­â­â­ å° | â­â­ å¤§ï¼ˆO(nÂ²)ï¼‰ |
| **å¯è§£é‡‹æ€§** | â­â­ é›£ | â­â­â­â­ æ³¨æ„åŠ›å¯è¦–åŒ– |
| **çŸ­åºåˆ—ï¼ˆ<100ï¼‰** | â­â­â­â­ | â­â­â­â­ |
| **é•·åºåˆ—ï¼ˆ>1000ï¼‰** | â­â­ | â­â­â­â­â­ |

**é¸æ“‡å»ºè­°**ï¼š
- âœ… **Transformer**ï¼šç¾ä»£ NLP é¦–é¸ï¼ˆGPTã€BERTï¼‰
- ğŸ”„ **RNN/LSTM**ï¼šè³‡æºå—é™ã€å¯¦æ™‚è™•ç†

---

## ğŸ“ å¯¦å‹™æŠ€å·§

### 1. æ³¨æ„åŠ›è¦–è¦ºåŒ–

```python
def visualize_attention(attention_weights, src_words, tgt_words):
    """
    è¦–è¦ºåŒ–æ³¨æ„åŠ›æ¬Šé‡

    æ¯”å–»ï¼šçœ‹ç¿»è­¯æ™‚ã€Œå°é½Šã€å“ªäº›è©
    """
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(10, 10))

    # ç¹ªè£½ç†±åŠ›åœ–
    im = ax.imshow(attention_weights, cmap='Blues')

    # è¨­å®šæ¨™ç±¤
    ax.set_xticks(range(len(src_words)))
    ax.set_yticks(range(len(tgt_words)))
    ax.set_xticklabels(src_words)
    ax.set_yticklabels(tgt_words)

    # æ—‹è½‰æ¨™ç±¤
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right")

    # æ¨™é¡Œ
    ax.set_title("Attention Weights")
    ax.set_xlabel("Source (English)")
    ax.set_ylabel("Target (Chinese)")

    plt.colorbar(im)
    plt.tight_layout()
    plt.savefig('attention_visualization.png', dpi=150)
    plt.show()

# ä½¿ç”¨
src = ['I', 'love', 'you']
tgt = ['æˆ‘', 'æ„›', 'ä½ ']
# attention_weights: (3, 3) çŸ©é™£
visualize_attention(attention_weights, src, tgt)
```

### 2. å­¸ç¿’ç‡é ç†±ï¼ˆWarmupï¼‰

**å•é¡Œ**ï¼šTransformer è¨“ç·´åˆæœŸä¸ç©©å®š

**è§£æ±º**ï¼šå­¸ç¿’ç‡é ç†±

```python
def transformer_lr_schedule(step, d_model, warmup_steps=4000):
    """
    Transformer å­¸ç¿’ç‡èª¿åº¦

    æ¯”å–»ï¼š
    - å‰æœŸï¼ˆwarmupï¼‰ï¼šæ…¢æ…¢åŠ é€Ÿï¼ˆå­¸ç¿’ç‡éå¢ï¼‰
    - å¾ŒæœŸï¼šé€æ¼¸æ¸›é€Ÿï¼ˆå­¸ç¿’ç‡éæ¸›ï¼‰

    å°±åƒé–‹è»Šï¼š
    èµ·æ­¥æ…¢ â†’ åŠ é€Ÿ â†’ å·¡èˆªé€Ÿåº¦ â†’ æ¸›é€Ÿ
    """
    arg1 = step ** -0.5
    arg2 = step * (warmup_steps ** -1.5)

    lr = (d_model ** -0.5) * min(arg1, arg2)

    return lr

# è¦–è¦ºåŒ–
import matplotlib.pyplot as plt

steps = range(1, 10000)
lrs = [transformer_lr_schedule(s, d_model=512) for s in steps]

plt.figure(figsize=(10, 6))
plt.plot(steps, lrs)
plt.xlabel('Steps')
plt.ylabel('Learning Rate')
plt.title('Transformer Learning Rate Schedule')
plt.grid(True)
plt.show()
```

### 3. Label Smoothing

**å•é¡Œ**ï¼šæ¨¡å‹éåº¦è‡ªä¿¡

```
æ™®é€šè¨“ç·´ï¼š
æ­£ç¢ºç­”æ¡ˆã€Œæˆ‘ã€â†’ æ©Ÿç‡ 1.0
å…¶ä»–ç­”æ¡ˆ     â†’ æ©Ÿç‡ 0.0

å•é¡Œï¼šéæ–¼çµ•å°ï¼Œä¸å¤ éˆæ´»
```

**è§£æ±º**ï¼šæ¨™ç±¤å¹³æ»‘

```python
def label_smoothing(true_label, vocab_size, smoothing=0.1):
    """
    æ¨™ç±¤å¹³æ»‘

    æ¯”å–»ï¼š
    ä¸è¦å¤ªçµ•å°ï¼Œä¿æŒä¸€é»ã€Œæ‡·ç–‘ã€

    æ­£ç¢ºç­”æ¡ˆï¼š0.9ï¼ˆåŸæœ¬ 1.0ï¼‰
    å…¶ä»–ç­”æ¡ˆï¼š0.1 / (vocab_size - 1)
    """
    confidence = 1.0 - smoothing
    smooth_value = smoothing / (vocab_size - 1)

    # åˆå§‹åŒ–ï¼ˆæ‰€æœ‰è©éƒ½æœ‰å°æ©Ÿç‡ï¼‰
    smoothed = np.full(vocab_size, smooth_value)

    # æ­£ç¢ºç­”æ¡ˆæœ‰æ›´é«˜æ©Ÿç‡
    smoothed[true_label] = confidence

    return smoothed
```

---

## ğŸ”— ç¸½çµ

### Transformer é©å‘½æ€§å‰µæ–°

1. **è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶**ï¼šè‡ªå‹•æ‰¾å‡ºè©ä¹‹é–“çš„é—œè¯
2. **ä¸¦è¡Œè¨ˆç®—**ï¼šä¸ç”¨æŒ‰é †åºè™•ç†ï¼Œè¶…å¿«
3. **é•·è·é›¢ä¾è³´**ï¼šç¬¬ 1 å€‹è©å’Œç¬¬ 1000 å€‹è©ç›´æ¥é€£æ¥

### æ ¸å¿ƒçµ„ä»¶

- **ç·¨ç¢¼å™¨**ï¼šç†è§£è¼¸å…¥
- **è§£ç¢¼å™¨**ï¼šç”Ÿæˆè¼¸å‡º
- **æ³¨æ„åŠ›**ï¼šæ‰¾å‡ºé‡è¦é—œè¯
- **ä½ç½®ç·¨ç¢¼**ï¼šåŠ å…¥é †åºè³‡è¨Š

### ä¸»è¦æ‡‰ç”¨

- **GPT**ï¼šæ–‡æœ¬ç”Ÿæˆï¼ˆChatGPTï¼‰
- **BERT**ï¼šæ–‡æœ¬ç†è§£ï¼ˆæœå°‹ã€åˆ†é¡ï¼‰
- **T5**ï¼šé€šç”¨è½‰æ›

### ä¸‹ä¸€æ­¥å­¸ç¿’

- **Vision Transformer**ï¼šTransformer ç”¨æ–¼åœ–åƒ
- **Efficient Transformers**ï¼šé™ä½è¨˜æ†¶é«”éœ€æ±‚
- **Sparse Attention**ï¼šç¨€ç–æ³¨æ„åŠ›æ©Ÿåˆ¶

---

*æœ€å¾Œæ›´æ–°: 2025-11-26*
