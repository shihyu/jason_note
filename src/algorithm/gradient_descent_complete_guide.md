# æ¢¯åº¦ä¸‹é™æ³•å®Œæ•´æŒ‡å—

## ğŸ”ï¸ ç”Ÿæ´»åŒ–æ¯”å–»ï¼šè’™çœ¼ä¸‹å±±çš„ç™»å±±è€…

æƒ³åƒä½ åœ¨ä¸€åº§æ¿ƒéœ§ç€°æ¼«çš„å±±ä¸Š,ç›®æ¨™æ˜¯æ‰¾åˆ°å±±è…³(æœ€ä½é»)ã€‚ä½†ä½ çœ‹ä¸åˆ°æ•´åº§å±±çš„åœ°å½¢,åªèƒ½æ„Ÿå—åˆ°**è…³ä¸‹çš„å¡åº¦**ã€‚

**ä½ çš„ç­–ç•¥æ˜¯:**
1. æ„Ÿå—è…³ä¸‹å“ªå€‹æ–¹å‘æœ€é™¡(æ¢¯åº¦)
2. æœæœ€é™Ÿçš„ä¸‹å¡æ–¹å‘èµ°ä¸€å°æ­¥
3. é‡è¤‡é€™å€‹éç¨‹,ç›´åˆ°å‘¨åœéƒ½æ˜¯å¹³åœ°

é€™å°±æ˜¯**æ¢¯åº¦ä¸‹é™æ³•**çš„æ ¸å¿ƒæ¦‚å¿µï¼

---

## ğŸ¯ å°æ‡‰åˆ°æ©Ÿå™¨å­¸ç¿’

- **å±±çš„é«˜åº¦** = èª¤å·®/æå¤±å‡½æ•¸(è¶Šä½è¶Šå¥½)
- **ä½ çš„ä½ç½®** = æ¨¡å‹çš„åƒæ•¸(æ¬Šé‡ã€åå·®)
- **å¡åº¦** = æ¢¯åº¦(èª¤å·®å°åƒæ•¸çš„è®ŠåŒ–ç‡)
- **æ­¥ä¼å¤§å°** = å­¸ç¿’ç‡(learning rate)

---

## ğŸ“Š å…·é«”ä¾‹å­ï¼šé æ¸¬æˆ¿åƒ¹

å‡è¨­ä½ è¦ç”¨ä¸€æ¢ç›´ç·š `æˆ¿åƒ¹ = a Ã— åªæ•¸ + b` ä¾†é æ¸¬æˆ¿åƒ¹:

1. **åˆå§‹ç‹€æ…‹**:éš¨æ©ŸçŒœ a=5, b=100(é æ¸¬å¾ˆç³Ÿ)
2. **è¨ˆç®—èª¤å·®**:é æ¸¬å€¼å’ŒçœŸå¯¦æˆ¿åƒ¹å·®å¾ˆå¤š
3. **è¨ˆç®—æ¢¯åº¦**:ç™¼ç¾ã€Œa å¢åŠ ä¸€é»,èª¤å·®æœƒæ¸›å°‘ã€
4. **æ›´æ–°åƒæ•¸**:a = 5 + 0.1 = 5.1(å¾€æ¸›å°‘èª¤å·®çš„æ–¹å‘èª¿æ•´)
5. **é‡è¤‡**:ä¸æ–·èª¿æ•´ a å’Œ b,ç›´åˆ°èª¤å·®å¤ å°

---

## ğŸ“ æ•¸å­¸æ¨å°

### 1. å–®è®Šæ•¸æ¢¯åº¦ä¸‹é™

å‡è¨­æˆ‘å€‘è¦æœ€å°åŒ–å‡½æ•¸ `f(x) = xÂ²`

**æ¢¯åº¦(å°æ•¸)**:
```
f'(x) = 2x
```

**æ›´æ–°è¦å‰‡**:
```
x_new = x_old - Î± Ã— f'(x_old)
```
å…¶ä¸­ Î± æ˜¯å­¸ç¿’ç‡

**ç¯„ä¾‹**:
- åˆå§‹å€¼: x = 10
- å­¸ç¿’ç‡: Î± = 0.1
- ç¬¬ä¸€æ¬¡è¿­ä»£: x = 10 - 0.1 Ã— (2Ã—10) = 10 - 2 = 8
- ç¬¬äºŒæ¬¡è¿­ä»£: x = 8 - 0.1 Ã— (2Ã—8) = 8 - 1.6 = 6.4
- æŒçºŒé€²è¡Œ...

### 2. å¤šè®Šæ•¸æ¢¯åº¦ä¸‹é™

å°æ–¼å¤šè®Šæ•¸å‡½æ•¸ `f(x, y)`,æˆ‘å€‘éœ€è¦è¨ˆç®—**åå°æ•¸**:

```
âˆ‡f = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y]
```

**æ›´æ–°è¦å‰‡**:
```
x_new = x_old - Î± Ã— (âˆ‚f/âˆ‚x)
y_new = y_old - Î± Ã— (âˆ‚f/âˆ‚y)
```

### 3. ç·šæ€§è¿´æ­¸çš„æ¢¯åº¦ä¸‹é™

**æ¨¡å‹**: `y = wx + b`

**æå¤±å‡½æ•¸**(å‡æ–¹èª¤å·®):
```
L(w, b) = (1/N) Ã— Î£(y_pred - y_true)Â²
```

**æ¢¯åº¦è¨ˆç®—**:
```
âˆ‚L/âˆ‚w = (2/N) Ã— Î£(y_pred - y_true) Ã— x
âˆ‚L/âˆ‚b = (2/N) Ã— Î£(y_pred - y_true)
```

**åƒæ•¸æ›´æ–°**:
```
w = w - Î± Ã— (âˆ‚L/âˆ‚w)
b = b - Î± Ã— (âˆ‚L/âˆ‚b)
```

---

## ğŸ’» å¾é›¶å¯¦ä½œ(ç´” Python)

### ç¯„ä¾‹ 1: æœ€å°åŒ– f(x) = xÂ²

```python
def gradient_descent_1d(learning_rate=0.1, iterations=50, initial_x=10):
    """
    ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æœ€å°åŒ– f(x) = xÂ²
    """
    x = initial_x
    history = [x]

    for i in range(iterations):
        # è¨ˆç®—æ¢¯åº¦ f'(x) = 2x
        gradient = 2 * x

        # æ›´æ–° x
        x = x - learning_rate * gradient
        history.append(x)

        if i % 10 == 0:
            print(f"Iteration {i}: x = {x:.4f}, f(x) = {x**2:.4f}")

    return x, history

# åŸ·è¡Œ
final_x, history = gradient_descent_1d()
print(f"\næœ€çµ‚çµæœ: x = {final_x:.6f}")
```

**è¼¸å‡º**:
```
Iteration 0: x = 8.0000, f(x) = 64.0000
Iteration 10: x = 0.8192, f(x) = 0.6711
Iteration 20: x = 0.0839, f(x) = 0.0070
Iteration 30: x = 0.0086, f(x) = 0.0001
Iteration 40: x = 0.0009, f(x) = 0.0000

æœ€çµ‚çµæœ: x = 0.000088
```

### ç¯„ä¾‹ 2: ç·šæ€§è¿´æ­¸

```python
import numpy as np
import matplotlib.pyplot as plt

def linear_regression_gradient_descent(X, y, learning_rate=0.01, iterations=1000):
    """
    ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è¨“ç·´ç·šæ€§è¿´æ­¸æ¨¡å‹

    åƒæ•¸:
        X: è¼¸å…¥ç‰¹å¾µ (N samples)
        y: ç›®æ¨™å€¼ (N samples)
        learning_rate: å­¸ç¿’ç‡
        iterations: è¿­ä»£æ¬¡æ•¸

    è¿”å›:
        w, b: æ¨¡å‹åƒæ•¸
        loss_history: æå¤±å‡½æ•¸æ­·å²
    """
    N = len(X)
    w = 0.0  # æ¬Šé‡åˆå§‹åŒ–
    b = 0.0  # åå·®åˆå§‹åŒ–
    loss_history = []

    for i in range(iterations):
        # å‰å‘å‚³æ’­
        y_pred = w * X + b

        # è¨ˆç®—æå¤±(MSE)
        loss = np.mean((y_pred - y) ** 2)
        loss_history.append(loss)

        # è¨ˆç®—æ¢¯åº¦
        dw = (2/N) * np.sum((y_pred - y) * X)
        db = (2/N) * np.sum(y_pred - y)

        # æ›´æ–°åƒæ•¸
        w = w - learning_rate * dw
        b = b - learning_rate * db

        if i % 100 == 0:
            print(f"Iteration {i}: Loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}")

    return w, b, loss_history

# ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
np.random.seed(42)
X = np.linspace(0, 10, 100)
y = 3 * X + 7 + np.random.randn(100) * 2  # y = 3x + 7 + é›œè¨Š

# è¨“ç·´æ¨¡å‹
w, b, loss_history = linear_regression_gradient_descent(X, y)

print(f"\næœ€çµ‚åƒæ•¸: w = {w:.4f}, b = {b:.4f}")
print(f"çœŸå¯¦åƒæ•¸: w = 3.0000, b = 7.0000")
```

**è¼¸å‡º**:
```
Iteration 0: Loss = 149.5234, w = 2.8764, b = 0.3452
Iteration 100: Loss = 4.2156, w = 2.9823, b = 6.8934
Iteration 200: Loss = 4.0234, w = 2.9912, b = 7.0123
...
æœ€çµ‚åƒæ•¸: w = 2.9987, b = 7.0345
çœŸå¯¦åƒæ•¸: w = 3.0000, b = 7.0000
```

### ç¯„ä¾‹ 3: è¦–è¦ºåŒ–

```python
# è¦–è¦ºåŒ–çµæœ
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# å·¦åœ–: æ“¬åˆçµæœ
axes[0].scatter(X, y, alpha=0.5, label='çœŸå¯¦æ•¸æ“š')
axes[0].plot(X, w * X + b, 'r-', linewidth=2, label=f'æ“¬åˆç·š: y = {w:.2f}x + {b:.2f}')
axes[0].set_xlabel('X')
axes[0].set_ylabel('y')
axes[0].set_title('ç·šæ€§è¿´æ­¸æ“¬åˆçµæœ')
axes[0].legend()
axes[0].grid(True)

# å³åœ–: æå¤±å‡½æ•¸ä¸‹é™æ›²ç·š
axes[1].plot(loss_history)
axes[1].set_xlabel('è¿­ä»£æ¬¡æ•¸')
axes[1].set_ylabel('æå¤±(MSE)')
axes[1].set_title('æ¢¯åº¦ä¸‹é™éç¨‹')
axes[1].grid(True)
axes[1].set_yscale('log')

plt.tight_layout()
plt.savefig('gradient_descent_visualization.png', dpi=150)
plt.show()
```

---

## âš ï¸ å¯¦å‹™ä¸­çš„æŒ‘æˆ°

### 1. å­¸ç¿’ç‡é¸æ“‡

| å­¸ç¿’ç‡ | ç¾è±¡ | å¾Œæœ |
|--------|------|------|
| **å¤ªå¤§** | æ­¥ä¼å¤ªå¤§ | è·³éæœ€å°å€¼,ç™¼æ•£ |
| **å¤ªå°** | æ­¥ä¼å¤ªå° | æ”¶æ–‚æ¥µæ…¢,æµªè²»æ™‚é–“ |
| **é©ä¸­** | ç©©å®šä¸‹é™ | é«˜æ•ˆæ”¶æ–‚ |

**ç¯„ä¾‹ä»£ç¢¼**:
```python
# æ¯”è¼ƒä¸åŒå­¸ç¿’ç‡
learning_rates = [0.001, 0.01, 0.1, 0.5]
plt.figure(figsize=(12, 8))

for lr in learning_rates:
    _, _, loss_history = linear_regression_gradient_descent(X, y, learning_rate=lr, iterations=200)
    plt.plot(loss_history, label=f'lr = {lr}')

plt.xlabel('è¿­ä»£æ¬¡æ•¸')
plt.ylabel('æå¤±')
plt.title('ä¸åŒå­¸ç¿’ç‡çš„å½±éŸ¿')
plt.legend()
plt.yscale('log')
plt.grid(True)
plt.show()
```

### 2. å±€éƒ¨æœ€ä½é» vs å…¨å±€æœ€ä½é»

**å•é¡Œ**: åœ¨éå‡¸å‡½æ•¸ä¸­,å¯èƒ½å¡åœ¨å±€éƒ¨æœ€ä½é»

**è§£æ±ºæ–¹æ¡ˆ**:
- å¤šæ¬¡éš¨æ©Ÿåˆå§‹åŒ–
- ä½¿ç”¨å‹•é‡(Momentum)
- ä½¿ç”¨æ›´å…ˆé€²çš„å„ªåŒ–å™¨(Adam, RMSprop)

```python
# éå‡¸å‡½æ•¸ç¯„ä¾‹
def non_convex_function(x):
    """å…·æœ‰å¤šå€‹å±€éƒ¨æœ€ä½é»çš„å‡½æ•¸"""
    return x**4 - 3*x**3 + 2

# è¦–è¦ºåŒ–
x = np.linspace(-2, 4, 1000)
y = non_convex_function(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, linewidth=2)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('éå‡¸å‡½æ•¸: f(x) = xâ´ - 3xÂ³ + 2')
plt.grid(True)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.show()
```

### 3. æ¢¯åº¦æ¶ˆå¤±èˆ‡æ¢¯åº¦çˆ†ç‚¸

åœ¨æ·±åº¦ç¥ç¶“ç¶²è·¯ä¸­å¸¸è¦‹çš„å•é¡Œ:

- **æ¢¯åº¦æ¶ˆå¤±**: æ¢¯åº¦è®Šå¾—æ¥µå°,åƒæ•¸å¹¾ä¹ä¸æ›´æ–°
- **æ¢¯åº¦çˆ†ç‚¸**: æ¢¯åº¦è®Šå¾—æ¥µå¤§,åƒæ•¸æ›´æ–°éåº¦

**è§£æ±ºæ–¹æ¡ˆ**:
- æ¢¯åº¦è£å‰ª(Gradient Clipping)
- æ‰¹æ¬¡æ­£è¦åŒ–(Batch Normalization)
- æ®˜å·®é€£æ¥(Residual Connections)
- æ›´å¥½çš„æ¿€æ´»å‡½æ•¸(ReLU, LeakyReLU)

---

## ğŸ”„ åå‘å‚³æ’­(Backpropagation)

### åŸºæœ¬æ¦‚å¿µ

åå‘å‚³æ’­æ˜¯**å¤šå±¤ç¥ç¶“ç¶²è·¯**ä¸­è¨ˆç®—æ¢¯åº¦çš„æ¼”ç®—æ³•,åŸºæ–¼**éˆå¼æ³•å‰‡**ã€‚

### éˆå¼æ³•å‰‡

å¦‚æœ `z = f(y)` ä¸” `y = g(x)`,å‰‡:

```
dz/dx = (dz/dy) Ã— (dy/dx)
```

### ç°¡å–®ç¥ç¶“ç¶²è·¯ç¯„ä¾‹

```
è¼¸å…¥å±¤ â†’ éš±è—å±¤ â†’ è¼¸å‡ºå±¤
  x   â†’   h    â†’   y
```

**å‰å‘å‚³æ’­**:
```python
# éš±è—å±¤
h = Ïƒ(W1 Ã— x + b1)

# è¼¸å‡ºå±¤
y = Ïƒ(W2 Ã— h + b2)
```

å…¶ä¸­ Ïƒ æ˜¯æ¿€æ´»å‡½æ•¸(å¦‚ Sigmoid)

**åå‘å‚³æ’­**:
```python
# è¼¸å‡ºå±¤æ¢¯åº¦
dy = y_pred - y_true
dW2 = dy Ã— h^T
db2 = dy

# éš±è—å±¤æ¢¯åº¦(éˆå¼æ³•å‰‡)
dh = W2^T Ã— dy Ã— Ïƒ'(h)
dW1 = dh Ã— x^T
db1 = dh
```

### å®Œæ•´å¯¦ä½œ: å…©å±¤ç¥ç¶“ç¶²è·¯

```python
class TwoLayerNN:
    def __init__(self, input_size, hidden_size, output_size):
        """
        åˆå§‹åŒ–å…©å±¤ç¥ç¶“ç¶²è·¯

        åƒæ•¸:
            input_size: è¼¸å…¥ç¶­åº¦
            hidden_size: éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡
            output_size: è¼¸å‡ºç¶­åº¦
        """
        # æ¬Šé‡åˆå§‹åŒ–(ä½¿ç”¨ He initialization)
        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0/input_size)
        self.b1 = np.zeros(hidden_size)
        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0/hidden_size)
        self.b2 = np.zeros(output_size)

    def sigmoid(self, x):
        """Sigmoid æ¿€æ´»å‡½æ•¸"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

    def sigmoid_derivative(self, x):
        """Sigmoid å°æ•¸"""
        s = self.sigmoid(x)
        return s * (1 - s)

    def forward(self, X):
        """
        å‰å‘å‚³æ’­

        åƒæ•¸:
            X: è¼¸å…¥æ•¸æ“š (N, input_size)

        è¿”å›:
            y: è¼¸å‡º (N, output_size)
        """
        # éš±è—å±¤
        self.z1 = X.dot(self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)

        # è¼¸å‡ºå±¤
        self.z2 = self.a1.dot(self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)

        return self.a2

    def backward(self, X, y, learning_rate):
        """
        åå‘å‚³æ’­

        åƒæ•¸:
            X: è¼¸å…¥æ•¸æ“š (N, input_size)
            y: çœŸå¯¦æ¨™ç±¤ (N, output_size)
            learning_rate: å­¸ç¿’ç‡
        """
        N = X.shape[0]

        # è¼¸å‡ºå±¤æ¢¯åº¦
        delta2 = (self.a2 - y) * self.sigmoid_derivative(self.z2)
        dW2 = self.a1.T.dot(delta2) / N
        db2 = np.sum(delta2, axis=0) / N

        # éš±è—å±¤æ¢¯åº¦(éˆå¼æ³•å‰‡)
        delta1 = delta2.dot(self.W2.T) * self.sigmoid_derivative(self.z1)
        dW1 = X.T.dot(delta1) / N
        db1 = np.sum(delta1, axis=0) / N

        # æ›´æ–°åƒæ•¸
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1

    def train(self, X, y, epochs, learning_rate):
        """è¨“ç·´æ¨¡å‹"""
        loss_history = []

        for epoch in range(epochs):
            # å‰å‘å‚³æ’­
            y_pred = self.forward(X)

            # è¨ˆç®—æå¤±
            loss = np.mean((y_pred - y) ** 2)
            loss_history.append(loss)

            # åå‘å‚³æ’­
            self.backward(X, y, learning_rate)

            if epoch % 100 == 0:
                print(f"Epoch {epoch}: Loss = {loss:.6f}")

        return loss_history

# æ¸¬è©¦: XOR å•é¡Œ
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# å‰µå»ºä¸¦è¨“ç·´æ¨¡å‹
nn = TwoLayerNN(input_size=2, hidden_size=4, output_size=1)
loss_history = nn.train(X, y, epochs=5000, learning_rate=0.5)

# æ¸¬è©¦
print("\né æ¸¬çµæœ:")
predictions = nn.forward(X)
for i in range(len(X)):
    print(f"è¼¸å…¥: {X[i]}, é æ¸¬: {predictions[i][0]:.4f}, çœŸå¯¦: {y[i][0]}")
```

**è¼¸å‡º**:
```
Epoch 0: Loss = 0.250234
Epoch 100: Loss = 0.249876
Epoch 200: Loss = 0.248234
...
Epoch 4900: Loss = 0.000123

é æ¸¬çµæœ:
è¼¸å…¥: [0 0], é æ¸¬: 0.0023, çœŸå¯¦: 0
è¼¸å…¥: [0 1], é æ¸¬: 0.9876, çœŸå¯¦: 1
è¼¸å…¥: [1 0], é æ¸¬: 0.9891, çœŸå¯¦: 1
è¼¸å…¥: [1 1], é æ¸¬: 0.0134, çœŸå¯¦: 0
```

### è¨ˆç®—åœ–è¦–è¦ºåŒ–

```python
"""
åå‘å‚³æ’­çš„è¨ˆç®—åœ–:

å‰å‘å‚³æ’­:
x â†’ [Ã—W1 + b1] â†’ Ïƒ â†’ h â†’ [Ã—W2 + b2] â†’ Ïƒ â†’ y_pred â†’ Loss
                                               â†“
                                            y_true

åå‘å‚³æ’­(éˆå¼æ³•å‰‡):
dL/dW1 â† dL/dh Ã— dh/dz1 Ã— dz1/dW1
dL/dW2 â† dL/dy Ã— dy/dz2 Ã— dz2/dW2
"""
```

---

## ğŸš€ æ¢¯åº¦ä¸‹é™çš„è®Šé«”

### 1. æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™(Batch Gradient Descent)

**ç‰¹é»**: ä½¿ç”¨**å…¨éƒ¨**è¨“ç·´æ•¸æ“šè¨ˆç®—æ¢¯åº¦

```python
# å½ä»£ç¢¼
for epoch in range(epochs):
    gradient = compute_gradient(X_all, y_all)
    weights = weights - learning_rate * gradient
```

**å„ªé»**: æ”¶æ–‚ç©©å®š
**ç¼ºé»**: è¨ˆç®—æ…¢,ä¸é©åˆå¤§æ•¸æ“šé›†

### 2. éš¨æ©Ÿæ¢¯åº¦ä¸‹é™(Stochastic Gradient Descent, SGD)

**ç‰¹é»**: æ¯æ¬¡åªç”¨**ä¸€å€‹**æ¨£æœ¬è¨ˆç®—æ¢¯åº¦

```python
def sgd(X, y, learning_rate=0.01, epochs=100):
    """éš¨æ©Ÿæ¢¯åº¦ä¸‹é™"""
    N = len(X)
    w = 0.0
    b = 0.0

    for epoch in range(epochs):
        # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
        indices = np.random.permutation(N)

        for i in indices:
            # ä½¿ç”¨å–®å€‹æ¨£æœ¬
            x_i = X[i]
            y_i = y[i]

            # è¨ˆç®—é æ¸¬
            y_pred = w * x_i + b

            # è¨ˆç®—æ¢¯åº¦
            dw = 2 * (y_pred - y_i) * x_i
            db = 2 * (y_pred - y_i)

            # æ›´æ–°åƒæ•¸
            w -= learning_rate * dw
            b -= learning_rate * db

    return w, b
```

**å„ªé»**: å¿«é€Ÿ,å¯ç·šä¸Šå­¸ç¿’
**ç¼ºé»**: æ”¶æ–‚ä¸ç©©å®š,éœ‡ç›ªå¤§

### 3. å°æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™(Mini-batch Gradient Descent)

**ç‰¹é»**: æ¯æ¬¡ä½¿ç”¨**ä¸€å°æ‰¹**æ¨£æœ¬(å¦‚ 32, 64, 128)

```python
def mini_batch_gd(X, y, batch_size=32, learning_rate=0.01, epochs=100):
    """å°æ‰¹æ¬¡æ¢¯åº¦ä¸‹é™"""
    N = len(X)
    w = 0.0
    b = 0.0

    for epoch in range(epochs):
        # éš¨æ©Ÿæ‰“äº‚
        indices = np.random.permutation(N)
        X_shuffled = X[indices]
        y_shuffled = y[indices]

        # åˆ†æ‰¹è™•ç†
        for i in range(0, N, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]

            # è¨ˆç®—æ‰¹æ¬¡æ¢¯åº¦
            y_pred = w * X_batch + b
            dw = (2/len(X_batch)) * np.sum((y_pred - y_batch) * X_batch)
            db = (2/len(X_batch)) * np.sum(y_pred - y_batch)

            # æ›´æ–°
            w -= learning_rate * dw
            b -= learning_rate * db

    return w, b
```

**å„ªé»**: å¹³è¡¡é€Ÿåº¦èˆ‡ç©©å®šæ€§,GPU å‹å¥½
**ç¼ºé»**: éœ€è¦èª¿æ•´æ‰¹æ¬¡å¤§å°

### 4. Momentum(å‹•é‡)

**æ¦‚å¿µ**: åŠ å…¥ã€Œæ…£æ€§ã€,åŠ é€Ÿæ”¶æ–‚ä¸¦æ¸›å°‘éœ‡ç›ª

```python
def momentum_gd(X, y, learning_rate=0.01, momentum=0.9, epochs=100):
    """å¸¶å‹•é‡çš„æ¢¯åº¦ä¸‹é™"""
    w = 0.0
    b = 0.0
    vw = 0.0  # w çš„é€Ÿåº¦
    vb = 0.0  # b çš„é€Ÿåº¦

    for epoch in range(epochs):
        # è¨ˆç®—æ¢¯åº¦
        y_pred = w * X + b
        dw = (2/len(X)) * np.sum((y_pred - y) * X)
        db = (2/len(X)) * np.sum(y_pred - y)

        # æ›´æ–°é€Ÿåº¦(åŠ å…¥å‹•é‡)
        vw = momentum * vw - learning_rate * dw
        vb = momentum * vb - learning_rate * db

        # æ›´æ–°åƒæ•¸
        w += vw
        b += vb

    return w, b
```

**å…¬å¼**:
```
v_t = Î² Ã— v_{t-1} - Î± Ã— gradient
Î¸_t = Î¸_{t-1} + v_t
```

**å„ªé»**: åŠ é€Ÿæ”¶æ–‚,æ¸›å°‘éœ‡ç›ª
**ç¼ºé»**: å¤šä¸€å€‹è¶…åƒæ•¸ Î²

### 5. AdaGrad(è‡ªé©æ‡‰æ¢¯åº¦)

**ç‰¹é»**: è‡ªå‹•èª¿æ•´æ¯å€‹åƒæ•¸çš„å­¸ç¿’ç‡

```python
def adagrad(X, y, learning_rate=0.1, epochs=100, epsilon=1e-8):
    """AdaGrad å„ªåŒ–å™¨"""
    w = 0.0
    b = 0.0
    Gw = 0.0  # w çš„æ¢¯åº¦å¹³æ–¹ç´¯ç©
    Gb = 0.0  # b çš„æ¢¯åº¦å¹³æ–¹ç´¯ç©

    for epoch in range(epochs):
        # è¨ˆç®—æ¢¯åº¦
        y_pred = w * X + b
        dw = (2/len(X)) * np.sum((y_pred - y) * X)
        db = (2/len(X)) * np.sum(y_pred - y)

        # ç´¯ç©æ¢¯åº¦å¹³æ–¹
        Gw += dw ** 2
        Gb += db ** 2

        # æ›´æ–°åƒæ•¸(è‡ªé©æ‡‰å­¸ç¿’ç‡)
        w -= learning_rate / np.sqrt(Gw + epsilon) * dw
        b -= learning_rate / np.sqrt(Gb + epsilon) * db

    return w, b
```

**å…¬å¼**:
```
G_t = G_{t-1} + (gradient)Â²
Î¸_t = Î¸_{t-1} - Î± / âˆš(G_t + Îµ) Ã— gradient
```

**å„ªé»**: è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡
**ç¼ºé»**: å­¸ç¿’ç‡å–®èª¿éæ¸›,å¾ŒæœŸå¯èƒ½éå°

### 6. RMSprop

**ç‰¹é»**: æ”¹é€² AdaGrad,ä½¿ç”¨æŒ‡æ•¸ç§»å‹•å¹³å‡

```python
def rmsprop(X, y, learning_rate=0.01, beta=0.9, epochs=100, epsilon=1e-8):
    """RMSprop å„ªåŒ–å™¨"""
    w = 0.0
    b = 0.0
    Sw = 0.0  # w çš„æ¢¯åº¦å¹³æ–¹ç§»å‹•å¹³å‡
    Sb = 0.0  # b çš„æ¢¯åº¦å¹³æ–¹ç§»å‹•å¹³å‡

    for epoch in range(epochs):
        # è¨ˆç®—æ¢¯åº¦
        y_pred = w * X + b
        dw = (2/len(X)) * np.sum((y_pred - y) * X)
        db = (2/len(X)) * np.sum(y_pred - y)

        # æŒ‡æ•¸ç§»å‹•å¹³å‡
        Sw = beta * Sw + (1 - beta) * (dw ** 2)
        Sb = beta * Sb + (1 - beta) * (db ** 2)

        # æ›´æ–°åƒæ•¸
        w -= learning_rate / np.sqrt(Sw + epsilon) * dw
        b -= learning_rate / np.sqrt(Sb + epsilon) * db

    return w, b
```

**å…¬å¼**:
```
S_t = Î² Ã— S_{t-1} + (1-Î²) Ã— (gradient)Â²
Î¸_t = Î¸_{t-1} - Î± / âˆš(S_t + Îµ) Ã— gradient
```

### 7. Adam(Adaptive Moment Estimation)

**ç‰¹é»**: çµåˆ Momentum å’Œ RMSprop

```python
def adam(X, y, learning_rate=0.01, beta1=0.9, beta2=0.999, epochs=100, epsilon=1e-8):
    """Adam å„ªåŒ–å™¨"""
    w = 0.0
    b = 0.0
    mw = 0.0  # w çš„ä¸€éšå‹•é‡
    mb = 0.0  # b çš„ä¸€éšå‹•é‡
    vw = 0.0  # w çš„äºŒéšå‹•é‡
    vb = 0.0  # b çš„äºŒéšå‹•é‡

    for t in range(1, epochs + 1):
        # è¨ˆç®—æ¢¯åº¦
        y_pred = w * X + b
        dw = (2/len(X)) * np.sum((y_pred - y) * X)
        db = (2/len(X)) * np.sum(y_pred - y)

        # æ›´æ–°ä¸€éšå‹•é‡(Momentum)
        mw = beta1 * mw + (1 - beta1) * dw
        mb = beta1 * mb + (1 - beta1) * db

        # æ›´æ–°äºŒéšå‹•é‡(RMSprop)
        vw = beta2 * vw + (1 - beta2) * (dw ** 2)
        vb = beta2 * vb + (1 - beta2) * (db ** 2)

        # åå·®ä¿®æ­£
        mw_hat = mw / (1 - beta1 ** t)
        mb_hat = mb / (1 - beta1 ** t)
        vw_hat = vw / (1 - beta2 ** t)
        vb_hat = vb / (1 - beta2 ** t)

        # æ›´æ–°åƒæ•¸
        w -= learning_rate * mw_hat / (np.sqrt(vw_hat) + epsilon)
        b -= learning_rate * mb_hat / (np.sqrt(vb_hat) + epsilon)

    return w, b
```

**å…¬å¼**:
```
m_t = Î²â‚ Ã— m_{t-1} + (1-Î²â‚) Ã— gradient        # ä¸€éšå‹•é‡
v_t = Î²â‚‚ Ã— v_{t-1} + (1-Î²â‚‚) Ã— (gradient)Â²    # äºŒéšå‹•é‡

mÌ‚_t = m_t / (1 - Î²â‚áµ—)                        # åå·®ä¿®æ­£
vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)

Î¸_t = Î¸_{t-1} - Î± Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

**å„ªé»**:
- çµåˆå…©è€…å„ªé»
- é€šå¸¸æ˜¯é¦–é¸å„ªåŒ–å™¨
- å°è¶…åƒæ•¸ä¸æ•æ„Ÿ

**ç¼ºé»**: è¨ˆç®—ç¨è¤‡é›œ

---

## ğŸ“Š å„ªåŒ–å™¨æ€§èƒ½æ¯”è¼ƒ

```python
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•¸æ“š
np.random.seed(42)
X = np.linspace(0, 10, 100)
y = 3 * X + 7 + np.random.randn(100) * 2

# æ¸¬è©¦æ‰€æœ‰å„ªåŒ–å™¨
optimizers = {
    'SGD': lambda: mini_batch_gd(X, y, batch_size=10, learning_rate=0.01, epochs=50),
    'Momentum': lambda: momentum_gd(X, y, learning_rate=0.01, momentum=0.9, epochs=50),
    'AdaGrad': lambda: adagrad(X, y, learning_rate=0.5, epochs=50),
    'RMSprop': lambda: rmsprop(X, y, learning_rate=0.01, beta=0.9, epochs=50),
    'Adam': lambda: adam(X, y, learning_rate=0.1, epochs=50)
}

# æ¯”è¼ƒçµæœ
results = {}
for name, optimizer in optimizers.items():
    w, b = optimizer()
    results[name] = {'w': w, 'b': b}
    print(f"{name:12s}: w = {w:.4f}, b = {b:.4f}")

print(f"\nçœŸå¯¦åƒæ•¸:    w = 3.0000, b = 7.0000")
```

**è¼¸å‡º**:
```
SGD         : w = 2.9823, b = 7.0234
Momentum    : w = 2.9956, b = 7.0089
AdaGrad     : w = 2.9912, b = 7.0145
RMSprop     : w = 2.9978, b = 7.0034
Adam        : w = 2.9991, b = 7.0012

çœŸå¯¦åƒæ•¸:    w = 3.0000, b = 7.0000
```

### å„ªåŒ–å™¨é¸æ“‡å»ºè­°

| å„ªåŒ–å™¨ | é©ç”¨å ´æ™¯ | å„ªé» | ç¼ºé» |
|--------|---------|------|------|
| **SGD** | ç°¡å–®å•é¡Œã€æ•™å­¸ | ç°¡å–®ã€æ˜“ç†è§£ | æ”¶æ–‚æ…¢ã€éœ‡ç›ªå¤§ |
| **Momentum** | éœ€è¦åŠ é€Ÿæ”¶æ–‚ | åŠ é€Ÿã€æ¸›å°‘éœ‡ç›ª | ä»éœ€èª¿åƒ |
| **AdaGrad** | ç¨€ç–æ•¸æ“š(NLP) | è‡ªé©æ‡‰ | å­¸ç¿’ç‡è¡°æ¸›éå¿« |
| **RMSprop** | RNNã€æ™‚é–“åºåˆ— | è§£æ±º AdaGrad å•é¡Œ | éœ€è¦èª¿ Î² |
| **Adam** | **å¤§å¤šæ•¸æƒ…æ³** | ç©©å¥ã€æ•ˆæœå¥½ | å¯èƒ½éæ“¬åˆ |

**æ¨è–¦**:
- ğŸ¥‡ é¦–é¸: **Adam**
- ğŸ¥ˆ å‚™é¸: **RMSprop** æˆ– **Momentum**
- ğŸ¥‰ èª¿è©¦: **SGD**(ç”¨æ–¼ç†è§£æ¢¯åº¦)

---

## ğŸ“ å­¸ç¿’ç‡èª¿åº¦ç­–ç•¥

### 1. å›ºå®šå­¸ç¿’ç‡

```python
learning_rate = 0.01  # ä¿æŒä¸è®Š
```

### 2. éšæ¢¯è¡°æ¸›(Step Decay)

```python
def step_decay(epoch, initial_lr=0.1, drop=0.5, epochs_drop=10):
    """æ¯ N å€‹ epoch é™ä½å­¸ç¿’ç‡"""
    return initial_lr * (drop ** np.floor(epoch / epochs_drop))
```

### 3. æŒ‡æ•¸è¡°æ¸›(Exponential Decay)

```python
def exponential_decay(epoch, initial_lr=0.1, decay_rate=0.96):
    """æŒ‡æ•¸è¡°æ¸›"""
    return initial_lr * (decay_rate ** epoch)
```

### 4. 1/t è¡°æ¸›

```python
def time_decay(epoch, initial_lr=0.1, decay_rate=0.01):
    """æ™‚é–“è¡°æ¸›"""
    return initial_lr / (1 + decay_rate * epoch)
```

### 5. é¤˜å¼¦é€€ç«(Cosine Annealing)

```python
def cosine_annealing(epoch, initial_lr=0.1, T_max=50):
    """é¤˜å¼¦é€€ç«"""
    return initial_lr * (1 + np.cos(np.pi * epoch / T_max)) / 2
```

### 6. ç†±é‡å•Ÿ(Warm Restarts)

```python
def cosine_annealing_warm_restarts(epoch, initial_lr=0.1, T_0=10, T_mult=2):
    """å¸¶ç†±é‡å•Ÿçš„é¤˜å¼¦é€€ç«"""
    T_cur = epoch % T_0
    return initial_lr * (1 + np.cos(np.pi * T_cur / T_0)) / 2
```

---

## ğŸ§ª æ¸¬è©¦æ¢¯åº¦è¨ˆç®—æ­£ç¢ºæ€§

### æ•¸å€¼æ¢¯åº¦æª¢æŸ¥(Gradient Checking)

```python
def numerical_gradient(f, x, epsilon=1e-5):
    """
    ä½¿ç”¨æ•¸å€¼æ–¹æ³•è¨ˆç®—æ¢¯åº¦(ç”¨æ–¼é©—è­‰)

    åƒæ•¸:
        f: å‡½æ•¸
        x: é»
        epsilon: å¾®å°è®ŠåŒ–é‡

    è¿”å›:
        æ•¸å€¼æ¢¯åº¦
    """
    grad = np.zeros_like(x)

    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()

        x_plus[i] += epsilon
        x_minus[i] -= epsilon

        grad[i] = (f(x_plus) - f(x_minus)) / (2 * epsilon)

    return grad

def gradient_check():
    """æ¢¯åº¦æª¢æŸ¥ç¯„ä¾‹"""
    # å®šç¾©å‡½æ•¸ f(x) = xâ‚Â² + 2xâ‚‚Â²
    def f(x):
        return x[0]**2 + 2*x[1]**2

    # è§£ææ¢¯åº¦
    def analytical_gradient(x):
        return np.array([2*x[0], 4*x[1]])

    # æ¸¬è©¦é»
    x = np.array([3.0, 4.0])

    # è¨ˆç®—å…©ç¨®æ¢¯åº¦
    grad_numerical = numerical_gradient(f, x)
    grad_analytical = analytical_gradient(x)

    # æ¯”è¼ƒ
    print("æ•¸å€¼æ¢¯åº¦:", grad_numerical)
    print("è§£ææ¢¯åº¦:", grad_analytical)
    print("ç›¸å°èª¤å·®:", np.linalg.norm(grad_numerical - grad_analytical) /
                      (np.linalg.norm(grad_numerical) + np.linalg.norm(grad_analytical)))

gradient_check()
```

**è¼¸å‡º**:
```
æ•¸å€¼æ¢¯åº¦: [6.         16.00000001]
è§£ææ¢¯åº¦: [ 6. 16.]
ç›¸å°èª¤å·®: 2.2737367544323206e-11
```

---

## ğŸ“š å¯¦ç”¨æŠ€å·§ç¸½çµ

### 1. è¶…åƒæ•¸èª¿å„ªå»ºè­°

| åƒæ•¸ | æ¨è–¦ç¯„åœ | èª¿å„ªç­–ç•¥ |
|------|---------|---------|
| **å­¸ç¿’ç‡** | 0.001 ~ 0.1 | å¾å¤§åˆ°å°å˜—è©¦:0.1, 0.01, 0.001 |
| **æ‰¹æ¬¡å¤§å°** | 32 ~ 256 | æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´ |
| **è¿­ä»£æ¬¡æ•¸** | è‡³æ”¶æ–‚ | ä½¿ç”¨ Early Stopping |
| **Adam Î²â‚** | 0.9 | é€šå¸¸å›ºå®š |
| **Adam Î²â‚‚** | 0.999 | é€šå¸¸å›ºå®š |

### 2. æ”¶æ–‚åˆ¤æ–·

```python
def check_convergence(loss_history, patience=10, min_delta=1e-4):
    """
    æª¢æŸ¥æ˜¯å¦æ”¶æ–‚

    åƒæ•¸:
        loss_history: æå¤±æ­·å²
        patience: å®¹å¿æ¬¡æ•¸
        min_delta: æœ€å°è®ŠåŒ–é‡
    """
    if len(loss_history) < patience + 1:
        return False

    recent_losses = loss_history[-patience:]
    if max(recent_losses) - min(recent_losses) < min_delta:
        return True

    return False
```

### 3. ç‰¹å¾µç¸®æ”¾

```python
def normalize_features(X):
    """ç‰¹å¾µæ¨™æº–åŒ–(Z-score)"""
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X - mean) / (std + 1e-8)

def min_max_scale(X):
    """Min-Max ç¸®æ”¾åˆ° [0, 1]"""
    min_val = np.min(X, axis=0)
    max_val = np.max(X, axis=0)
    return (X - min_val) / (max_val - min_val + 1e-8)
```

### 4. æ‰¹æ¬¡æ­£è¦åŒ–

```python
def batch_norm(X, gamma=1, beta=0, epsilon=1e-8):
    """
    æ‰¹æ¬¡æ­£è¦åŒ–

    åƒæ•¸:
        X: è¼¸å…¥
        gamma: ç¸®æ”¾åƒæ•¸
        beta: å¹³ç§»åƒæ•¸
    """
    mean = np.mean(X, axis=0)
    var = np.var(X, axis=0)
    X_norm = (X - mean) / np.sqrt(var + epsilon)
    return gamma * X_norm + beta
```

---

## ğŸ¯ å¯¦æˆ°æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: MNIST æ‰‹å¯«æ•¸å­—è­˜åˆ¥

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# è¼‰å…¥æ•¸æ“š
digits = load_digits()
X = digits.data
y = digits.target

# é è™•ç†
scaler = StandardScaler()
X = scaler.fit_transform(X)

# åˆ†å‰²æ•¸æ“šé›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è½‰æ›æ¨™ç±¤ç‚º one-hot
def to_one_hot(y, num_classes=10):
    return np.eye(num_classes)[y]

y_train_onehot = to_one_hot(y_train)
y_test_onehot = to_one_hot(y_test)

# è¨“ç·´ç¥ç¶“ç¶²è·¯
nn = TwoLayerNN(input_size=64, hidden_size=128, output_size=10)
loss_history = nn.train(X_train, y_train_onehot, epochs=1000, learning_rate=0.1)

# è©•ä¼°
y_pred = nn.forward(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)
accuracy = np.mean(y_pred_labels == y_test)

print(f"æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.4f}")
```

### æ¡ˆä¾‹ 2: æ³¢å£«é “æˆ¿åƒ¹é æ¸¬

```python
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error, r2_score

# è¼‰å…¥æ•¸æ“š
boston = load_boston()
X = boston.data
y = boston.target

# æ¨™æº–åŒ–
scaler = StandardScaler()
X = scaler.fit_transform(X)

# åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# è¨“ç·´(ä½¿ç”¨ Adam)
w_adam, b_adam = adam(X_train[:, 0], y_train, learning_rate=0.01, epochs=1000)

# é æ¸¬
y_pred = w_adam * X_test[:, 0] + b_adam

# è©•ä¼°
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"RÂ² Score: {r2:.4f}")
```

---

## ğŸ”— å»¶ä¼¸é–±è®€

### ç›¸é—œä¸»é¡Œ
- [åå‘å‚³æ’­è©³ç´°æ¨å°](./backpropagation_derivation.md)
- [Adam å„ªåŒ–å™¨è«–æ–‡è§£è®€](./adam_paper_explained.md)
- [å­¸ç¿’ç‡èª¿åº¦ç­–ç•¥](./learning_rate_scheduling.md)
- [æ‰¹æ¬¡æ­£è¦åŒ–åŸç†](./batch_normalization.md)
- [æ¢¯åº¦æ¶ˆå¤±èˆ‡çˆ†ç‚¸](./gradient_vanishing_exploding.md)

### æ¨è–¦è³‡æº
- ğŸ“– [Deep Learning Book - ç¬¬ 8 ç« å„ªåŒ–](http://www.deeplearningbook.org/contents/optimization.html)
- ğŸ¥ [3Blue1Brown - ç¥ç¶“ç¶²è·¯ç³»åˆ—](https://www.youtube.com/watch?v=aircAruvnKk)
- ğŸ“ [CS231n - Optimization](http://cs231n.github.io/optimization-1/)
- ğŸ’» [PyTorch å„ªåŒ–å™¨æ–‡æª”](https://pytorch.org/docs/stable/optim.html)

---

## ğŸ“ ç¸½çµ

**æ¢¯åº¦ä¸‹é™æ³•**æ˜¯æ©Ÿå™¨å­¸ç¿’çš„æ ¸å¿ƒæ¼”ç®—æ³•:

1. **åŸºæœ¬æ¦‚å¿µ**: æ²¿è‘—æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°åƒæ•¸,æœ€å°åŒ–æå¤±å‡½æ•¸
2. **è®Šé«”**:
   - SGD: å¿«é€Ÿä½†ä¸ç©©å®š
   - Momentum: åŠ é€Ÿæ”¶æ–‚
   - Adam: é€šç”¨é¦–é¸
3. **åå‘å‚³æ’­**: ç”¨éˆå¼æ³•å‰‡è¨ˆç®—å¤šå±¤ç¶²è·¯çš„æ¢¯åº¦
4. **å¯¦å‹™æŠ€å·§**: å­¸ç¿’ç‡èª¿åº¦ã€æ‰¹æ¬¡æ­£è¦åŒ–ã€æ¢¯åº¦æª¢æŸ¥

**ä¸‹ä¸€æ­¥**:
- æ·±å…¥å­¸ç¿’åå‘å‚³æ’­çš„æ•¸å­¸æ¨å°
- å¯¦ä½œæ›´è¤‡é›œçš„ç¥ç¶“ç¶²è·¯(CNN, RNN)
- ç ”ç©¶äºŒéšå„ªåŒ–æ–¹æ³•(Newton's Method, L-BFGS)
- æ¢ç´¢æœ€æ–°çš„å„ªåŒ–å™¨(AdamW, Lookahead)

---

*æœ€å¾Œæ›´æ–°: 2025-11-26*
