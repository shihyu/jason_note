# å¼·åŒ–å­¸ç¿’å®Œæ•´æŒ‡å— - ç”¨ç”Ÿæ´»æ¯”å–»ç†è§£

## ğŸ¯ æ ¸å¿ƒæ¯”å–»ï¼šè¨“ç·´å¯µç‰© vs è¨“ç·´ AI

### ç›£ç£å­¸ç¿’ vs å¼·åŒ–å­¸ç¿’

```
ç›£ç£å­¸ç¿’ï¼ˆSupervised Learningï¼‰ï¼š
è€å¸«å‘Šè¨´ä½ ã€Œæ­£ç¢ºç­”æ¡ˆã€

ä¾‹å­ï¼š
è€å¸«ï¼šã€Œé€™æ˜¯è²“ã€ï¼ˆçµ¦æ¨™ç±¤ï¼‰
å­¸ç”Ÿï¼šã€Œæˆ‘è¨˜ä½äº†ã€
è€å¸«ï¼šã€Œé€™æ˜¯ç‹—ã€
å­¸ç”Ÿï¼šã€Œæˆ‘è¨˜ä½äº†ã€

æ¯”å–»ï¼šè€ƒè©¦æœ‰ã€Œæ¨™æº–ç­”æ¡ˆã€
```

```
å¼·åŒ–å­¸ç¿’ï¼ˆReinforcement Learningï¼‰ï¼š
æ²’æœ‰æ¨™æº–ç­”æ¡ˆï¼Œåªæœ‰ã€Œçå‹µã€å’Œã€Œæ‡²ç½°ã€

ä¾‹å­ï¼š
è¨“ç·´ç‹—ç‹—ï¼š
ç‹—ç‹—åä¸‹ â†’ çµ¦é›¶é£Ÿï¼ˆçå‹µ +10ï¼‰âœ“
ç‹—ç‹—äº‚å« â†’ ä¸ç†å®ƒï¼ˆæ‡²ç½° -5ï¼‰âœ—
ç‹—ç‹—æ’¿çƒ â†’ æ‘¸é ­ç¨±è®šï¼ˆçå‹µ +20ï¼‰âœ“

ç‹—ç‹—å­¸æœƒï¼š
ã€Œåä¸‹ã€å’Œã€Œæ’¿çƒã€æœƒå¾—åˆ°çå‹µ
â†’ ä»¥å¾Œå¤šåšé€™äº›è¡Œç‚º

æ¯”å–»ï¼šéŠæˆ²æ‰“æ€ªå‡ç´š
- æ‰“è´æ€ªç‰© â†’ +100 ç¶“é©—å€¼
- è¢«æ€ªç‰©æ‰“ â†’ -50 è¡€é‡
- é€šé—œ â†’ +1000 çå‹µ
```

---

## ğŸ“– ç”Ÿæ´»åŒ–æ¡ˆä¾‹ï¼šå­¸é¨è…³è¸è»Š

### éç¨‹åˆ†æ

```
ç¬¬ 1 æ¬¡å˜—è©¦ï¼š
å‹•ä½œï¼šå…©æ‰‹æ”¾é–‹
çµæœï¼šç«‹åˆ»æ‘”å€’ï¼ˆç—›ï¼ï¼‰
çå‹µï¼š-100
å­¸ç¿’ï¼šã€Œå…©æ‰‹æ”¾é–‹ã€= å£ä¸»æ„

ç¬¬ 2 æ¬¡å˜—è©¦ï¼š
å‹•ä½œï¼šæ¡ç·ŠæŠŠæ‰‹ï¼Œæ…¢æ…¢é¨
çµæœï¼šé¨äº† 2 å…¬å°º
çå‹µï¼š+10
å­¸ç¿’ï¼šã€Œæ¡æŠŠæ‰‹ã€æ¯”ã€Œæ”¾é–‹ã€å¥½

ç¬¬ 10 æ¬¡å˜—è©¦ï¼š
å‹•ä½œï¼šä¿æŒå¹³è¡¡ï¼Œè¸©è¸æ¿
çµæœï¼šé¨äº† 50 å…¬å°º
çå‹µï¼š+100
å­¸ç¿’ï¼šã€Œä¿æŒå¹³è¡¡ã€å¾ˆé‡è¦

ç¬¬ 100 æ¬¡å˜—è©¦ï¼š
å‹•ä½œï¼šç†Ÿç·´æ§åˆ¶
çµæœï¼šé¨äº† 10 å…¬é‡Œï¼
çå‹µï¼š+1000
å­¸ç¿’ï¼šã€Œå·²ç¶“å­¸æœƒäº†ã€
```

### æ ¸å¿ƒæ¦‚å¿µ

1. **Agentï¼ˆæ™ºèƒ½é«”ï¼‰**ï¼šä½ ï¼ˆé¨è»Šçš„äººï¼‰
2. **Environmentï¼ˆç’°å¢ƒï¼‰**ï¼šè…³è¸è»Š + é“è·¯
3. **Stateï¼ˆç‹€æ…‹ï¼‰**ï¼šç•¶å‰è»Šé€Ÿã€å¹³è¡¡ç‹€æ…‹
4. **Actionï¼ˆå‹•ä½œï¼‰**ï¼šè½‰æ–¹å‘ç›¤ã€è¸©è¸æ¿
5. **Rewardï¼ˆçå‹µï¼‰**ï¼šé¨å¾—é  = æ­£çå‹µï¼Œæ‘”å€’ = è² çå‹µ
6. **Policyï¼ˆç­–ç•¥ï¼‰**ï¼šåœ¨ä»€éº¼ç‹€æ…‹åšä»€éº¼å‹•ä½œ

---

## ğŸ® RL çš„ç¶“å…¸å•é¡Œï¼šMulti-Armed Bandit

### æ¯”å–»ï¼šåƒè§’å­è€è™æ©Ÿ

```
æƒ…å¢ƒï¼šè³­å ´æœ‰ 10 å°è€è™æ©Ÿ

æ©Ÿå™¨ 1ï¼šå¹³å‡æ¯æ¬¡è´ $5
æ©Ÿå™¨ 2ï¼šå¹³å‡æ¯æ¬¡è´ $3
æ©Ÿå™¨ 3ï¼šå¹³å‡æ¯æ¬¡è´ $10ï¼ˆæœ€å¥½ï¼Œä½†ä½ ä¸çŸ¥é“ï¼‰
...
æ©Ÿå™¨ 10ï¼šå¹³å‡æ¯æ¬¡è´ $1

å•é¡Œï¼š
ä½ æœ‰ 100 æ¬¡æ©Ÿæœƒ
å¦‚ä½•æœ€å¤§åŒ–ç¸½çé‡‘ï¼Ÿ

ç­–ç•¥ 1ï¼šæ¢ç´¢ï¼ˆExplorationï¼‰
ã€Œæ¯å°éƒ½è©¦è©¦ï¼Œæ‰¾å‡ºæœ€å¥½çš„ã€
â†’ èŠ±å¾ˆå¤šæ¬¡å˜—è©¦ã€Œçˆ›æ©Ÿå™¨ã€

ç­–ç•¥ 2ï¼šåˆ©ç”¨ï¼ˆExploitationï¼‰
ã€Œä¸€ç›´ç”¨ç›®å‰æœ€å¥½çš„æ©Ÿå™¨ã€
â†’ å¯èƒ½éŒ¯éã€ŒçœŸæ­£æœ€å¥½ã€çš„æ©Ÿå™¨

æœ€ä½³ç­–ç•¥ï¼šæ¢ç´¢ + åˆ©ç”¨å¹³è¡¡
å‰ 20 æ¬¡ï¼šæ¢ç´¢ï¼ˆæ¯å°éƒ½è©¦ï¼‰
å¾Œ 80 æ¬¡ï¼šåˆ©ç”¨ï¼ˆå°ˆç”¨æœ€å¥½çš„ï¼‰
```

### Epsilon-Greedy ç­–ç•¥

```python
def epsilon_greedy(Q, epsilon=0.1):
    """
    Epsilon-Greedy ç­–ç•¥

    æ¯”å–»ï¼š
    - 90% æ™‚é–“ï¼šé¸æœ€å¥½çš„ï¼ˆåˆ©ç”¨ï¼‰
    - 10% æ™‚é–“ï¼šéš¨æ©Ÿè©¦è©¦ï¼ˆæ¢ç´¢ï¼‰

    åƒæ•¸ï¼š
        Q: æ¯å€‹å‹•ä½œçš„ä¼°è¨ˆåƒ¹å€¼
        epsilon: æ¢ç´¢æ©Ÿç‡
    """
    if np.random.rand() < epsilon:
        # æ¢ç´¢ï¼šéš¨æ©Ÿé¸æ“‡
        return np.random.randint(len(Q))
    else:
        # åˆ©ç”¨ï¼šé¸æœ€å¥½çš„
        return np.argmax(Q)

# ä½¿ç”¨ç¯„ä¾‹
Q = [5, 3, 10, 7, 1]  # æ¯å°æ©Ÿå™¨çš„ä¼°è¨ˆåƒ¹å€¼
action = epsilon_greedy(Q, epsilon=0.1)
```

---

## ğŸ—ï¸ é¦¬å¯å¤«æ±ºç­–éç¨‹ï¼ˆMDPï¼‰

### æ­£å¼å®šç¾©

```
MDP = (S, A, P, R, Î³)

Sï¼ˆStatesï¼‰ï¼šæ‰€æœ‰å¯èƒ½ç‹€æ…‹
Aï¼ˆActionsï¼‰ï¼šæ‰€æœ‰å¯èƒ½å‹•ä½œ
Pï¼ˆTransitionï¼‰ï¼šè½‰ç§»æ©Ÿç‡ï¼ˆåšå‹•ä½œå¾Œåˆ°å“ªå€‹ç‹€æ…‹ï¼‰
Rï¼ˆRewardï¼‰ï¼šçå‹µå‡½æ•¸
Î³ï¼ˆGammaï¼‰ï¼šæŠ˜æ‰£å› å­ï¼ˆæœªä¾†çå‹µçš„é‡è¦æ€§ï¼‰
```

### ç”Ÿæ´»åŒ–ä¾‹å­ï¼šä¸Šå­¸è·¯ç·šé¸æ“‡

```
ç‹€æ…‹ï¼ˆSï¼‰ï¼š
- s1ï¼šå®¶
- s2ï¼šå…¬è»Šç«™
- s3ï¼šæ·é‹ç«™
- s4ï¼šå­¸æ ¡

å‹•ä½œï¼ˆAï¼‰ï¼š
- èµ°è·¯
- æ­å…¬è»Š
- æ­æ·é‹

è½‰ç§»æ©Ÿç‡ï¼ˆPï¼‰ï¼š
å¾å®¶èµ°è·¯åˆ°å…¬è»Šç«™ï¼š100%
å¾å®¶æ­å…¬è»Šåˆ°å­¸æ ¡ï¼š80%ï¼ˆå¯èƒ½å¡è»Šï¼‰
å¾å…¬è»Šç«™æ­æ·é‹åˆ°å­¸æ ¡ï¼š95%

çå‹µï¼ˆRï¼‰ï¼š
åˆ°é”å­¸æ ¡ï¼š+100
é²åˆ°ï¼š-50
èŠ±éŒ¢æ­è»Šï¼š-5

ç›®æ¨™ï¼š
æ‰¾å‡ºã€Œæœ€å¤§åŒ–ç¸½çå‹µã€çš„ç­–ç•¥
ï¼ˆæº–æ™‚åˆ°ã€èŠ±éŒ¢å°‘ï¼‰
```

### æŠ˜æ‰£å› å­ï¼ˆGammaï¼‰

**å•é¡Œ**ï¼šæœªä¾†çå‹µå€¼å¤šå°‘ï¼Ÿ

```
æƒ…å¢ƒï¼šä»Šå¤©è³º $100 vs æ˜å¤©è³º $100

ç¾å¯¦ï¼šä»Šå¤©çš„éŒ¢æ›´å€¼éŒ¢
â†’ ç”¨ã€ŒæŠ˜æ‰£å› å­ã€Î³

Î³ = 0.9ï¼ˆå¸¸ç”¨å€¼ï¼‰

ä»Šå¤©ï¼š$100 Ã— 1.0 = $100
æ˜å¤©ï¼š$100 Ã— 0.9 = $90
å¾Œå¤©ï¼š$100 Ã— 0.9Â² = $81
...

æ¯”å–»ï¼š
ä»Šå¤©çš„é³¥ > æ˜å¤©çš„é³¥
å³æ™‚çå‹µ > æœªä¾†çå‹µ
```

---

## ğŸ§® æ ¸å¿ƒç®—æ³• 1ï¼šQ-Learning

### æ¦‚å¿µï¼šå­¸ç¿’ã€Œå‹•ä½œåƒ¹å€¼ã€

```
Q(s, a)ï¼šåœ¨ç‹€æ…‹ s åšå‹•ä½œ a çš„ã€Œé•·æœŸåƒ¹å€¼ã€

æ¯”å–»ï¼š
Q(åœ¨å®¶, èµ°è·¯) = 50ï¼ˆç´¯ä¸æ­»ï¼Œä½†æ…¢ï¼‰
Q(åœ¨å®¶, æ­å…¬è»Š) = 80ï¼ˆå¿«ï¼Œä½†å¯èƒ½å¡è»Šï¼‰
Q(åœ¨å®¶, æ­æ·é‹) = 95ï¼ˆæœ€å¥½ï¼ï¼‰

ç›®æ¨™ï¼š
å­¸ç¿’æ‰€æœ‰ Q(s, a)
ç„¶å¾Œæ¯æ¬¡é¸ã€ŒQ å€¼æœ€å¤§ã€çš„å‹•ä½œ
```

### Q-Learning æ›´æ–°è¦å‰‡

```
Q(s, a) â† Q(s, a) + Î± Ã— [r + Î³ Ã— max Q(s', a') - Q(s, a)]
                         â†‘                  â†‘
                      å³æ™‚çå‹µ           æœªä¾†æœ€å¤§åƒ¹å€¼

æ¯”å–»ï¼š
ä½ åŸæœ¬ä»¥ç‚ºã€Œæ­å…¬è»Šã€åƒ¹å€¼ 80
çµæœé€™æ¬¡ï¼š
- å¾—åˆ°çå‹µï¼š+100ï¼ˆæº–æ™‚åˆ°å­¸æ ¡ï¼‰
- åˆ°é”æ–°ç‹€æ…‹ï¼šå­¸æ ¡
- å­¸æ ¡èƒ½åšçš„å‹•ä½œæœ€å¤§åƒ¹å€¼ï¼š0ï¼ˆçµ‚é»ï¼‰

æ›´æ–°ï¼š
Q(åœ¨å®¶, æ­å…¬è»Š) = 80 + 0.1 Ã— [100 + 0 - 80]
                = 80 + 2
                = 82ï¼ˆåƒ¹å€¼æå‡äº†ï¼ï¼‰
```

### å®Œæ•´å¯¦ä½œï¼šèµ°è¿·å®®

```python
import numpy as np

class QLearningMaze:
    def __init__(self, maze, start, goal):
        """
        Q-Learning è§£è¿·å®®

        æ¯”å–»ï¼š
        è®“ AI å­¸æœƒèµ°å‡ºè¿·å®®

        åƒæ•¸ï¼š
            maze: è¿·å®®åœ°åœ–ï¼ˆ0=è·¯ï¼Œ1=ç‰†ï¼‰
            start: èµ·é»åº§æ¨™
            goal: çµ‚é»åº§æ¨™
        """
        self.maze = maze
        self.start = start
        self.goal = goal

        # Q è¡¨ï¼šQ[state][action] = åƒ¹å€¼
        self.Q = {}

        # å‹•ä½œï¼šä¸Šã€ä¸‹ã€å·¦ã€å³
        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

        # è¶…åƒæ•¸
        self.alpha = 0.1    # å­¸ç¿’ç‡
        self.gamma = 0.9    # æŠ˜æ‰£å› å­
        self.epsilon = 0.1  # æ¢ç´¢ç‡

    def get_q_value(self, state, action):
        """ç²å– Q å€¼"""
        if (state, action) not in self.Q:
            self.Q[(state, action)] = 0.0
        return self.Q[(state, action)]

    def choose_action(self, state):
        """
        é¸æ“‡å‹•ä½œï¼ˆEpsilon-Greedyï¼‰

        æ¯”å–»ï¼š
        90% æ™‚é–“ï¼šèµ°ã€Œçœ‹èµ·ä¾†æœ€å¥½ã€çš„è·¯
        10% æ™‚é–“ï¼šéš¨æ©Ÿæ¢ç´¢ï¼ˆå¯èƒ½ç™¼ç¾æ–°è·¯ï¼‰
        """
        if np.random.rand() < self.epsilon:
            # æ¢ç´¢ï¼šéš¨æ©Ÿ
            return self.actions[np.random.randint(len(self.actions))]
        else:
            # åˆ©ç”¨ï¼šé¸ Q å€¼æœ€å¤§çš„
            q_values = [self.get_q_value(state, a) for a in self.actions]
            max_q = max(q_values)
            # å¦‚æœæœ‰å¤šå€‹å‹•ä½œ Q å€¼ç›¸åŒï¼Œéš¨æ©Ÿé¸ä¸€å€‹
            best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]
            return best_actions[np.random.randint(len(best_actions))]

    def is_valid(self, pos):
        """æª¢æŸ¥ä½ç½®æ˜¯å¦åˆæ³•"""
        x, y = pos
        if x < 0 or x >= len(self.maze) or y < 0 or y >= len(self.maze[0]):
            return False
        if self.maze[x][y] == 1:  # ç‰†
            return False
        return True

    def get_reward(self, pos):
        """
        ç²å–çå‹µ

        æ¯”å–»ï¼š
        - åˆ°é”çµ‚é»ï¼šå¤§çå‹µ
        - æ’ç‰†ï¼šæ‡²ç½°
        - æ­£å¸¸ç§»å‹•ï¼šå°æ‡²ç½°ï¼ˆé¼“å‹µå¿«é»åˆ°ï¼‰
        """
        if pos == self.goal:
            return 100  # åˆ°é”çµ‚é»ï¼
        elif not self.is_valid(pos):
            return -10  # æ’ç‰†ï¼
        else:
            return -1   # æ¯æ­¥éƒ½æœ‰å°æˆæœ¬ï¼ˆé¼“å‹µå¿«é»åˆ°ï¼‰

    def train(self, episodes=1000):
        """
        è¨“ç·´ Q-Learning

        æ¯”å–»ï¼š
        è®“ AI èµ° 1000 æ¬¡è¿·å®®ï¼Œæ¯æ¬¡éƒ½å­¸ç¿’
        """
        for episode in range(episodes):
            # å¾èµ·é»é–‹å§‹
            state = self.start
            total_reward = 0

            for step in range(100):  # æœ€å¤šèµ° 100 æ­¥
                # 1. é¸æ“‡å‹•ä½œ
                action = self.choose_action(state)

                # 2. åŸ·è¡Œå‹•ä½œ
                next_x = state[0] + action[0]
                next_y = state[1] + action[1]
                next_state = (next_x, next_y)

                # 3. ç²å–çå‹µ
                reward = self.get_reward(next_state)
                total_reward += reward

                # 4. æ›´æ–° Q å€¼
                if self.is_valid(next_state):
                    # è¨ˆç®—ä¸‹ä¸€æ­¥çš„æœ€å¤§ Q å€¼
                    next_q_values = [self.get_q_value(next_state, a) for a in self.actions]
                    max_next_q = max(next_q_values)

                    # Q-Learning æ›´æ–°å…¬å¼
                    current_q = self.get_q_value(state, action)
                    new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)
                    self.Q[(state, action)] = new_q

                    # ç§»å‹•åˆ°ä¸‹ä¸€å€‹ç‹€æ…‹
                    state = next_state

                # 5. åˆ°é”çµ‚é»ï¼ŒçµæŸé€™ä¸€è¼ª
                if state == self.goal:
                    break

            # æ¯ 100 è¼ªå°å‡ºé€²åº¦
            if episode % 100 == 0:
                print(f"Episode {episode}: Total Reward = {total_reward}")

    def show_policy(self):
        """
        é¡¯ç¤ºå­¸åˆ°çš„ç­–ç•¥

        æ¯”å–»ï¼š
        é¡¯ç¤ºã€Œåœ¨æ¯å€‹ä½ç½®æ‡‰è©²å¾€å“ªèµ°ã€
        """
        print("\nå­¸åˆ°çš„ç­–ç•¥ï¼ˆâ†‘â†“â†â†’ï¼‰ï¼š")
        for i in range(len(self.maze)):
            for j in range(len(self.maze[0])):
                if self.maze[i][j] == 1:
                    print("â–ˆ", end=" ")  # ç‰†
                elif (i, j) == self.goal:
                    print("G", end=" ")  # çµ‚é»
                else:
                    # æ‰¾å‡ºæœ€ä½³å‹•ä½œ
                    state = (i, j)
                    q_values = [self.get_q_value(state, a) for a in self.actions]
                    best_action_idx = np.argmax(q_values)
                    arrows = ["â†‘", "â†“", "â†", "â†’"]
                    print(arrows[best_action_idx], end=" ")
            print()


# æ¸¬è©¦ï¼šèµ°è¿·å®®
if __name__ == "__main__":
    # å®šç¾©è¿·å®®ï¼ˆ0=è·¯ï¼Œ1=ç‰†ï¼‰
    maze = np.array([
        [0, 0, 0, 0, 1],
        [0, 1, 0, 0, 1],
        [0, 1, 0, 0, 0],
        [0, 0, 0, 1, 0],
        [1, 0, 0, 0, 0]
    ])

    start = (0, 0)  # å·¦ä¸Šè§’
    goal = (4, 4)   # å³ä¸‹è§’

    # å‰µå»º Q-Learning æ™ºèƒ½é«”
    agent = QLearningMaze(maze, start, goal)

    print("é–‹å§‹è¨“ç·´...")
    agent.train(episodes=1000)

    # é¡¯ç¤ºå­¸åˆ°çš„ç­–ç•¥
    agent.show_policy()
```

**è¼¸å‡ºç¯„ä¾‹**ï¼š
```
é–‹å§‹è¨“ç·´...
Episode 0: Total Reward = -105
Episode 100: Total Reward = 85
Episode 200: Total Reward = 92
...
Episode 900: Total Reward = 96

å­¸åˆ°çš„ç­–ç•¥ï¼ˆâ†‘â†“â†â†’ï¼‰ï¼š
â†’ â†’ â†“ â†“ â–ˆ
â†“ â–ˆ â†“ â† â–ˆ
â†“ â–ˆ â†“ â† â†
â†’ â†’ â†“ â–ˆ â†
â–ˆ â†’ â†’ â†’ G
```

---

## ğŸ§  æ ¸å¿ƒç®—æ³• 2ï¼šDeep Q-Network (DQN)

### å•é¡Œï¼šQ è¡¨å¤ªå¤§

```
Q-Learning çš„å•é¡Œï¼š
éœ€è¦å­˜å„² Q(s, a) å°æ‰€æœ‰ (s, a)

ç°¡å–®ä¾‹å­ï¼ˆè¿·å®®ï¼‰ï¼š
10Ã—10 è¿·å®® Ã— 4 å€‹å‹•ä½œ = 400 å€‹å€¼
â†’ å¯ä»¥ç”¨è¡¨æ ¼

è¤‡é›œä¾‹å­ï¼ˆAtari éŠæˆ²ï¼‰ï¼š
210Ã—160Ã—3 åƒç´  Ã— 18 å€‹å‹•ä½œ = 20,217,600 å€‹å€¼
â†’ è¡¨æ ¼å¤ªå¤§ï¼Œå­˜ä¸ä¸‹ï¼
```

### è§£æ±ºï¼šç”¨ç¥ç¶“ç¶²è·¯è¿‘ä¼¼ Q å‡½æ•¸

```
Q-Learningï¼š
ç”¨ã€Œè¡¨æ ¼ã€å­˜ Q(s, a)

DQNï¼š
ç”¨ã€Œç¥ç¶“ç¶²è·¯ã€è¿‘ä¼¼ Q(s, a)

è¼¸å…¥ï¼šç‹€æ…‹ sï¼ˆå¦‚éŠæˆ²ç•«é¢ï¼‰
è¼¸å‡ºï¼šæ‰€æœ‰å‹•ä½œçš„ Q å€¼ [Q(s, a1), Q(s, a2), ...]

æ¯”å–»ï¼š
Q-Learning = æ­»è¨˜ç¡¬èƒŒï¼ˆè¨˜ä½æ¯å€‹æƒ…æ³ï¼‰
DQN = ç†è§£è¦å¾‹ï¼ˆå­¸æœƒæ³›åŒ–ï¼‰
```

### DQN æ¶æ§‹

```python
class DQN:
    def __init__(self, state_dim, action_dim):
        """
        Deep Q-Network

        æ¯”å–»ï¼š
        ç”¨ç¥ç¶“ç¶²è·¯ã€Œä¼°è¨ˆã€æ¯å€‹å‹•ä½œçš„åƒ¹å€¼

        åƒæ•¸ï¼š
            state_dim: ç‹€æ…‹ç¶­åº¦ï¼ˆå¦‚ Atari éŠæˆ²ï¼š210Ã—160Ã—3ï¼‰
            action_dim: å‹•ä½œæ•¸é‡ï¼ˆå¦‚ 18 å€‹æŒ‰éµï¼‰
        """
        self.state_dim = state_dim
        self.action_dim = action_dim

        # ä¸»ç¶²è·¯ï¼ˆç”¨æ–¼é¸æ“‡å‹•ä½œï¼‰
        self.q_network = self.build_network()

        # ç›®æ¨™ç¶²è·¯ï¼ˆç”¨æ–¼è¨ˆç®—ç›®æ¨™ Q å€¼ï¼‰
        self.target_network = self.build_network()

        # ç¶“é©—å›æ”¾ç·©è¡å€
        self.memory = []
        self.memory_size = 10000

    def build_network(self):
        """
        å»ºç«‹ç¥ç¶“ç¶²è·¯

        æ¶æ§‹ï¼š
        è¼¸å…¥ï¼ˆéŠæˆ²ç•«é¢ï¼‰
            â†“ å·ç©å±¤ï¼ˆæå–ç‰¹å¾µï¼‰
        éš±è—å±¤
            â†“ å…¨é€£æ¥å±¤
        è¼¸å‡ºï¼ˆæ¯å€‹å‹•ä½œçš„ Q å€¼ï¼‰
        """
        class QNetwork:
            def __init__(self, state_dim, action_dim):
                # ç°¡åŒ–å¯¦ä½œï¼ˆå¯¦éš›æœƒç”¨ CNNï¼‰
                self.w1 = np.random.randn(state_dim, 128) * 0.01
                self.w2 = np.random.randn(128, action_dim) * 0.01

            def forward(self, state):
                """é æ¸¬ Q å€¼"""
                h = np.maximum(0, state.dot(self.w1))  # ReLU
                q_values = h.dot(self.w2)
                return q_values

        return QNetwork(self.state_dim, self.action_dim)

    def choose_action(self, state, epsilon=0.1):
        """
        é¸æ“‡å‹•ä½œï¼ˆEpsilon-Greedyï¼‰

        æ¯”å–»ï¼š
        å¤§éƒ¨åˆ†æ™‚é–“ï¼šé¸ã€Œç¥ç¶“ç¶²è·¯èªç‚ºæœ€å¥½ã€çš„å‹•ä½œ
        å°‘éƒ¨åˆ†æ™‚é–“ï¼šéš¨æ©Ÿæ¢ç´¢
        """
        if np.random.rand() < epsilon:
            return np.random.randint(self.action_dim)
        else:
            q_values = self.q_network.forward(state)
            return np.argmax(q_values)

    def store_transition(self, state, action, reward, next_state, done):
        """
        å­˜å„²ç¶“é©—

        æ¯”å–»ï¼š
        è¨˜ä½ã€Œæˆ‘åšäº†ä»€éº¼ï¼Œç™¼ç”Ÿäº†ä»€éº¼ã€
        ä¹‹å¾Œé‡æ’­é€™äº›ç¶“é©—ä¾†å­¸ç¿’
        """
        if len(self.memory) >= self.memory_size:
            self.memory.pop(0)  # ç§»é™¤æœ€èˆŠçš„ç¶“é©—

        self.memory.append((state, action, reward, next_state, done))

    def train(self, batch_size=32):
        """
        è¨“ç·´ç¶²è·¯ï¼ˆç¶“é©—å›æ”¾ï¼‰

        æ¯”å–»ï¼š
        å¾è¨˜æ†¶ä¸­ã€Œéš¨æ©ŸæŠ½å–ã€ä¸€æ‰¹ç¶“é©—
        ç”¨é€™äº›ç¶“é©—è¨“ç·´ç¥ç¶“ç¶²è·¯
        """
        if len(self.memory) < batch_size:
            return  # ç¶“é©—ä¸å¤ ï¼Œä¸è¨“ç·´

        # 1. éš¨æ©ŸæŠ½å–ä¸€æ‰¹ç¶“é©—
        batch = random.sample(self.memory, batch_size)

        for state, action, reward, next_state, done in batch:
            # 2. è¨ˆç®—ç›®æ¨™ Q å€¼
            if done:
                target_q = reward
            else:
                # ç”¨ç›®æ¨™ç¶²è·¯è¨ˆç®—ä¸‹ä¸€æ­¥çš„æœ€å¤§ Q å€¼
                next_q_values = self.target_network.forward(next_state)
                target_q = reward + 0.99 * np.max(next_q_values)

            # 3. è¨ˆç®—ç•¶å‰ Q å€¼
            q_values = self.q_network.forward(state)
            current_q = q_values[action]

            # 4. è¨ˆç®—æå¤±ä¸¦æ›´æ–°ï¼ˆçœç•¥åå‘å‚³æ’­ç´°ç¯€ï¼‰
            loss = (target_q - current_q) ** 2
            # ... åå‘å‚³æ’­æ›´æ–°æ¬Šé‡

    def update_target_network(self):
        """
        æ›´æ–°ç›®æ¨™ç¶²è·¯

        æ¯”å–»ï¼š
        æ¯éš”ä¸€æ®µæ™‚é–“ï¼ŒæŠŠã€Œä¸»ç¶²è·¯ã€çš„çŸ¥è­˜è¤‡è£½åˆ°ã€Œç›®æ¨™ç¶²è·¯ã€
        é¿å…è¨“ç·´ä¸ç©©å®š
        """
        self.target_network = self.q_network.copy()
```

### DQN çš„é—œéµæŠ€è¡“

#### 1. ç¶“é©—å›æ”¾ï¼ˆExperience Replayï¼‰

**å•é¡Œ**ï¼šé€£çºŒçš„ç¶“é©—é«˜åº¦ç›¸é—œ

```
æ‰“éŠæˆ²é€£çºŒ 100 å¹€ï¼š
å¹€ 1: å‘å³ç§»å‹•
å¹€ 2: å‘å³ç§»å‹•
å¹€ 3: å‘å³ç§»å‹•
...

å¦‚æœæŒ‰é †åºå­¸ç¿’ â†’ åªå­¸åˆ°ã€Œä¸€ç›´å‘å³ã€
â†’ å¿˜è¨˜å…¶ä»–å‹•ä½œ
```

**è§£æ±º**ï¼šéš¨æ©ŸæŠ½å–ç¶“é©—

```
è¨˜æ†¶æ± ï¼šå­˜å„²éå» 10,000 å€‹ç¶“é©—
æ¯æ¬¡è¨“ç·´ï¼šéš¨æ©ŸæŠ½å– 32 å€‹

å¥½è™•ï¼š
- æ‰“ç ´ç›¸é—œæ€§
- é‡è¤‡åˆ©ç”¨æ•¸æ“š
- è¨“ç·´æ›´ç©©å®š
```

#### 2. ç›®æ¨™ç¶²è·¯ï¼ˆTarget Networkï¼‰

**å•é¡Œ**ï¼šè¿½é€ç§»å‹•ç›®æ¨™

```
æ›´æ–° Q(s, a)ï¼š
ç›®æ¨™ = r + Î³ Ã— max Q(s', a')
              â†‘
        ä½†é€™å€‹ Q ä¹Ÿåœ¨è®Šï¼

æ¯”å–»ï¼š
ä½ æƒ³è·‘åˆ°ã€Œçµ‚é»ã€
ä½†ã€Œçµ‚é»ã€ä¸€ç›´åœ¨ç§»å‹•
â†’ æ°¸é è¿½ä¸åˆ°
```

**è§£æ±º**ï¼šå›ºå®šç›®æ¨™

```
ä¸»ç¶²è·¯ï¼šæ¯æ­¥éƒ½æ›´æ–°
ç›®æ¨™ç¶²è·¯ï¼šæ¯ 1000 æ­¥æ‰æ›´æ–°ä¸€æ¬¡

ç”¨ã€Œå›ºå®šçš„ã€ç›®æ¨™ç¶²è·¯è¨ˆç®—ç›®æ¨™ Q å€¼
â†’ è¨“ç·´æ›´ç©©å®š
```

---

## ğŸ® ç¶“å…¸æ¡ˆä¾‹ï¼šAlphaGo

### AlphaGo çš„æ ¸å¿ƒæŠ€è¡“

#### 1. è’™ç‰¹å¡ç¾…æ¨¹æœç´¢ï¼ˆMCTSï¼‰

**æ¯”å–»**ï¼šä¸‹æ£‹æ™‚çš„ã€Œé åˆ¤ã€

```
ç•¶å‰æ£‹ç›¤ç‹€æ…‹
    â†“
å¯èƒ½çš„ä¸‹ä¸€æ­¥ï¼ˆ5 å€‹ä½ç½®ï¼‰
    â†“
é¸æ“‡ã€Œæœ€æœ‰å¸Œæœ›ã€çš„ 3 å€‹
    â†“
å°æ¯å€‹ä½ç½®ï¼š
  æ¨¡æ“¬ã€Œéš¨æ©Ÿä¸‹å®Œã€é€™ç›¤æ£‹
  çœ‹èª°è´çš„æ©Ÿç‡é«˜
    â†“
é¸æ“‡ã€Œå‹ç‡æœ€é«˜ã€çš„ä½ç½®
```

#### 2. ç­–ç•¥ç¶²è·¯ï¼ˆPolicy Networkï¼‰

**ä½œç”¨**ï¼šé æ¸¬ã€Œäººé¡é«˜æ‰‹æœƒä¸‹å“ªã€

```
è¼¸å…¥ï¼šç•¶å‰æ£‹ç›¤
è¼¸å‡ºï¼šæ¯å€‹ä½ç½®çš„æ©Ÿç‡

è¨“ç·´æ–¹å¼ï¼š
å­¸ç¿’ã€Œäººé¡æ£‹è­œã€
â†’ æ¨¡ä»¿é«˜æ‰‹ä¸‹æ³•
```

#### 3. åƒ¹å€¼ç¶²è·¯ï¼ˆValue Networkï¼‰

**ä½œç”¨**ï¼šè©•ä¼°ã€Œç•¶å‰å±€é¢èª°å„ªå‹¢ã€

```
è¼¸å…¥ï¼šç•¶å‰æ£‹ç›¤
è¼¸å‡ºï¼šå‹ç‡ï¼ˆ-1 åˆ° 1ï¼‰

è¨“ç·´æ–¹å¼ï¼š
è‡ªæˆ‘å°å¼ˆ
â†’ å­¸ç¿’åˆ¤æ–·å±€é¢
```

#### 4. å¼·åŒ–å­¸ç¿’ï¼ˆSelf-Playï¼‰

**éç¨‹**ï¼š

```
ç¬¬ 1 ä»£ AlphaGoï¼š
å­¸ç¿’äººé¡æ£‹è­œï¼ˆç›£ç£å­¸ç¿’ï¼‰
â†’ é”åˆ°æ¥­é¤˜é«˜æ‰‹æ°´å¹³

ç¬¬ 2 ä»£ AlphaGoï¼š
è‡ªå·±å’Œè‡ªå·±ä¸‹ï¼ˆå¼·åŒ–å­¸ç¿’ï¼‰
â†’ ç™¼ç¾æ–°æ‹›å¼
â†’ è¶…è¶Šäººé¡

ç¬¬ N ä»£ AlphaGoï¼š
ç¹¼çºŒè‡ªæˆ‘å°å¼ˆ
â†’ è¶Šä¾†è¶Šå¼·

æ¯”å–»ï¼š
ä¸€é–‹å§‹ï¼šæ¨¡ä»¿è€å¸«
å¾Œä¾†ï¼šè‡ªå·±ç·´ç¿’ï¼Œè¶…è¶Šè€å¸«
```

---

## ğŸš€ é€²éšç®—æ³•

### 1. Policy Gradientï¼ˆç­–ç•¥æ¢¯åº¦ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šç›´æ¥å­¸ç¿’ç­–ç•¥ï¼ˆä¸å­¸ Q å‡½æ•¸ï¼‰

```
Q-Learning / DQNï¼š
å­¸ç¿’ Q(s, a) â†’ é¸æœ€å¤§ Q çš„å‹•ä½œ

Policy Gradientï¼š
ç›´æ¥å­¸ç¿’ Ï€(a|s)ï¼ˆåœ¨ç‹€æ…‹ s é¸å‹•ä½œ a çš„æ©Ÿç‡ï¼‰

æ¯”å–»ï¼š
Q-Learning = è€ƒæ…®ã€Œæ¯å€‹é¸é …çš„åƒ¹å€¼ã€å†é¸
Policy Gradient = ç›´æ¥ã€Œæ†‘ç›´è¦ºã€é¸
```

**æ›´æ–°è¦å‰‡**ï¼š

```
å¥½çš„å‹•ä½œ â†’ å¢åŠ æ©Ÿç‡
å£çš„å‹•ä½œ â†’ æ¸›å°‘æ©Ÿç‡

æ•¸å­¸ï¼š
âˆ‡Î¸ J(Î¸) = E[âˆ‡Î¸ log Ï€(a|s) Ã— R]
          â†‘              â†‘
      å¢åŠ å¥½å‹•ä½œæ©Ÿç‡   çå‹µ
```

### 2. Actor-Critic

**çµåˆå…©è€…å„ªé»**ï¼š

```
Actorï¼ˆæ¼”å“¡/ç­–ç•¥ç¶²è·¯ï¼‰ï¼š
ã€Œæˆ‘æ±ºå®šåšä»€éº¼å‹•ä½œã€
è¼¸å‡ºï¼šå‹•ä½œæ©Ÿç‡åˆ†ä½ˆ

Criticï¼ˆè©•å§”/åƒ¹å€¼ç¶²è·¯ï¼‰ï¼š
ã€Œæˆ‘è©•ä¼°é€™å€‹å‹•ä½œå¥½ä¸å¥½ã€
è¼¸å‡ºï¼šç‹€æ…‹åƒ¹å€¼ V(s)

è¨“ç·´ï¼š
Actorï¼šæ ¹æ“š Critic çš„è©•åƒ¹èª¿æ•´ç­–ç•¥
Criticï¼šæ ¹æ“šå¯¦éš›çå‹µèª¿æ•´è©•ä¼°

æ¯”å–»ï¼š
Actor = æ¼”å“¡è¡¨æ¼”
Critic = è©•å§”æ‰“åˆ†
æ¼”å“¡æ ¹æ“šè©•å§”åé¥‹æ”¹é€²æ¼”æŠ€
```

### 3. PPOï¼ˆProximal Policy Optimizationï¼‰

**å•é¡Œ**ï¼šç­–ç•¥æ›´æ–°å¤ªå¤§ â†’ æ€§èƒ½å´©æ½°

```
æƒ…æ³ï¼š
èˆŠç­–ç•¥ï¼šé‚„ä¸éŒ¯ï¼ˆå‹ç‡ 60%ï¼‰
æ›´æ–°ï¼šå¤ªæ¿€é€²
æ–°ç­–ç•¥ï¼šå´©æ½°ï¼ˆå‹ç‡ 10%ï¼‰

æ¯”å–»ï¼š
é–‹è»Šæ™‚çªç„¶ã€ŒçŒ›æ‰“æ–¹å‘ç›¤ã€
â†’ è»Šå­å¤±æ§
```

**è§£æ±º**ï¼šé™åˆ¶æ›´æ–°å¹…åº¦

```
PPO çš„åšæ³•ï¼š
æ›´æ–°æ™‚ï¼Œé™åˆ¶ã€Œæ–°èˆŠç­–ç•¥ä¸èƒ½å·®å¤ªå¤šã€

æ•¸å­¸ï¼š
clip(æ–°ç­–ç•¥/èˆŠç­–ç•¥, 0.8, 1.2)
      â†‘
   é™åˆ¶åœ¨ Â±20% ä»¥å…§

æ¯”å–»ï¼š
é–‹è»Šæ™‚ã€Œæ…¢æ…¢è½‰æ–¹å‘ç›¤ã€
â†’ å¹³ç©©å‰é€²
```

### 4. A3Cï¼ˆAsynchronous Advantage Actor-Criticï¼‰

**æ ¸å¿ƒ**ï¼šå¤šå€‹æ™ºèƒ½é«”ä¸¦è¡Œå­¸ç¿’

```
å‚³çµ± RLï¼š
1 å€‹æ™ºèƒ½é«” â†’ æ”¶é›†ç¶“é©— â†’ å­¸ç¿’
â†’ æ…¢

A3Cï¼š
10 å€‹æ™ºèƒ½é«”åŒæ™‚ç©éŠæˆ²
â†’ æ”¶é›†ç¶“é©—å¿« 10 å€
â†’ å®šæœŸåŒæ­¥åƒæ•¸

æ¯”å–»ï¼š
1 å€‹å­¸ç”Ÿå­¸ç¿’ vs 10 å€‹å­¸ç”Ÿä¸€èµ·å­¸ç¿’
â†’ 10 å€æ•¸æ“šï¼Œå­¸å¾—æ›´å¿«
```

---

## ğŸ“Š RL ç®—æ³•æ¯”è¼ƒ

| ç®—æ³• | é¡å‹ | å„ªé» | ç¼ºé» | é©ç”¨å ´æ™¯ |
|------|------|------|------|----------|
| **Q-Learning** | Value-based | ç°¡å–®æ˜“æ‡‚ | ç„¡æ³•è™•ç†é€£çºŒå‹•ä½œ | é›¢æ•£å‹•ä½œç©ºé–“ |
| **DQN** | Value-based | è™•ç†é«˜ç¶­ç‹€æ…‹ | æ¨£æœ¬æ•ˆç‡ä½ | Atari éŠæˆ² |
| **Policy Gradient** | Policy-based | é€£çºŒå‹•ä½œ | é«˜æ–¹å·® | æ©Ÿå™¨äººæ§åˆ¶ |
| **Actor-Critic** | æ··åˆ | ä½æ–¹å·® | å¯¦ä½œè¤‡é›œ | è¤‡é›œä»»å‹™ |
| **PPO** | Policy-based | ç©©å®š | èª¿åƒæ•æ„Ÿ | **é€šç”¨é¦–é¸** |
| **A3C** | æ··åˆ | è¨“ç·´å¿« | éœ€è¦å¤šæ ¸ | å¤§è¦æ¨¡è¨“ç·´ |

---

## ğŸ¯ å¯¦æˆ°æ‡‰ç”¨

### 1. éŠæˆ² AI

```
Atari éŠæˆ²ï¼ˆDQNï¼‰ï¼š
- Breakoutï¼ˆæ‰“ç£šå¡Šï¼‰
- Space Invadersï¼ˆå¤ªç©ºä¾µç•¥è€…ï¼‰
- Pongï¼ˆä¹’ä¹“çƒï¼‰

AlphaGoï¼ˆMCTS + DNNï¼‰ï¼š
- åœæ£‹

OpenAI Fiveï¼ˆPPOï¼‰ï¼š
- Dota 2

AlphaStarï¼ˆMulti-Agent RLï¼‰ï¼š
- æ˜Ÿæµ·çˆ­éœ¸ II
```

### 2. æ©Ÿå™¨äººæ§åˆ¶

```
ä»»å‹™ï¼š
- èµ°è·¯ï¼ˆæ³¢å£«é “å‹•åŠ›æ©Ÿå™¨äººï¼‰
- æŠ“å–ç‰©å“
- é–‹é–€

æŒ‘æˆ°ï¼š
- é€£çºŒå‹•ä½œç©ºé–“
- ç‰©ç†æ¨¡æ“¬
- å®‰å…¨æ€§

ç®—æ³•ï¼šPPOã€SAC
```

### 3. è‡ªå‹•é§•é§›

```
ä»»å‹™ï¼š
æ§åˆ¶æ–¹å‘ç›¤ã€æ²¹é–€ã€å‰è»Š

çå‹µè¨­è¨ˆï¼š
+1ï¼šä¿æŒè»Šé“
+10ï¼šé¿é–‹éšœç¤™ç‰©
-100ï¼šæ’è»Š
+50ï¼šåˆ°é”ç›®çš„åœ°

ç®—æ³•ï¼šDQNã€PPO
```

### 4. æ¨è–¦ç³»çµ±

```
ä»»å‹™ï¼š
æ¨è–¦ç”¨æˆ¶å¯èƒ½å–œæ­¡çš„å…§å®¹

ç‹€æ…‹ï¼šç”¨æˆ¶æ­·å²è¡Œç‚º
å‹•ä½œï¼šæ¨è–¦æŸå€‹å•†å“
çå‹µï¼šç”¨æˆ¶é»æ“Š/è³¼è²·

ç®—æ³•ï¼šContextual Banditã€DQN
```

---

## ğŸ“ å¯¦å‹™å»ºè­°

### 1. çå‹µè¨­è¨ˆï¼ˆæœ€é‡è¦ï¼ï¼‰

```
å¥½çš„çå‹µè¨­è¨ˆï¼š
âœ… æ˜ç¢ºç›®æ¨™
âœ… ç¨€ç–çå‹µ + å¯†é›†çå‹µ
âœ… é¿å…ã€Œä½œå¼Šã€

ä¾‹å­ï¼šè¨“ç·´æ©Ÿå™¨äººèµ°è·¯

å£è¨­è¨ˆï¼š
åªæœ‰ã€Œèµ°åˆ°çµ‚é»ã€æ‰æœ‰çå‹µ
â†’ æ©Ÿå™¨äººå¾ˆé›£å­¸æœƒï¼ˆå¤ªç¨€ç–ï¼‰

å¥½è¨­è¨ˆï¼š
+0.1ï¼šæ¯å¾€å‰ä¸€æ­¥
+10ï¼šåˆ°é”çµ‚é»
-1ï¼šæ‘”å€’

æ›´å¥½è¨­è¨ˆï¼š
+0.1ï¼šå¾€å‰
+0.05ï¼šä¿æŒå¹³è¡¡
+10ï¼šåˆ°é”çµ‚é»
-10ï¼šæ‘”å€’
```

### 2. è¶…åƒæ•¸èª¿å„ª

```python
# æ¨è–¦èµ·é»
config = {
    'learning_rate': 3e-4,      # å­¸ç¿’ç‡
    'gamma': 0.99,              # æŠ˜æ‰£å› å­
    'epsilon': 0.1,             # æ¢ç´¢ç‡
    'batch_size': 64,           # æ‰¹æ¬¡å¤§å°
    'buffer_size': 100000,      # ç¶“é©—æ± å¤§å°
    'update_freq': 4,           # æ›´æ–°é »ç‡
    'target_update_freq': 1000, # ç›®æ¨™ç¶²è·¯æ›´æ–°é »ç‡
}
```

### 3. èª¿è©¦æŠ€å·§

```
å•é¡Œ 1ï¼šä¸å­¸ç¿’ï¼ˆçå‹µä¸€ç›´å¾ˆä½ï¼‰
â†’ æª¢æŸ¥çå‹µè¨­è¨ˆ
â†’ é™ä½å­¸ç¿’ç‡
â†’ å¢åŠ æ¢ç´¢ç‡

å•é¡Œ 2ï¼šè¨“ç·´ä¸ç©©å®šï¼ˆçå‹µéœ‡ç›ªï¼‰
â†’ ä½¿ç”¨ç›®æ¨™ç¶²è·¯
â†’ æ¸›å°å­¸ç¿’ç‡
â†’ å¢å¤§æ‰¹æ¬¡å¤§å°

å•é¡Œ 3ï¼šéæ“¬åˆï¼ˆè¨“ç·´å¥½ï¼Œæ¸¬è©¦å·®ï¼‰
â†’ å¢åŠ æ¢ç´¢
â†’ ä½¿ç”¨ Dropout
â†’ ç°¡åŒ–ç¶²è·¯
```

---

## ğŸ”— ç¸½çµ

### RL æ ¸å¿ƒæ€æƒ³

1. **è©¦éŒ¯å­¸ç¿’**ï¼šé€éçå‹µ/æ‡²ç½°å­¸ç¿’
2. **å»¶é²çå‹µ**ï¼šè€ƒæ…®é•·æœŸå½±éŸ¿
3. **æ¢ç´¢èˆ‡åˆ©ç”¨**ï¼šå¹³è¡¡å˜—è©¦æ–°ç­–ç•¥å’Œä½¿ç”¨å·²çŸ¥æœ€ä½³ç­–ç•¥

### ä¸»è¦æŒ‘æˆ°

- çå‹µç¨€ç–ï¼ˆé›£ä»¥å­¸ç¿’ï¼‰
- æ¢ç´¢æ•ˆç‡ä½
- è¨“ç·´ä¸ç©©å®š
- æ¨£æœ¬æ•ˆç‡ä½

### ä¸»è¦æ‡‰ç”¨

- éŠæˆ² AIï¼ˆè¶…è¶Šäººé¡ï¼‰
- æ©Ÿå™¨äººæ§åˆ¶
- è‡ªå‹•é§•é§›
- æ¨è–¦ç³»çµ±

### æœªä¾†æ–¹å‘

- Model-Based RLï¼ˆçµåˆæ¨¡å‹ï¼‰
- Meta-RLï¼ˆå­¸æœƒå­¸ç¿’ï¼‰
- Multi-Agent RLï¼ˆå¤šæ™ºèƒ½é«”ï¼‰
- Offline RLï¼ˆé›¢ç·šå­¸ç¿’ï¼‰

---

*æœ€å¾Œæ›´æ–°: 2025-11-26*
