# 百萬級 TPS 的交易所餘額系統

## 百萬級吞吐量的交易所餘額系統 — 系統設計

[

![vic](https://miro.medium.com/v2/resize:fill:56:56/1*oKDGV_y2Sq7nAUHmdtgNhg.png)



](https://vicxu.medium.com/?source=post_page---byline--3297140272b2---------------------------------------)

13 min read

4 days ago

> **在高併發寫入與資料一致性之間，尋找系統設計的最佳解**

全球交易所系統，一秒需要處理上萬筆訂單，上萬筆訂單意味著上萬筆餘額變動，尤其是造市商或交易員一秒會產生 100~1000 訂單行為 (e.g 下單或撤單)，因此設計出同時兼顧**高併發、低延遲，並能保障資料正確**的餘額服務是一個不小的挑戰。

## 目錄

-   功能性需求 & 非功能性需求
-   系統設計 — 高吞吐餘額更新的效能瓶頸是啥？
-   系統設計 — 如何達到每秒百萬筆更新的吞吐量？
-   系統設計 — 如何設計可靠的快取層？
-   系統設計 — 如何打造可靠的 Event Driven 架構？
-   系統設計 — 如何乘載千萬到億級用戶？
-   系統設計 — 如何做到 zero downtime 以及高可用？
-   系統設計 — 如何實現分散式 transaction 同時降低其他服務串接的複雜度？
-   架構圖
-   總結 & AXS 專案

## 功能性需求：

**寫入行為：**  
\- 餘額更新，扣錢加錢  
\- 支援不同幣種餘額更新，支援單個用戶多筆更新  
\- 支援可用餘額與凍結餘額更新

**寫入限制：**  
\- 冪等性，相同訂單不能扣或加多次  
\- 餘額不能變成負數，餘額不夠要阻擋  
\- 每筆更新要有對應的流水  
\- 多筆更新具備 atomic 特性  
\- 驗證 client 請求是授權過的內部系統才能發起寫入

讀取行為：  
\- 查詢餘額  
\- 查詢餘額更新紀錄

查詢限制：  
\- 查詢資料不能落後寫入資料太多

系統整合：  
\- 與其他服務整合時，要實現分散式 transaction  
\- 確保跨服務之間的資料一致性

## 非功能性需求：

**系統規模：**  
\- 面向全球用戶  
\- 用戶數千萬~億級，DAU 10~50萬級

**吞吐量與 latency：**  
\- 查詢與寫入 P95 latency < 50ms  
\- avg 每秒處理 10w 筆餘額更新，尖峰要處理 100w 筆

**系統可用性：**  
\- 系統更新要 zero downtime  
\- 系統要有冗余，避免單點故障

## 系統設計

### 高吞吐餘額更新的效能瓶頸是啥？

單純 DB UPDATE ，即便是樂觀鎖在高併發情境下，P90 latency 也會到 1sec，avg latency 甚至會到 500ms，原因是鎖競爭，即便是不同用戶併發，雖沒有 row lock 競爭，但資料庫記憶體更新過程仍會競爭 page level lock。

鎖競爭導致大量 thread 互相等待，提高 latency 降低吞吐量，因此高效餘額更新要消除鎖，但如何在沒有鎖的情況下，確保資料更新正確？

> **_答案是用 single thread 處理相同用戶更新請求，不同用戶可是不同 thread 並行處理，類似 sharding。_**

single thread 一筆筆更新吞吐可能比樂觀鎖併發還差，因此要聚合請求，透過 batch update 一條 SQL 寫入多筆更新，雖然該方案吞吐已經比直接用 DB 樂觀鎖好了，但要達到每秒 10w ~ 100w 筆更新還不夠。

### 如何達到每秒百萬筆更新的吞吐量？

DB 在快也要 50~100ms，且多個 thread 同時 batch update 也會搶佔 CPU 資源，因此無法透過 DB 達到百萬級吞吐。

而解法是快取，用 redis 是一個方案，但執行 redis 指令有 tcp round trip ，總共仍要花 1~5ms。

最快方式是用 application 層的 in memory cache，沒有網路傳輸時間，沒有硬碟 I/O，是微秒等級的操作，輕鬆就能達到百萬的吞吐量，但用記憶體也帶來新的挑戰：

-   scale 多台 server 時記憶體資料該怎麼同步？
-   如何確保資料可靠，不遺失？
-   如何即時查詢記憶體中用戶更新後餘額？

### 如何設計可靠的快取層？

> _scale 多台 server 時記憶體資料該怎麼同步？_

為了消除鎖的使用，相同用戶更新請求要送往同一個 Server，因此不同 Server 記憶體中的餘額資料不需要彼此同步，各自管理被分配的用戶餘額就好，類似 sharding 概念。

> 如何確保資料可靠，不遺失？

資料庫 (e.g MySQL & PostgreSQL) 更新其實也是先在記憶體完成，那他們是怎麼保證 Durability？

更新在記憶體完成後，會將異動寫進 WAL (Write Ahead Log)，在透過 background worker 批次寫入硬碟，服務 crash 時從 WAL 回放資料。

類似機制，可用 Kafka 實現 WAL，餘額變動請求 produce 到 Kafka MQ ，consumer 消費請求，在記憶體完成更新後處理下一筆，更新結果透過 background worker 定期聚合多筆 batch update 寫入資料庫。

同時 flush worker 會 commit 最後一筆請求的 offset，當服務 crash，consumer 會從資料庫讀取餘額，並從 flush worker commit 的下個 offset 開始消費，不會有資料遺失。

> 如何即時查詢記憶體中用戶更新後餘額？

資料分散在不同 Server，因此需要一個額外資料庫查詢餘額，DB 的餘額會延遲，因此可用 thread pool 將記憶體中餘額的 snapshot 寫到 redis 。

由於 thread pool 是並行更新，每個 snapshot 要有時間戳，並採用 lua script 加 Last Win Write 機制，時間戳較大的值覆蓋較小的值，最終 redis 餘額會是最新 snapshot，且 thread pool 是併發執行，更新速度不會落後記憶體處理速度太多。

### 如何打造可靠的 Event Driven 架構？

實現類似 DB WAL 機制，本質是 Event Sourcing 的概念，也就是 queue 中資料要作為最終的資料結果。

[

![Become a member](https://miro.medium.com/v2/da:true/resize:fit:0/60026f4340686a391639ac58864da18070aa773cea45de6e55fa47fd56bfdb74)

![Become a member](https://miro.medium.com/v2/da:true/resize:fit:0/c061bd6cb52734164bf0c66f2543a6bc2acbe24ae3985dc15c898b3ddb2e1940)

](https://medium.com/plans?source=upgrade_membership---post_li_non_moc_upsell--3297140272b2---------------------------------------)

由於 DB UPDATE/INSERT 會有隨機 I/O ，成本高，因此將變動內容先寫入 append only 的結構中 (e.g WAL) 在異步寫回 B+Tree，一旦 DB Crash 記憶體中的資料都會消失，WAL 內資料就是資料的最終結果。

因此 WAL 需具備 持久化 & 可回放 特性，而 Kafka 本身就具備這兩個特性，是最佳的選擇，此外 consumer group 還能幫助實現 sharding 功能，相同用戶更新請求，透過 produce key 送進相同 partition 後，給相同 consumer 單線程處理並存在同一個 server 中的記憶體。

除了 持久化 & 可回放，還要有冪等性，相同 Event 不能被處理兩次，consumer 可透過紀錄狀態來阻擋，而 producer 可透過 outbox pattern 加 db unique index 來實現可靠的冪等性。

為了安全考量，client 端 (i.e 其他微服務) 不能直接往 kafka topic 推訊息，需要透過 API (e.g grpc) 呼叫，好處有：

-   可在 header 中加 HMAC-SHA256 的 sign 驗證 client 權限，避免有未授權的餘額更新。
-   封裝 producer 邏輯，確保 produce key 一致，不會出現相同用戶請求送往不同 partition 情況
-   結合 outbox pattern，先將餘額變動紀錄寫入 DB，同時用 unique key 實現冪等，db commit 後 produce 到 Kafka，produce 成功後更新紀錄狀態，若失敗可透過 db 紀錄重推

### 如何乘載千萬到億級用戶？

> 資料 Partition＆ Sharding 策略

千萬級用戶數需要用 table partition 分區，可在 schema 定義 shard\_id 欄位並採用 range partition 策略，剛方法好處為：

-   partition 可分群管理，例如 shard\_id (1~3) 為散戶，shard\_id (4–10) 為大戶
-   partition 遷移方便，若某個 partition 裡都是高頻交易員，可在建立新的 partition & shard\_id，透過 update shard\_id 的方式遷移資料到新 partition

此外有明確 shard\_id 欄位還能將 consumer group 分區與 db partition 綁定，例如相同 shard\_id 用戶都送往同個 consumer，並在同個 batch update 指令一起寫入 db，避免了 batch update 指令要跨 partition 更新的風險。

此外 redis 儲存大量用戶餘額也要 cluster，redis cluster 支援 sharding 功能，若用 lua script 批次更新不同用戶餘額，這些用戶資料要在同台 server，將 {shard\_id} 放入 redis key 中，作為 hash tag 可確保相同 shard\_id 用戶在同台 redis db，consumer 就可用 lua script 批次更新多筆用戶餘額。

如果是億級別用戶數，單台 write DB 會是效能瓶頸，需要 cluster + sharding，沿用上面 shard\_id 欄位策略，可用 zookeeper 管理 shard routing 表，方便管理，實作也比 consistent hashing 簡單。

> 應用層記憶體儲存的優化

若用有 GC 的語言 (e.g go or java)，當 heap 有太多物件，GC scan 會吃掉不少 CPU，且多物件分散在記憶體中不同位置，要跳來跳去掃描，更吃效能，因此需要降低 GC overhead 的記憶體儲存策略。

例如 go 的 big cache 套件此用 單一大 bytes array 儲存所有物件，物件要先 marshal 成 bytes 然後放進 bytes array，這樣不論有多少個物件，對 GC 來說只有一個 bytes array 物件要掃描。

而從 bytes array 中用 key 定位資料需要額外的 hashmap，該 hashmap 結構為 **{ key: 資料 array 起始 index}**，可快速從 bytes array 找到資料片段，unmarshal 回物件，而該 hashmap 結構單純，即便有很多 item GC 掃描也快，相反地，單純的 hashmap cache，value 是物件，物件的 field 又有指標 (e.g slice or map) 就會導致 GC scan 多很多層。

但 bytes array 缺點是會造成 memory fragment，由於更新資料要避免搬移後面 item，不能在同個陣列位置擴充，要將當前位置區段資料 mark unused，然後將新資料 append 到陣列最後面並更新 hashmap value 的 index，因此 mark unused 記憶體實際上屬於 bytes array 的一部份，不能馬上還給 kernel，多次更新後會導致很多 unused 區段，因此需要定期 compact，重建一個 bytes array 把 unused 區段還給 kernel。

### 如何做到 zero downtime 以及高可用？

要實現 consumer zero downtime 更新，要先把新 consumer 啟動，等舊 consumer 關閉後，新 consumer 在開始消費，若直接用 Kafka consumer group rebalance 機制無法 zero downtime，因為 rebalance 過程會花 1~5 秒。

因此需要自行管理 partition 以及 offset 分配，並用 consumer assign API 指定 partition 和 offset 就能立刻 consume 不用等 rebalance，但 assign 要指定 offset ，因此不能馬上啟動兩個 consumer 從相同 offset 開始消費，解決該問題需要 leader election 機制。

consumer 的 flush worker 會在同個 transaction 內，批次更新餘額以及更新最後一筆 Kafka offset，搶到 leader 的 consumer 可從 db 直接讀取 offset 值，透過 assign API 開始消費，而 leader election 機制可用 Redis lua script 實現，多個 consumer 對同個 key get-then-set，先搶到的成為 leader 執行 assign，當 leader graceful shutdown 時把 key 清掉，其他 consumer 就能立刻搶道 key 接替消費。

該 leader election 機制還可實現高可用，搶到 key 的 leader 要定期延長該 key 的 TTL，如果 leader 掛掉無法延長，key 就會過期，其他 consumer 就能拿到 key 後接續消費，只要將負責相同 partition 的 consumer server 部署在不同 AZ 就能實現高可用。

### 如何實現分散式 transaction 同時降低其他服務串接的複雜度？

餘額服務會與其他多個微服務串接，跨服務之間為確保資料一致性要實現分散式 transaction，為了效能考量已經採用 event driven 架構，因此 2 phase commit 不可行，且 2 phase commit 效能也差。

因此要用 saga pattern，例如下單服務跟餘額服務的串接：

1.  下單服務建立訂單 commit 後打餘額變更請求 API
2.  餘額請求透過 consumer 處理，處理完將結果 produce 到 Kafka
3.  下單服務消費 Kafka topic 確認餘額更新狀況，依照結果執行對應邏輯

服務之間溝通都是透過 MQ 異步交互，但 MQ 訊息不是 100% 可靠，因此需要 retry 機制，例如下單服務定期打餘額服務的 API，檢查訂單狀態是處理中的餘額更新結果。

但若有 N 個微服務，就要有 N 個 retry 機制，因此可反過來思考，在餘額服務提供一個 ACK API，其他微服務收到更新結果後送 ACK，餘額服務內部建置一個 cron server 定期將沒收到 ACK 的餘額更新 callback 給其他微服務，以此降低其他微服務與餘額服務整合的複雜度。

此外餘額服務也可提供 blocking 查詢 API，由於更新操作是異步且原子性限於單用戶，因此要實現轉帳功能，例如 A 轉 100 U 給 B，需先將 A 帳號扣 100 U，收到成功後，在加 100 U 到 B 帳號，若想把該操作封裝成一個 API，需要 blocking API 等待 A 成功 return，然後馬上加 100 U 到 B，在 blocking 等 B 成功，最後 response。

## 架構圖

Press enter or click to view image in full size

![](https://miro.medium.com/v2/resize:fit:1225/1*Dy49GQo55OllxQatOfa6Pw.png)

## 總結 & AXS 專案

這是我第二篇系統設計文章，第一篇介紹 [_短連結系統設計_](https://www.threads.com/@chill.vic.22/post/DRTHqf3EjC0?xmt=AQF0TmxIEwHTql1s2WQJXRSeGOf-EKn-_Vo9h0u8sQKduA) 時提到 CAP 中的 AP 系統，因此想試著設計有大量更新，且要處理 race condition 以及資料一致性的情境。

該架構主要靈感來自 DB 的 WAL，透過 Event Driven 架構 + 記憶體層優化效能，然而紙上談兵終究會面對現實的工程挑戰：

-   Leader Election 實作會不會容易出現腦裂？
-   Database Batch Update 實際效能為何，會落後記憶體太多嗎？
-   Redis LWW Lua Script 會很吃 Redis CPU 嗎？

系統設計雖然能提供大方向，但只有實作才能發掘細節！

> 「Talk is cheap, show me the code.」

因此基於上述設計，我開發了一個 Prototype — — [**AXS**](https://github.com/vx416/axs) ！並用 K6 壓測吞吐是能達到百萬的！

詳細的壓測數據可以看 [AXS Stress Test Doc](https://github.com/vx416/axs/blob/main/stresstest/README.md)！

專案文件可參考 [AXS README](https://github.com/vx416/axs/blob/main/README.md)，歡迎大家 fork 下來玩！

**About ME**  
我是 chill vic，最近開始寫系統設計系列文章，歡迎追蹤我的 Thread 收到第一手文章資訊 [https://www.threads.com/@chill.vic.22](https://www.threads.com/@chill.vic.22)
