# 主從複製是怎麼實現的？

大家好，我是小林哥。

我在前兩篇已經給大家圖解了 AOF 和 RDB，這兩個持久化技術保證了即使在服務器重啟的情況下也不會丟失數據（或少量損失）。

不過，由於數據都是存儲在一臺服務器上，如果出事就完犢子了，比如：

- 如果服務器發生了宕機，由於數據恢復是需要點時間，那麼這個期間是無法服務新的請求的；
- 如果這臺服務器的硬盤出現了故障，可能數據就都丟失了。

要避免這種單點故障，最好的辦法是將數據備份到其他服務器上，讓這些服務器也可以對外提供服務，這樣即使有一臺服務器出現了故障，其他服務器依然可以繼續提供服務。

![圖片](https://img-blog.csdnimg.cn/img_convert/22c7fe97ce5d3c382b08d83a4d8a5b96.png)

多臺服務器要保存同一份數據，這裡問題就來了。

這些服務器之間的數據如何保持一致性呢？數據的讀寫操作是否每臺服務器都可以處理？

Redis 提供了**主從複製模式**，來避免上述的問題。

這個模式可以保證多臺服務器的數據一致性，且主從服務器之間採用的是「讀寫分離」的方式。

主服務器可以進行讀寫操作，當發生寫操作時自動將寫操作同步給從服務器，而從服務器一般是隻讀，並接受主服務器同步過來寫操作命令，然後執行這條命令。

![圖片](https://img-blog.csdnimg.cn/img_convert/2b7231b6aabb9a9a2e2390ab3a280b2d.png)

也就是說，所有的數據修改只在主服務器上進行，然後將最新的數據同步給從服務器，這樣就使得主從服務器的數據是一致的。

同步這兩個字說的簡單，但是這個同步過程並沒有想象中那麼簡單，要考慮的事情不是一兩個。

我們先來看看，主從服務器間的第一次同步是如何工作的？

## 第一次同步

多臺服務器之間要通過什麼方式來確定誰是主服務器，或者誰是從服務器呢？

我們可以使用 `replicaof`（Redis 5.0 之前使用 slaveof）命令形成主服務器和從服務器的關係。

比如，現在有服務器 A 和 服務器 B，我們在服務器 B 上執行下面這條命令：

```
# 服務器 B 執行這條命令
replicaof <服務器 A 的 IP 地址> <服務器 A 的 Redis 端口號>
```

接著，服務器 B 就會變成服務器 A 的「從服務器」，然後與主服務器進行第一次同步。

主從服務器間的第一次同步的過程可分為三個階段：

- 第一階段是建立鏈接、協商同步；
- 第二階段是主服務器同步數據給從服務器；
- 第三階段是主服務器發送新寫操作命令給從服務器。

為了讓你更清楚瞭解這三個階段，我畫了一張圖。

![圖片](https://img-blog.csdnimg.cn/img_convert/ea4f7e86baf2435af3999e5cd38b6a26.png)

接下來，我在具體介紹每一個階段都做了什麼。

*第一階段：建立鏈接、協商同步*

執行了 replicaof 命令後，從服務器就會給主服務器發送 `psync` 命令，表示要進行數據同步。

psync 命令包含兩個參數，分別是**主服務器的 runID** 和**複製進度 offset**。

- runID，每個 Redis 服務器在啟動時都會自動生產一個隨機的 ID 來唯一標識自己。當從服務器和主服務器第一次同步時，因為不知道主服務器的 run ID，所以將其設置為 "?"。
- offset，表示複製的進度，第一次同步時，其值為 -1。

主服務器收到 psync 命令後，會用 `FULLRESYNC` 作為響應命令返回給對方。

並且這個響應命令會帶上兩個參數：主服務器的 runID 和主服務器目前的複製進度 offset。從服務器收到響應後，會記錄這兩個值。

FULLRESYNC 響應命令的意圖是採用**全量複製**的方式，也就是主服務器會把所有的數據都同步給從服務器。

所以，第一階段的工作時為了全量複製做準備。

那具體怎麼全量同步呀呢？我們可以往下看第二階段。

*第二階段：主服務器同步數據給從服務器*

接著，主服務器會執行 bgsave 命令來生成 RDB 文件，然後把文件發送給從服務器。

從服務器收到 RDB 文件後，會先清空當前的數據，然後載入 RDB 文件。

這裡有一點要注意，主服務器生成 RDB 這個過程是不會阻塞主線程的，因為 bgsave 命令是產生了一個子進程來做生成 RDB 文件的工作，是異步工作的，這樣 Redis 依然可以正常處理命令。

但是，這期間的寫操作命令並沒有記錄到剛剛生成的 RDB 文件中，這時主從服務器間的數據就不一致了。

那麼為了保證主從服務器的數據一致性，**主服務器在下面這三個時間間隙中將收到的寫操作命令，寫入到 replication buffer 緩衝區裡**：

-  主服務器生成 RDB 文件期間；
- 主服務器發送 RDB 文件給從服務器期間；
- 「從服務器」加載 RDB 文件期間；

*第三階段：主服務器發送新寫操作命令給從服務器*

在主服務器生成的 RDB 文件發送完，從服務器收到 RDB 文件後，丟棄所有舊數據，將 RDB 數據載入到內存。完成 RDB 的載入後，會回覆一個確認消息給主服務器。

接著，主服務器將 replication buffer 緩衝區裡所記錄的寫操作命令發送給從服務器，從服務器執行來自主服務器 replication buffer  緩衝區裡發來的命令，這時主從服務器的數據就一致了。

至此，主從服務器的第一次同步的工作就完成了。

## 命令傳播

主從服務器在完成第一次同步後，雙方之間就會維護一個 TCP 連接。

![圖片](https://img-blog.csdnimg.cn/img_convert/03eacec67cc58ff8d5819d0872ddd41e.png)

後續主服務器可以通過這個連接繼續將寫操作命令傳播給從服務器，然後從服務器執行該命令，使得與主服務器的數據庫狀態相同。

而且這個連接是長連接的，目的是避免頻繁的 TCP 連接和斷開帶來的性能開銷。

上面的這個過程被稱為**基於長連接的命令傳播**，通過這種方式來保證第一次同步後的主從服務器的數據一致性。

## 分攤主服務器的壓力

在前面的分析中，我們可以知道主從服務器在第一次數據同步的過程中，主服務器會做兩件耗時的操作：生成 RDB 文件和傳輸 RDB 文件。

主服務器是可以有多個從服務器的，如果從服務器數量非常多，而且都與主服務器進行全量同步的話，就會帶來兩個問題：

- 由於是通過 bgsave 命令來生成 RDB 文件的，那麼主服務器就會忙於使用 fork() 創建子進程，如果主服務器的內存數據非大，在執行 fork() 函數時是會阻塞主線程的，從而使得 Redis 無法正常處理請求；
- 傳輸 RDB 文件會佔用主服務器的網絡帶寬，會對主服務器響應命令請求產生影響。

這種情況就好像，剛創業的公司，由於人不多，所以員工都歸老闆一個人管，但是隨著公司的發展，人員的擴充，老闆慢慢就無法承擔全部員工的管理工作了。

要解決這個問題，老闆就需要設立經理職位，由經理管理多名普通員工，然後老闆只需要管理經理就好。

Redis 也是一樣的，從服務器可以有自己的從服務器，我們可以把擁有從服務器的從服務器當作經理角色，它不僅可以接收主服務器的同步數據，自己也可以同時作為主服務器的形式將數據同步給從服務器，組織形式如下圖：

![圖片](https://img-blog.csdnimg.cn/img_convert/4d850bfe8d712d3d67ff13e59b919452.png)

通過這種方式，**主服務器生成 RDB 和傳輸 RDB 的壓力可以分攤到充當經理角色的從服務器**。

那具體怎麼做到的呢？

其實很簡單，我們在「從服務器」上執行下面這條命令，使其作為目標服務器的從服務器：

```
replicaof <目標服務器的IP> 6379
```

此時如果目標服務器本身也是「從服務器」，那麼該目標服務器就會成為「經理」的角色，不僅可以接受主服務器同步的數據，也會把數據同步給自己旗下的從服務器，從而減輕主服務器的負擔。

## 增量複製

主從服務器在完成第一次同步後，就會基於長連接進行命令傳播。

可是，網絡總是不按套路出牌的嘛，說延遲就延遲，說斷開就斷開。

如果主從服務器間的網絡連接斷開了，那麼就無法進行命令傳播了，這時從服務器的數據就沒辦法和主服務器保持一致了，客戶端就可能從「從服務器」讀到舊的數據。

![圖片](https://img-blog.csdnimg.cn/img_convert/4845008abadaa871613873f5ffdcb542.png)

那麼問題來了，如果此時斷開的網絡，又恢復正常了，要怎麼繼續保證主從服務器的數據一致性呢？

在 Redis 2.8 之前，如果主從服務器在命令同步時出現了網絡斷開又恢復的情況，從服務器就會和主服務器重新進行一次全量複製，很明顯這樣的開銷太大了，必須要改進一波。

所以，從 Redis 2.8 開始，網絡斷開又恢復後，從主從服務器會採用**增量複製**的方式繼續同步，也就是隻會把網絡斷開期間主服務器接收到的寫操作命令，同步給從服務器。

網絡恢復後的增量複製過程如下圖：

![圖片](https://img-blog.csdnimg.cn/img_convert/e081b470870daeb763062bb873a4477e.png)

主要有三個步驟：

- 從服務器在恢復網絡後，會發送 psync 命令給主服務器，此時的 psync 命令裡的 offset 參數不是 -1；
- 主服務器收到該命令後，然後用 CONTINUE 響應命令告訴從服務器接下來採用增量複製的方式同步數據；
- 然後主服務將主從服務器斷線期間，所執行的寫命令發送給從服務器，然後從服務器執行這些命令。

那麼關鍵的問題來了，**主服務器怎麼知道要將哪些增量數據發送給從服務器呢？**

答案藏在這兩個東西里：

- **repl_backlog_buffer**，是一個「**環形**」緩衝區，用於主從服務器斷連後，從中找到差異的數據；
- **replication offset**，標記上面那個緩衝區的同步進度，主從服務器都有各自的偏移量，主服務器使用 master_repl_offset 來記錄自己「*寫*」到的位置，從服務器使用 slave_repl_offset 來記錄自己「*讀*」到的位置。

那 repl_backlog_buffer 緩衝區是什麼時候寫入的呢？

在主服務器進行命令傳播時，不僅會將寫命令發送給從服務器，還會將寫命令寫入到 repl_backlog_buffer 緩衝區裡，因此 這個緩衝區裡會保存著最近傳播的寫命令。

網絡斷開後，當從服務器重新連上主服務器時，從服務器會通過 psync 命令將自己的複製偏移量 slave_repl_offset 發送給主服務器，主服務器根據自己的 master_repl_offset 和 slave_repl_offset 之間的差距，然後來決定對從服務器執行哪種同步操作：

- 如果判斷出從服務器要讀取的數據還在 repl_backlog_buffer 緩衝區裡，那麼主服務器將採用**增量同步**的方式；
- 相反，如果判斷出從服務器要讀取的數據已經不存在 
  repl_backlog_buffer 緩衝區裡，那麼主服務器將採用**全量同步**的方式。

當主服務器在 repl_backlog_buffer 中找到主從服務器差異（增量）的數據後，就會將增量的數據寫入到 replication buffer 緩衝區，這個緩衝區我們前面也提到過，它是緩存將要傳播給從服務器的命令。

![圖片](https://img-blog.csdnimg.cn/img_convert/2db4831516b9a8b79f833cf0593c1f12.png)

repl_backlog_buffer 緩行緩衝區的默認大小是 1M，並且由於它是一個環形緩衝區，所以當緩衝區寫滿後，主服務器繼續寫入的話，就會覆蓋之前的數據。因此，當主服務器的寫入速度遠超於從服務器的讀取速度，緩衝區的數據一下就會被覆蓋。

那麼在網絡恢復時，如果從服務器想讀的數據已經被覆蓋了，主服務器就會採用全量同步，這個方式比增量同步的性能損耗要大很多。

因此，**為了避免在網絡恢復時，主服務器頻繁地使用全量同步的方式，我們應該調整下 repl_backlog_buffer 緩衝區大小，儘可能的大一些**，減少出現從服務器要讀取的數據被覆蓋的概率，從而使得主服務器採用增量同步的方式。

那 repl_backlog_buffer 緩衝區具體要調整到多大呢？

repl_backlog_buffer 最小的大小可以根據這面這個公式估算。

![圖片](https://img-blog.csdnimg.cn/img_convert/5e9e65a4a59b3688fa37cadbd87bb5ac.png)

我來解釋下這個公式的意思：

- second 為從服務器斷線後重新連接上主服務器所需的平均 時間(以秒計算)。
- write_size_per_second 則是主服務器平均每秒產生的寫命令數據量大小。

舉個例子，如果主服務器平均每秒產生 1 MB 的寫命令，而從服務器斷線之後平均要 5 秒才能重新連接主服務器。

那麼 repl_backlog_buffer 大小就不能低於 5 MB，否則新寫地命令就會覆蓋舊數據了。

當然，為了應對一些突發的情況，可以將 repl_backlog_buffer 的大小設置為此基礎上的 2 倍，也就是 10 MB。

關於 repl_backlog_buffer 大小修改的方法，只需要修改配置文件裡下面這個參數項的值就可以。

```shell
repl-backlog-size 1mb
```

## 總結

主從複製共有三種模式：**全量複製、基於長連接的命令傳播、增量複製**。

主從服務器第一次同步的時候，就是採用全量複製，此時主服務器會兩個耗時的地方，分別是生成 RDB 文件和傳輸 RDB 文件。為了避免過多的從服務器和主服務器進行全量複製，可以把一部分從服務器升級為「經理角色」，讓它也有自己的從服務器，通過這樣可以分攤主服務器的壓力。

第一次同步完成後，主從服務器都會維護著一個長連接，主服務器在接收到寫操作命令後，就會通過這個連接將寫命令傳播給從服務器，來保證主從服務器的數據一致性。

如果遇到網絡斷開，增量複製就可以上場了，不過這個還跟 repl_backlog_size 這個大小有關係。

如果它配置的過小，主從服務器網絡恢復時，可能發生「從服務器」想讀的數據已經被覆蓋了，那麼這時就會導致主服務器採用全量複製的方式。所以為了避免這種情況的頻繁發生，要調大這個參數的值，以降低主從服務器斷開後全量同步的概率。

## 面試題

### Redis主從節點是長連接還是短鏈接？

長連接

### 怎麼判斷 Redis 某個節點是否正常工作？

Redis 判斷節點是否正常工作，基本都是通過互相的 ping-pong 心態檢測機制，如果有一半以上的節點去 ping 一個節點的時候沒有 pong 迴應，集群就會認為這個節點掛掉了，會斷開與這個節點的連接。

Redis 主從節點發送的心態間隔是不一樣的，而且作用也有一點區別：

- Redis 主節點默認每隔 10 秒對從節點發送 ping 命令，判斷從節點的存活性和連接狀態，可通過參數repl-ping-slave-period控制發送頻率。
- Redis 從節點每隔 1 秒發送 replconf ack{offset} 命令，給主節點上報自身當前的複製偏移量，目的是為了：
  - 實時監測主從節點網絡狀態；
  - 上報自身複製偏移量， 檢查複製數據是否丟失， 如果從節點數據丟失， 再從主節點的複製緩衝區中拉取丟失數據。

### 主從複製架構中，過期key如何處理？

主節點處理了一個key或者通過淘汰算法淘汰了一個key，這個時間主節點模擬一條del命令發送給從節點，從節點收到該命令後，就進行刪除key的操作。

### Redis 是同步複製還是異步複製？

Redis 主節點每次收到寫命令之後，先寫到內部的緩衝區，然後異步發送給從節點。

### 主從複製中兩個 Buffer(replication buffer 、repl backlog buffer)有什麼區別？

replication buffer 、repl backlog buffer 區別如下：

- 出現的階段不一樣：
  - repl backlog buffer 是在增量複製階段出現，**一個主節點只分配一個 repl backlog buffer**；
  - replication buffer 是在全量複製階段和增量複製階段都會出現，**主節點會給每個新連接的從節點，分配一個 replication buffer**；
- 這兩個 Buffer 都有大小限制的，當緩衝區滿了之後，發生的事情不一樣：
  - 當 repl backlog buffer 滿了，因為是環形結構，會直接**覆蓋起始位置數據**;
  - 當 replication buffer 滿了，會導致連接斷開，刪除緩存，從節點重新連接，**重新開始全量複製**。

### 如何應對主從數據不一致？

> 為什麼會出現主從數據不一致？

主從數據不一致，就是指客戶端從從節點中讀取到的值和主節點中的最新值並不一致。

之所以會出現主從數據不一致的現象，是**因為主從節點間的命令複製是異步進行的**，所以無法實現強一致性保證（主從數據時時刻刻保持一致）。

具體來說，在主從節點命令傳播階段，主節點收到新的寫命令後，會發送給從節點。但是，主節點並不會等到從節點實際執行完命令後，再把結果返回給客戶端，而是主節點自己在本地執行完命令後，就會向客戶端返回結果了。如果從節點還沒有執行主節點同步過來的命令，主從節點間的數據就不一致了。

> 如何如何應對主從數據不一致？

第一種方法，儘量保證主從節點間的網絡連接狀況良好，避免主從節點在不同的機房。

第二種方法，可以開發一個外部程序來監控主從節點間的複製進度。具體做法：

- Redis 的 INFO replication 命令可以查看主節點接收寫命令的進度信息（master_repl_offset）和從節點複製寫命令的進度信息（slave_repl_offset），所以，我們就可以開發一個監控程序，先用 INFO replication 命令查到主、從節點的進度，然後，我們用 master_repl_offset 減去 slave_repl_offset，這樣就能得到從節點和主節點間的複製進度差值了。
- 如果某個從節點的進度差值大於我們預設的閾值，我們可以讓客戶端不再和這個從節點連接進行數據讀取，這樣就可以減少讀到不一致數據的情況。不過，為了避免出現客戶端和所有從節點都不能連接的情況，我們需要把複製進度差值的閾值設置得大一些。

### 主從切換如何減少數據丟失？

主從切換過程中，產生數據丟失的情況有兩種：

- 異步複製同步丟失
- 集群產生腦裂數據丟失

我們不可能保證數據完全不丟失，只能做到使得儘量少的數據丟失。

#### 異步複製同步丟失

對於 Redis 主節點與從節點之間的數據複製，是異步複製的，當客戶端發送寫請求給主節點的時候，客戶端會返回 ok，接著主節點將寫請求異步同步給各個從節點，但是如果此時主節點還沒來得及同步給從節點時發生了斷電，那麼主節點內存中的數據會丟失。

> 減少異步複製的數據丟失的方案

Redis 配置裡有一個參數 min-slaves-max-lag，表示一旦所有的從節點數據複製和同步的延遲都超過了 min-slaves-max-lag 定義的值，那麼主節點就會拒絕接收任何請求。

假設將 min-slaves-max-lag 配置為 10s 後，根據目前 master->slave 的複製速度，如果數據同步完成所需要時間超過10s，就會認為 master 未來宕機後損失的數據會很多，master 就拒絕寫入新請求。這樣就能將 master 和 slave 數據差控制在10s內，即使 master 宕機也只是這未複製的 10s 數據。

那麼對於客戶端，當客戶端發現 master 不可寫後，我們可以採取降級措施，將數據暫時寫入本地緩存和磁盤中，在一段時間（等 master 恢復正常）後重新寫入 master 來保證數據不丟失，也可以將數據寫入 kafka 消息隊列，等 master 恢復正常，再隔一段時間去消費 kafka 中的數據，讓將數據重新寫入 master 。

#### 集群產生腦裂數據丟失

先來理解集群的腦裂現象，這就好比一個人有兩個大腦，那麼到底受誰控制呢？

那麼在 Redis 中，集群腦裂產生數據丟失的現象是怎樣的呢？

在 Redis 主從架構中，部署方式一般是「一主多從」，主節點提供寫操作，從節點提供讀操作。

如果主節點的網絡突然發生了問題，它與所有的從節點都失聯了，但是此時的主節點和客戶端的網絡是正常的，這個客戶端並不知道 Redis 內部已經出現了問題，還在照樣的向這個失聯的主節點寫數據（過程A），此時這些數據被主節點緩存到了緩衝區裡，因為主從節點之間的網絡問題，這些數據都是無法同步給從節點的。

這時，哨兵也發現主節點失聯了，它就認為主節點掛了（但實際上主節點正常運行，只是網絡出問題了），於是哨兵就會在從節點中選舉出一個 leeder 作為主節點，這時集群就有兩個主節點了  —— **腦裂出現了**。

這時候網絡突然好了，哨兵因為之前已經選舉出一個新主節點了，它就會把舊主節點降級為從節點（A），然後從節點（A）會向新主節點請求數據同步，**因為第一次同步是全量同步的方式，此時的從節點（A）會清空掉自己本地的數據，然後再做全量同步。所以，之前客戶端在過程 A 寫入的數據就會丟失了，也就是集群產生腦裂數據丟失的問題**。

總結一句話就是：由於網絡問題，集群節點之間失去聯繫。主從數據不同步；重新平衡選舉，產生兩個主服務。等網絡恢復，舊主節點會降級為從節點，再與新主節點進行同步複製的時候，由於會從節點會清空自己的緩衝區，所以導致之前客戶端寫入的數據丟失了。

> 減少腦裂的數據丟的方案

當主節點發現「從節點下線的數量太多」，或者「網絡延遲太大」的時候，那麼主節點會禁止寫操作，直接把錯誤返回給客戶端。

在 Redis 的配置文件中有兩個參數我們可以設置：

- min-slaves-to-write x，主節點必須要有至少 x 個從節點連接，如果小於這個數，主節點會禁止寫數據。
- min-slaves-max-lag x，主從數據複製和同步的延遲不能超過 x 秒，如果超過，主節點會禁止寫數據。

我們可以把 min-slaves-to-write 和 min-slaves-max-lag 這兩個配置項搭配起來使用，分別給它們設置一定的閾值，假設為 N 和 T。

這兩個配置項組合後的要求是，**主節點連接的從節點中至少有 N 個從節點，「並且」主節點進行數據複製時的 ACK 消息延遲不能超過 T 秒**，否則，主節點就不會再接收客戶端的寫請求了。

即使原主節點是假故障，它在假故障期間也無法響應哨兵心跳，也不能和從節點進行同步，自然也就無法和從節點進行 ACK 確認了。這樣一來，min-slaves-to-write 和 min-slaves-max-lag 的組合要求就無法得到滿足，**原主節點就會被限制接收客戶端寫請求，客戶端也就不能在原主節點中寫入新數據了**。

**等到新主節點上線時，就只有新主節點能接收和處理客戶端請求，此時，新寫的數據會被直接寫到新主節點中。而原主節點會被哨兵降為從節點，即使它的數據被清空了，也不會有新數據丟失。我再來給你舉個例子。**

假設我們將 min-slaves-to-write 設置為 1，把 min-slaves-max-lag 設置為 12s，把哨兵的 down-after-milliseconds 設置為 10s，主節點因為某些原因卡住了 15s，導致哨兵判斷主節點客觀下線，開始進行主從切換。同時，因為原主節點卡住了 15s，沒有一個從節點能和原主節點在 12s 內進行數據複製，原主節點也無法接收客戶端請求了。這樣一來，主從切換完成後，也只有新主節點能接收請求，不會發生腦裂，也就不會發生數據丟失的問題了。

### 主從如何做到故障自動切換？

主節點掛了 ，從節點是無法自動升級為主節點的，這個過程需要人工處理，在此期間 Redis 無法對外提供寫操作。

此時，Redis 哨兵機制就登場了，哨兵在發現主節點出現故障時，由哨兵自動完成故障發現和故障轉移，並通知給應用方，從而實現高可用性。

----

最新的圖解文章都在公眾號首發，別忘記關注哦！！如果你想加入百人技術交流群，掃碼下方二維碼回覆「加群」。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)
