# 4.21 用了 TCP 協議，數據一定不會丟嗎？

>來源：公眾號@小白debug
>
>原文地址：[用了 TCP 協議，數據一定不會丟嗎？](https://mp.weixin.qq.com/s/XNJoaVnYT1SxHsdNWeAaUw)

大家後，我是小林。

問大家一句：TCP 是一個可靠的傳輸協議，那它一定能保證數據不丟失嗎？

這次，就跟大家探討這個問題。

## 數據包的發送流程

首先，我們兩個手機的綠皮聊天軟件客戶端，要通信，中間會通過它們家服務器。大概長這樣。

![聊天軟件三端通信](https://img-blog.csdnimg.cn/img_convert/1d0a1d60ca4f720423911cf8f25c4ac3.png)

但為了**簡化模型**，我們把中間的服務器給省略掉，假設這是個端到端的通信。且為了保證消息的可靠性，我們盲猜它們之間用的是**TCP協議**進行通信。

![聊天軟件兩端通信](https://img-blog.csdnimg.cn/img_convert/7e8bae365b8d27560aac1cd28f501156.png)

為了發送數據包，兩端首先會通過**三次握手**，建立TCP連接。

一個數據包，從聊天框裡發出，消息會從**聊天軟件**所在的**用戶空間**拷貝到**內核空間**的**發送緩衝區（send buffer）**，數據包就這樣順著**傳輸層、網絡層，進入到數據鏈路層，在這裡數據包會經過流控（qdisc），再通過RingBuffer發到物理層的網卡**。數據就這樣順著**網卡**發到了**紛繁複雜**的網絡世界裡。這裡頭數據會經過n多個**路由器和交換機**之間的跳轉，最後到達**目的機器的網卡**處。

此時目的機器的網卡會通知**DMA**將數據包信息放到`RingBuffer`中，再觸發一個**硬中斷**給`CPU`，`CPU`觸發**軟中斷**讓`ksoftirqd`去`RingBuffer`收包，於是一個數據包就這樣順著**物理層，數據鏈路層，網絡層，傳輸層**，最後從內核空間拷貝到用戶空間裡的**聊天軟件**裡。

![網絡發包收包全景圖](https://img-blog.csdnimg.cn/img_convert/28e4d6b004530fbf75fe346d181baa81.png)

> 畫了那麼大一張圖，只水了200字做解釋，我多少是有些心痛的。

到這裡，拋開一些細節，大家大概知道了一個數據包從**發送到接收**的宏觀過程。

可以看到，這上面全是密密麻麻的**名詞**。

整條鏈路下來，有不少地方可能會發生丟包。

但為了不讓大家**保持蹲姿太久**影響身體健康，我這邊只重點講下幾個**常見容易發生丟包的場景**。

## 建立連接時丟包

TCP協議會通過**三次握手**建立連接。大概長下面這樣。

![TCP三次握手](https://img-blog.csdnimg.cn/img_convert/923f5005edb536c0d07b096bbf2ca282.png)

在服務端，第一次握手之後，會先建立個**半連接**，然後再發出第二次握手。這時候需要有個地方可以**暫存**這些半連接。這個地方就叫**半連接隊列**。

如果之後第三次握手來了，半連接就會升級為全連接，然後暫存到另外一個叫**全連接隊列**的地方，坐等程序執行`accept()`方法將其取走使用。

![半連接隊列和全連接隊列](https://img-blog.csdnimg.cn/img_convert/02a78bb83fe167324f26e8c910d7a7a2.png)

是隊列就有長度，有長度就有可能會滿，如果它們**滿了**，那新來的包就會被**丟棄**。

可以通過下面的方式查看是否存在這種丟包行為。

```shell
# 全連接隊列溢出次數
# netstat -s | grep overflowed
    4343 times the listen queue of a socket overflowed

# 半連接隊列溢出次數
# netstat -s | grep -i "SYNs to LISTEN sockets dropped"
    109 times the listen queue of a socket overflowed 
```

從現象來看就是連接建立失敗。

![圖片](https://img-blog.csdnimg.cn/img_convert/591d630098b4fc5316a5005f1e94b844.png)

## 流量控制丟包

應用層能髮網絡數據包的軟件有那麼多，如果所有數據不加控制一股腦衝入到網卡，網卡會吃不消，那怎麼辦？讓數據按一定的規則排個隊依次處理，也就是所謂的**qdisc**(**Q**ueueing **Disc**iplines，排隊規則)，這也是我們常說的**流量控制**機制。

排隊，得先有個隊列，而隊列有個**長度**。

我們可以通過下面的`ifconfig`命令查看到，裡面涉及到的`txqueuelen`後面的數字`1000`，其實就是流控隊列的長度。

當發送數據過快，流控隊列長度`txqueuelen`又不夠大時，就容易出現**丟包**現象。

![qdisc丟包](https://img-blog.csdnimg.cn/img_convert/6f2821018be08a2f27561155e8085de4.png)

可以通過下面的`ifconfig`命令，查看TX下的dropped字段，當它大於0時，則**有可能**是發生了流控丟包。

```shell
# ifconfig eth0
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.21.66.69  netmask 255.255.240.0  broadcast 172.21.79.255
        inet6 fe80::216:3eff:fe25:269f  prefixlen 64  scopeid 0x20<link>
        ether 00:16:3e:25:26:9f  txqueuelen 1000  (Ethernet)
        RX packets 6962682  bytes 1119047079 (1.0 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 9688919  bytes 2072511384 (1.9 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```

當遇到這種情況時，我們可以嘗試修改下流控隊列的長度。比如像下面這樣將eth0網卡的流控隊列長度從1000提升為1500.

```shell
# ifconfig eth0 txqueuelen 1500
```

## 網卡丟包

網卡和它的驅動導致丟包的場景也比較常見，原因很多，比如**網線質量差，接觸不良**。除此之外，我們來聊幾個常見的場景。

### RingBuffer過小導致丟包

上面提到，在接收數據時，會將數據暫存到`RingBuffer`接收緩衝區中，然後等著內核觸發軟中斷慢慢收走。如果這個**緩衝區過小**，而這時候發送的數據又過快，就有可能發生溢出，此時也會產生**丟包**。

![RingBuffer滿了導致丟包](https://img-blog.csdnimg.cn/img_convert/8f3ed2d6c4e2e154849f1e661528fe89.png)

我們可以通過下面的命令去查看是否發生過這樣的事情。

```shell
# ifconfig
eth0:  RX errors 0  dropped 0  overruns 0  frame 0
```

查看上面的`overruns`指標，它記錄了由於`RingBuffer`長度不足導致的溢出次數。

當然，用`ethtool`命令也能查看。

```shell
# ethtool -S eth0|grep rx_queue_0_drops
```

但這裡需要注意的是，因為一個網卡里是可以有**多個RingBuffer**的，所以上面的`rx_queue_0_drops`裡的0代表的是**第0個RingBuffer**的丟包數，對於多隊列的網卡，這個0還可以改成其他數字。但我的家庭條件不允許我看其他隊列的丟包數，所以上面的命令對我來說是夠用了。。。

當發現有這類型丟包的時候，可以通過下面的命令查看當前網卡的配置。

```shell
#ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:        4096
RX Mini:    0
RX Jumbo:    0
TX:        4096
Current hardware settings:
RX:        1024
RX Mini:    0
RX Jumbo:    0
TX:        1024
```

上面的輸出內容，含義是**RingBuffer最大支持4096的長度，但現在實際只用了1024。**

想要修改這個長度可以執行`ethtool -G eth1 rx 4096 tx 4096`將發送和接收RingBuffer的長度都改為4096。

**RingBuffer**增大之後，可以減少因為容量小而導致的丟包情況。

### 網卡性能不足

網卡作為硬件，**傳輸速度是有上限的**。當網絡傳輸速度過大，達到網卡上限時，就會發生丟包。這種情況一般常見於壓測場景。

我們可以通過`ethtool`加網卡名，獲得當前網卡支持的最大速度。

```shell
# ethtool eth0
Settings for eth0:
    Speed: 10000Mb/s
```

可以看到，我這邊用的網卡能支持的最大傳輸速度**speed=1000Mb/s**。

也就是俗稱的千兆網卡，但注意這裡的單位是**Mb**，這裡的**b是指bit，而不是Byte。1Byte=8bit**。所以10000Mb/s還要除以8，也就是理論上網卡最大傳輸速度是`1000/8 = 125MB/s`。

我們可以通過`sar命令`從網絡接口層面來分析數據包的收發情況。

```shell
# sar -n DEV 1
Linux 3.10.0-1127.19.1.el7.x86_64      2022年07月27日     _x86_64_    (1 CPU)

08時35分39秒     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s    rxcmp/s   txcmp/s  rxmcst/s
08時35分40秒      eth0      6.06      4.04      0.35    121682.33   0.00    0.00     0.00
```

其中 **txkB/s是指當前每秒發送的字節（byte）總數，rxkB/s是指每秒接收的字節（byte）總數**。

當兩者加起來的值約等於`12~13w字節`的時候，也就對應大概`125MB/s`的傳輸速度。此時達到網卡性能極限，就會開始丟包。

遇到這個問題，優先看下你的服務是不是真有這麼大的**真實流量**，如果是的話可以考慮下拆分服務，或者就忍痛充錢升級下配置吧。

## 接收緩衝區丟包

我們一般使用`TCP socket`進行網絡編程的時候，內核都會分配一個**發送緩衝區**和一個**接收緩衝區**。

當我們想要發一個數據包，會在代碼裡執行`send(msg)`，這時候數據包並不是一把梭直接就走網卡飛出去的。而是將數據拷貝到內核**發送緩衝區**就完事**返回**了，至於**什麼時候發數據，發多少數據**，這個後續由內核自己做決定。

![tcp_sendmsg邏輯](https://img-blog.csdnimg.cn/img_convert/9cd22437777205662048c73cc5855add.png)

而**接收緩衝區**作用也類似，從外部網絡收到的數據包就暫存在這個地方，然後坐等用戶空間的應用程序將數據包取走。

這兩個緩衝區是有大小限制的，可以通過下面的命令去查看。

```shell
# 查看接收緩衝區
# sysctl net.ipv4.tcp_rmem
net.ipv4.tcp_rmem = 4096    87380   6291456

# 查看發送緩衝區
# sysctl net.ipv4.tcp_wmem
net.ipv4.tcp_wmem = 4096    16384   4194304
```

不管是接收緩衝區還是發送緩衝區，都能看到三個數值，分別對應緩衝區的**最小值，默認值和最大值 （min、default、max）。緩衝區會在min和max之間動態調整。**

**那麼問題來了，如果緩衝區設置過小會怎麼樣？**

對於**發送緩衝區**，執行send的時候，如果是**阻塞**調用，那就會等，等到緩衝區有空位可以發數據。

![send阻塞](https://img-blog.csdnimg.cn/img_convert/7312e536393463dcf0d57aeb07f28ed5.gif)

如果是**非阻塞**調用，就會**立刻返回**一個 `EAGAIN` 錯誤信息，意思是  `Try again`。讓應用程序下次再重試。這種情況下一般不會發生丟包。

![send非阻塞](https://img-blog.csdnimg.cn/img_convert/f378a299ca60c490ee5437e1143916c8.gif)

當接受緩衝區滿了，事情就不一樣了，它的TCP接收窗口會變為0，也就是所謂的**零窗口**，並且會通過數據包裡的`win=0`，告訴發送端，"球球了，頂不住了，別發了"。一般這種情況下，發送端就該停止發消息了，但如果這時候確實還有數據發來，就會發生**丟包**。

![recv_buffer丟包](https://img-blog.csdnimg.cn/img_convert/2df66c2e1d9f1245813e8d1de7482e0c.png)

我們可以通過下面的命令裡的`TCPRcvQDrop`查看到有沒有發生過這種丟包現象。

```shell
cat /proc/net/netstat
TcpExt: SyncookiesSent TCPRcvQDrop SyncookiesFailed
TcpExt: 0              157              60116
```

但是說個傷心的事情，我們一般也看不到這個`TCPRcvQDrop`，因為這個是`5.9版本`裡引入的打點，而我們的服務器用的一般是`2.x~3.x`左右版本。你可以通過下面的命令查看下你用的是什麼版本的linux內核。

```shell
# cat /proc/version
Linux version 3.10.0-1127.19.1.el7.x86_64
```

## 兩端之間的網絡丟包

前面提到的是兩端機器內部的網絡丟包，除此之外，兩端之間那麼長的一條鏈路都屬於外部網絡，這中間有各種路由器和交換機還有光纜啥的，丟包也是很經常發生的。

這些丟包行為發生在中間鏈路的某些個機器上，我們當然是沒權限去登錄這些機器。但我們可以通過一些命令觀察整個鏈路的連通情況。

### **ping命令查看丟包**

比如我們知道目的地的域名是 `baidu.com`。想知道你的機器到baidu服務器之間，有沒有產生丟包行為。可以使用ping命令。

![ping查看丟包](https://img-blog.csdnimg.cn/img_convert/56bdca9995c0c2a343b2b73b67933b78.png)

倒數第二行裡有個`100% packet loss`，意思是丟包率100%。

但這樣其實你只能知道**你的機器和目的機器之間有沒有丟包。**

**那如果你想知道你和目的機器之間的這條鏈路，哪個節點丟包了，有沒有辦法呢?**

有。

### **mtr命令**

mtr命令可以查看到你的機器和目的機器之間的每個節點的丟包情況。

像下面這樣執行命令。

![mtr_icmp](https://img-blog.csdnimg.cn/img_convert/4a2d8dbfb648bcced864fb653af9f036.png)

其中 -r 是指report，以報告的形式打印結果。

可以看到`Host`那一列，出現的都是鏈路中間每一跳的機器，`Loss`的那一列就是指這一跳對應的丟包率。

需要注意的是，中間有一些是host是`???`，那個是因為**mtr默認用的是ICMP包**，有些節點限制了**ICMP包**，導致不能正常展示。

我們可以在mtr命令里加個`-u`，也就是使用**udp包**，就能看到部分???對應的IP。

![mtr-udp](https://img-blog.csdnimg.cn/img_convert/0650adc524ab7d82028dc83cfc9961e1.png)

把**ICMP包和UDP包的結果**拼在一起看，就是**比較完整**的鏈路圖了。

還有個小細節，`Loss`那一列，我們在icmp的場景下，關注**最後一行**，如果是0%，那不管前面loss是100%還是80%都無所謂，那些都是**節點限制**導致的**虛報**。

但如果**最後一行是20%，再往前幾行都是20%左右**，那說明丟包就是從最接近的那一行開始產生的，長時間是這樣，那很可能這一跳出了點問題。如果是公司內網的話，你可以帶著這條線索去找對應的網絡同事。如果是外網的話，那耐心點等等吧，別人家的開發會比你更著急。

![圖片](https://img-blog.csdnimg.cn/img_convert/7142a4e285024dc6aadea4255984c485.png)

## 發生丟包了怎麼辦

說了這麼多。只是想告訴大家，**丟包是很常見的，幾乎不可避免的一件事情**。

但問題來了，發生丟包了怎麼辦？

這個好辦，用**TCP協議**去做傳輸。

![TCP是什麼](https://img-blog.csdnimg.cn/img_convert/b2225e071fec7cfb240aa295ed4037bf.png)

建立了TCP連接的兩端，發送端在發出數據後會等待接收端回覆`ack包`，`ack包`的目的是為了告訴對方自己確實收到了數據，但如果中間鏈路發生了丟包，那發送端會遲遲收不到確認ack，於是就會進行**重傳**。以此來保證每個數據包都確確實實到達了接收端。

假設現在網斷了，我們還用聊天軟件發消息，聊天軟件會使用TCP不斷嘗試重傳數據，**如果重傳期間網絡恢復了**，那數據就能正常發過去。但如果多次重試直到超時都還是失敗，這時候你將收穫一個**紅色感嘆號**。

![圖片](https://img-blog.csdnimg.cn/img_convert/c1460d52efe7c5e4d80c2f7160d5b126.png)

這時候問題又來了。

假設**某綠皮聊天軟件用的就是TCP協議。**

在聊天的時候， 發生丟包了，丟包了會**重試**，重試失敗了還會出現**紅色感嘆號。**

於是乎，問題就變成了，**用了 TCP 協議，就一定不會丟包嗎？**

## 用了TCP協議就一定不會丟包嗎

我們知道TCP位於**傳輸層**，在它的上面還有各種**應用層協議**，比如常見的HTTP或者各類RPC協議。

![四層網絡協議](https://img-blog.csdnimg.cn/img_convert/c6794dd51c8780f12e4022fc964ebb0a.png)

TCP保證的可靠性，是**傳輸層的可靠性**。也就是說，**TCP只保證數據從A機器的傳輸層可靠地發到B機器的傳輸層。**

至於數據到了接收端的傳輸層之後，能不能保證到應用層，TCP並不管。

假設現在，我們輸入一條消息，從聊天框發出，走到**傳輸層TCP協議的發送緩衝區**，不管中間有沒有丟包，最後通過重傳都保證發到了對方的**傳輸層TCP接收緩衝區**，此時接收端回覆了一個`ack`，發送端收到這個`ack`後就會將自己**發送緩衝區**裡的消息給扔掉。到這裡TCP的任務就結束了。

TCP任務是結束了，但聊天軟件的任務沒結束。

**聊天軟件還需要將數據從TCP的接收緩衝區裡讀出來，如果在讀出來這一刻，手機由於內存不足或其他各種原因，導致軟件崩潰閃退了。**

發送端以為自己發的消息已經發給對方了，但接收端卻並沒有收到這條消息。

於是乎，**消息就丟了。**

![使用TCP協議卻發生丟包](https://img-blog.csdnimg.cn/img_convert/9286ab84bcaa74576bc11c8e9322fee9.png)

**雖然概率很小，但它就是發生了**。

合情合理，邏輯自洽。

## 這類丟包問題怎麼解決？

故事到這裡也到尾聲了，感動之餘，我們來**聊點掏心窩子的話**。

**其實前面說的都對，沒有一句是假話**。

但某綠皮聊天軟件這麼成熟，怎麼可能沒考慮過這一點呢。

大家應該還記得我們文章開頭提到過，**為了簡單**，就將服務器那一方給省略了，從三端通信變成了兩端通信，所以才有了這個丟包問題。

**現在我們重新將服務器加回來。**

![聊天軟件三端通信](https://img-blog.csdnimg.cn/img_convert/d53659df39d64db4780d2816bd8314d1.png)

大家有沒有發現，有時候我們在手機裡聊了一大堆內容，然後登錄電腦版，它能將最近的聊天記錄都同步到電腦版上。也就是說服務器**可能**記錄了我們最近發過什麼數據，假設**每條消息都有個id**，服務器和聊天軟件每次都拿**最新消息的id**進行對比，就能知道兩端消息是否一致，就像**對賬**一樣。

對於**發送方**，只要定時跟服務端的內容對賬一下，就知道哪條消息沒發送成功，直接重發就好了。

如果**接收方**的聊天軟件崩潰了，重啟後跟服務器稍微通信一下就知道少了哪條數據，同步上來就是了，所以也不存在上面提到的丟包情況。

可以看出，**TCP只保證傳輸層的消息可靠性，並不保證應用層的消息可靠性。如果我們還想保證應用層的消息可靠性，就需要應用層自己去實現邏輯做保證。**

那麼問題叒來了，**兩端通信的時候也能對賬，為什麼還要引入第三端服務器？**

主要有三個原因。

- 第一，如果是兩端通信，你聊天軟件裡有`1000個`好友，你就得建立`1000個`連接。但如果引入服務端，你只需要跟服務器建立`1個`連接就夠了，**聊天軟件消耗的資源越少，手機就越省電**。
- 第二，就是**安全問題**，如果還是兩端通信，隨便一個人找你對賬一下，你就把聊天記錄給同步過去了，這並不合適吧。如果對方別有用心，信息就洩露了。引入第三方服務端就可以很方便的做各種**鑑權**校驗。
- 第三，是**軟件版本問題**。軟件裝到用戶手機之後，軟件更不更新就是由用戶說了算了。如果還是兩端通信，且兩端的**軟件版本跨度太大**，很容易產生各種兼容性問題，但引入第三端服務器，就可以強制部分過低版本升級，否則不能使用軟件。但對於大部分兼容性問題，給服務端加兼容邏輯就好了，不需要強制用戶更新軟件。

所以看到這裡大家應該明白了，我把服務端去掉，並不單純是**為了簡單**。

## 總結

- 數據從發送端到接收端，鏈路很長，任何一個地方都可能發生丟包，幾乎可以說丟包不可避免。
- 平時沒事也不用關注丟包，大部分時候TCP的重傳機制保證了消息可靠性。
- 當你發現服務異常的時候，比如接口延時很高，總是失敗的時候，可以用ping或者mtr命令看下是不是中間鏈路發生了丟包。
- TCP只保證傳輸層的消息可靠性，並不保證應用層的消息可靠性。如果我們還想保證應用層的消息可靠性，就需要應用層自己去實現邏輯做保證。

----

***哈嘍，我是小林，就愛圖解計算機基礎，如果覺得文章對你有幫助，歡迎微信搜索「小林coding」***

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E5%85%B6%E4%BB%96/%E5%85%AC%E4%BC%97%E5%8F%B7%E4%BB%8B%E7%BB%8D.png)