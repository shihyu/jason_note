# Mojo çˆ¬èŸ²å®Œæ•´æŒ‡å—

## ğŸ“š ç›®éŒ„
- [Mojo å®‰è£](#mojo-å®‰è£)
- [ç’°å¢ƒæº–å‚™](#ç’°å¢ƒæº–å‚™)
- [åŸºç¤çˆ¬èŸ²ç¯„ä¾‹](#åŸºç¤çˆ¬èŸ²ç¯„ä¾‹)
- [é€²éšçˆ¬èŸ²ç¯„ä¾‹](#é€²éšçˆ¬èŸ²ç¯„ä¾‹)
- [å¯¦ç”¨å·¥å…·å‡½æ•¸](#å¯¦ç”¨å·¥å…·å‡½æ•¸)
- [æœ€ä½³å¯¦è¸](#æœ€ä½³å¯¦è¸)
- [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

## ğŸš€ Mojo å®‰è£

### ç³»çµ±éœ€æ±‚
- **ä½œæ¥­ç³»çµ±**: Ubuntu 20.04+ æˆ– macOS 12+
- **ç¡¬é«”**: x86-64 æˆ– ARM64 æ¶æ§‹
- **è¨˜æ†¶é«”**: è‡³å°‘ 4GB RAM

### å®‰è£æ­¥é©Ÿ

#### æ–¹æ³• 1: å®˜æ–¹å®‰è£å™¨ï¼ˆæ¨è–¦ï¼‰
```bash
# 1. è¨ªå• Modular å®˜ç¶²
curl -s https://get.modular.com | sh -

# 2. å®‰è£ Mojo
modular install mojo

# 3. è¨­å®šç’°å¢ƒè®Šæ•¸
echo 'export MODULAR_HOME="$HOME/.modular"' >> ~/.bashrc
echo 'export PATH="$MODULAR_HOME/pkg/packages.modular.com_mojo/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

# 4. é©—è­‰å®‰è£
mojo --version
```

#### æ–¹æ³• 2: MAX Platform
```bash
# 1. è¨»å†Šä¸¦ä¸‹è¼‰ MAX Platform
# 2. å®‰è£ MAX
sudo dpkg -i max-*.deb

# 3. å•Ÿå‹• MAX
max auth login

# 4. å®‰è£ Mojo
max install mojo

# 5. é©—è­‰
mojo --version
```

### é–‹ç™¼ç’°å¢ƒè¨­å®š

#### VS Code æ“´å±•
```bash
# å®‰è£ Mojo èªè¨€æ”¯æ´
code --install-extension modular-mojotools.mojo
```

#### Jupyter Notebook æ”¯æ´
```bash
# å®‰è£ Jupyter
pip install jupyter

# è¨»å†Š Mojo æ ¸å¿ƒ
max install jupyter

# å•Ÿå‹• Jupyter
jupyter notebook
```

## ğŸ› ï¸ ç’°å¢ƒæº–å‚™

### Python ä¾è³´å®‰è£
```bash
# å®‰è£çˆ¬èŸ²å¿…è¦å¥—ä»¶
pip install requests beautifulsoup4 lxml html5lib aiohttp

# å¯é¸å¥—ä»¶
pip install selenium pandas numpy
```

### é …ç›®çµæ§‹
```
mojo-crawler/
â”œâ”€â”€ crawler.mojo          # ä¸»çˆ¬èŸ²æª”æ¡ˆ
â”œâ”€â”€ utils.mojo           # å·¥å…·å‡½æ•¸
â”œâ”€â”€ config.mojo          # é…ç½®æª”æ¡ˆ
â”œâ”€â”€ requirements.txt     # Python ä¾è³´
â””â”€â”€ README.md           # èªªæ˜æ–‡ä»¶
```

## ğŸ•·ï¸ åŸºç¤çˆ¬èŸ²ç¯„ä¾‹

### 1. ç°¡å–®çš„ç¶²é çˆ¬èŸ²

```mojo
# crawler.mojo
from python import Python

def main():
    """åŸºç¤çˆ¬èŸ²ç¯„ä¾‹"""
    
    # å°å…¥ Python æ¨¡çµ„
    let requests = Python.import_module("requests")
    let bs4 = Python.import_module("bs4")
    
    # è¨­å®šè«‹æ±‚æ¨™é ­
    let headers = Python.dict()
    headers["User-Agent"] = "Mozilla/5.0 (compatible; MojoCrawler/1.0)"
    
    try:
        # ç™¼é€ HTTP è«‹æ±‚
        let url = "https://example.com"
        let response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            print("âœ… è«‹æ±‚æˆåŠŸ")
            
            # è§£æ HTML
            let soup = bs4.BeautifulSoup(response.content, "html.parser")
            
            # æå–æ¨™é¡Œ
            let title = soup.find("title")
            if title:
                print("æ¨™é¡Œ:", title.get_text())
            
            # æå–æ‰€æœ‰é€£çµ
            let links = soup.find_all("a", href=True)
            print(f"æ‰¾åˆ° {len(links)} å€‹é€£çµ")
            
            for i in range(min(5, len(links))):  # åªé¡¯ç¤ºå‰5å€‹
                let link = links[i]
                print(f"  {i+1}. {link.get_text()}: {link['href']}")
                
        else:
            print("âŒ è«‹æ±‚å¤±æ•—, ç‹€æ…‹ç¢¼:", response.status_code)
            
    except Exception as e:
        print("éŒ¯èª¤:", e)
```

### 2. JSON API çˆ¬èŸ²

```mojo
def crawl_json_api():
    """çˆ¬å– JSON API æ•¸æ“š"""
    
    let requests = Python.import_module("requests")
    let json = Python.import_module("json")
    
    let headers = Python.dict()
    headers["Accept"] = "application/json"
    headers["User-Agent"] = "MojoCrawler/1.0"
    
    try:
        # çˆ¬å– API æ•¸æ“š
        let api_url = "https://jsonplaceholder.typicode.com/posts"
        let response = requests.get(api_url, headers=headers)
        
        if response.status_code == 200:
            let data = response.json()
            print(f"ğŸ“Š ç²å–åˆ° {len(data)} ç­†æ•¸æ“š")
            
            # è™•ç†å‰3ç­†æ•¸æ“š
            for i in range(min(3, len(data))):
                let post = data[i]
                print(f"\nğŸ“ è²¼æ–‡ {i+1}:")
                print(f"  æ¨™é¡Œ: {post['title']}")
                print(f"  ç”¨æˆ¶ID: {post['userId']}")
                print(f"  å…§å®¹: {post['body'][:50]}...")
                
        else:
            print("âŒ API è«‹æ±‚å¤±æ•—")
            
    except Exception as e:
        print("éŒ¯èª¤:", e)
```

## ğŸš€ é€²éšçˆ¬èŸ²ç¯„ä¾‹

### 1. é¢å‘å°è±¡çš„çˆ¬èŸ²é¡

```mojo
from python import Python
from memory import Reference

struct WebCrawler:
    """é«˜æ€§èƒ½ç¶²é çˆ¬èŸ²"""
    var session: PythonObject
    var headers: PythonObject
    var delay: Float64
    var max_retries: Int
    
    fn __init__(inout self):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        let requests = Python.import_module("requests")
        self.session = requests.Session()
        
        # è¨­å®šæ¨™é ­
        self.headers = Python.dict()
        self.headers["User-Agent"] = "Mozilla/5.0 (compatible; MojoCrawler/2.0)"
        self.headers["Accept"] = "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        self.headers["Accept-Language"] = "zh-TW,zh;q=0.9,en;q=0.8"
        self.headers["Accept-Encoding"] = "gzip, deflate, br"
        self.headers["Connection"] = "keep-alive"
        
        self.delay = 1.0
        self.max_retries = 3
    
    fn fetch_url(self, url: String) raises -> PythonObject:
        """ç²å– URL å…§å®¹ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰"""
        let time = Python.import_module("time")
        
        for retry in range(self.max_retries):
            try:
                let response = self.session.get(
                    url, 
                    headers=self.headers, 
                    timeout=10,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:  # Too Many Requests
                    print(f"âš ï¸ è«‹æ±‚éæ–¼é »ç¹ï¼Œç­‰å¾… {(retry + 1) * 2} ç§’...")
                    time.sleep((retry + 1) * 2)
                else:
                    print(f"âŒ HTTP {response.status_code}")
                    
            except Exception as e:
                print(f"ğŸ”„ é‡è©¦ {retry + 1}/{self.max_retries}: {e}")
                if retry < self.max_retries - 1:
                    time.sleep(self.delay * (retry + 1))
        
        raise Error("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†")
    
    fn parse_html(self, html_content: PythonObject) -> PythonObject:
        """è§£æ HTML å…§å®¹"""
        let bs4 = Python.import_module("bs4")
        return bs4.BeautifulSoup(html_content, "html.parser")
    
    fn extract_data(self, soup: PythonObject) -> PythonObject:
        """æå–çµæ§‹åŒ–æ•¸æ“š"""
        let data = Python.dict()
        
        # æå–æ¨™é¡Œ
        let title = soup.find("title")
        data["title"] = title.get_text().strip() if title else "ç„¡æ¨™é¡Œ"
        
        # æå– meta æè¿°
        let meta_desc = soup.find("meta", attrs={"name": "description"})
        data["description"] = meta_desc.get("content", "") if meta_desc else ""
        
        # æå–æ‰€æœ‰æ¨™é¡Œ
        let headings = Python.list()
        for level in range(1, 7):  # h1-h6
            let tags = soup.find_all(f"h{level}")
            for i in range(len(tags)):
                let heading = tags[i]
                headings.append({
                    "level": level,
                    "text": heading.get_text().strip()
                })
        data["headings"] = headings
        
        # æå–é€£çµ
        let links = Python.list()
        let link_tags = soup.find_all("a", href=True)
        for i in range(len(link_tags)):
            let link = link_tags[i]
            links.append({
                "text": link.get_text().strip(),
                "url": link["href"]
            })
        data["links"] = links
        
        # æå–åœ–ç‰‡
        let images = Python.list()
        let img_tags = soup.find_all("img", src=True)
        for i in range(len(img_tags)):
            let img = img_tags[i]
            images.append({
                "src": img["src"],
                "alt": img.get("alt", "")
            })
        data["images"] = images
        
        return data
```

### 2. æ‰¹é‡çˆ¬èŸ²ç¯„ä¾‹

```mojo
def batch_crawl_demo():
    """æ‰¹é‡çˆ¬èŸ²ç¤ºä¾‹"""
    
    let time = Python.import_module("time")
    let json = Python.import_module("json")
    
    var crawler = WebCrawler()
    
    # è¦çˆ¬å–çš„ URL åˆ—è¡¨
    let urls = [
        "https://example.com",
        "https://httpbin.org/html",
        "https://httpbin.org/json"
    ]
    
    let results = Python.list()
    
    print("ğŸš€ é–‹å§‹æ‰¹é‡çˆ¬å–...")
    
    for i in range(len(urls)):
        let url = urls[i]
        print(f"\nğŸ“„ æ­£åœ¨è™•ç†ç¬¬ {i+1}/{len(urls)} å€‹: {url}")
        
        try:
            # ç²å–é é¢
            let response = crawler.fetch_url(url)
            
            # è§£æå…§å®¹
            if "application/json" in str(response.headers.get("content-type", "")):
                # JSON æ•¸æ“š
                let data = response.json()
                results.append({
                    "url": url,
                    "type": "json",
                    "data": data
                })
            else:
                # HTML æ•¸æ“š
                let soup = crawler.parse_html(response.content)
                let extracted_data = crawler.extract_data(soup)
                results.append({
                    "url": url,
                    "type": "html",
                    "data": extracted_data
                })
            
            print("âœ… è™•ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ è™•ç†å¤±æ•—: {e}")
            results.append({
                "url": url,
                "type": "error",
                "error": str(e)
            })
        
        # è«‹æ±‚é–“éš”
        if i < len(urls) - 1:
            time.sleep(crawler.delay)
    
    # ä¿å­˜çµæœ
    try:
        with open("crawl_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print("\nğŸ’¾ çµæœå·²ä¿å­˜åˆ° crawl_results.json")
    except Exception as e:
        print(f"ğŸ’¥ ä¿å­˜å¤±æ•—: {e}")
    
    print(f"\nğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆ! å…±è™•ç† {len(urls)} å€‹ URL")
```

## ğŸ› ï¸ å¯¦ç”¨å·¥å…·å‡½æ•¸

### 1. URL å·¥å…·

```mojo
def normalize_url(base_url: String, relative_url: String) -> String:
    """è¦ç¯„åŒ– URL"""
    let urllib = Python.import_module("urllib.parse")
    return str(urllib.urljoin(base_url, relative_url))

def is_valid_url(url: String) -> Bool:
    """æª¢æŸ¥ URL æ˜¯å¦æœ‰æ•ˆ"""
    let urllib = Python.import_module("urllib.parse")
    let parsed = urllib.urlparse(url)
    return bool(parsed.netloc and parsed.scheme)
```

### 2. æ•¸æ“šè™•ç†å·¥å…·

```mojo
def clean_text(text: PythonObject) -> String:
    """æ¸…ç†æ–‡æœ¬æ•¸æ“š"""
    let re = Python.import_module("re")
    
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    cleaned = re.sub(r'\s+', ' ', str(text))
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    cleaned = re.sub(r'[^\w\s\u4e00-\u9fff]', '', cleaned)
    
    return str(cleaned).strip()

def extract_emails(text: String) -> PythonObject:
    """æå–é›»å­éƒµä»¶åœ°å€"""
    let re = Python.import_module("re")
    let pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return re.findall(pattern, text)

def extract_phone_numbers(text: String) -> PythonObject:
    """æå–é›»è©±è™Ÿç¢¼ï¼ˆè‡ºç£æ ¼å¼ï¼‰"""
    let re = Python.import_module("re")
    let patterns = [
        r'09\d{8}',           # æ‰‹æ©Ÿè™Ÿç¢¼
        r'0\d{1,2}-\d{7,8}',  # å¸‚è©±
        r'\(\d{2,3}\)\d{7,8}' # æ‹¬è™Ÿæ ¼å¼
    ]
    
    let results = Python.list()
    for pattern in patterns:
        let matches = re.findall(pattern, text)
        results.extend(matches)
    
    return results
```

### 3. æ•¸æ“šå­˜å„²å·¥å…·

```mojo
def save_to_csv(data: PythonObject, filename: String):
    """ä¿å­˜æ•¸æ“šåˆ° CSV"""
    let pandas = Python.import_module("pandas")
    
    try:
        let df = pandas.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
    except Exception as e:
        print(f"ğŸ’¥ CSV ä¿å­˜å¤±æ•—: {e}")

def save_to_json(data: PythonObject, filename: String):
    """ä¿å­˜æ•¸æ“šåˆ° JSON"""
    let json = Python.import_module("json")
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
    except Exception as e:
        print(f"ğŸ’¥ JSON ä¿å­˜å¤±æ•—: {e}")
```

## ğŸ“‹ æœ€ä½³å¯¦è¸

### 1. å°Šé‡ robots.txt

```mojo
def check_robots_txt(base_url: String) -> Bool:
    """æª¢æŸ¥ robots.txt"""
    let urllib = Python.import_module("urllib.robotparser")
    
    let robots_url = base_url.rstrip('/') + '/robots.txt'
    let rp = urllib.RobotFileParser()
    rp.set_url(robots_url)
    
    try:
        rp.read()
        return rp.can_fetch('*', base_url)
    except:
        return True  # å¦‚æœç„¡æ³•è®€å–ï¼Œå‡è¨­å…è¨±
```

### 2. è«‹æ±‚é™åˆ¶

```mojo
struct RateLimiter:
    """è«‹æ±‚é€Ÿç‡é™åˆ¶å™¨"""
    var last_request_time: Float64
    var min_interval: Float64
    
    fn __init__(inout self, requests_per_second: Float64):
        self.last_request_time = 0.0
        self.min_interval = 1.0 / requests_per_second
    
    fn wait_if_needed(inout self):
        """å¦‚æœ‰éœ€è¦å‰‡ç­‰å¾…"""
        let time = Python.import_module("time")
        let current_time = float(time.time())
        
        let time_since_last = current_time - self.last_request_time
        if time_since_last < self.min_interval:
            let wait_time = self.min_interval - time_since_last
            time.sleep(wait_time)
        
        self.last_request_time = float(time.time())
```

### 3. éŒ¯èª¤è™•ç†

```mojo
def robust_crawl(url: String) -> PythonObject:
    """å…·æœ‰å¼·å¥éŒ¯èª¤è™•ç†çš„çˆ¬èŸ²å‡½æ•¸"""
    let requests = Python.import_module("requests")
    let time = Python.import_module("time")
    
    let max_retries = 3
    let backoff_factor = 2
    
    for attempt in range(max_retries):
        try:
            let response = requests.get(
                url,
                timeout=10,
                headers={"User-Agent": "MojoCrawler/1.0"}
            )
            
            if response.status_code == 200:
                return response
            elif response.status_code == 429:
                let wait_time = backoff_factor ** attempt
                print(f"â³ è«‹æ±‚é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
            else:
                print(f"âŒ HTTP {response.status_code}")
                
        except Exception as e:
            print(f"ğŸ”„ å˜—è©¦ {attempt + 1}: {e}")
            if attempt < max_retries - 1:
                time.sleep(backoff_factor ** attempt)
    
    raise Error("æ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—äº†")
```

## ğŸ’¡ å¸¸è¦‹å•é¡Œ

### Q: Mojo ç›¸æ¯” Python æœ‰ä»€éº¼å„ªå‹¢ï¼Ÿ

**A:** Mojo åœ¨çˆ¬èŸ²æ–¹é¢çš„å„ªå‹¢ï¼š
- **æ€§èƒ½**: æ¯” Python å¿« 10-100 å€
- **è¨˜æ†¶é«”æ•ˆç‡**: æ›´å¥½çš„è¨˜æ†¶é«”ç®¡ç†
- **ä¸¦è¡Œè™•ç†**: åŸç”Ÿæ”¯æ´ä¸¦è¡Œè¨ˆç®—
- **å…¼å®¹æ€§**: å¯ä»¥ç›´æ¥ä½¿ç”¨ Python å¥—ä»¶

### Q: å¦‚ä½•è™•ç†åçˆ¬èŸ²æ©Ÿåˆ¶ï¼Ÿ

**A:** å¸¸è¦‹ç­–ç•¥ï¼š
```mojo
# 1. éš¨æ©Ÿ User-Agent
let user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
]

# 2. ä½¿ç”¨ä»£ç†
let proxies = {
    "http": "http://proxy:port",
    "https": "https://proxy:port"
}

# 3. æ¨¡æ“¬çœŸå¯¦è¡Œç‚º
time.sleep(random.uniform(1, 3))
```

### Q: å¦‚ä½•è™•ç† JavaScript æ¸²æŸ“çš„é é¢ï¼Ÿ

**A:** ä½¿ç”¨ Seleniumï¼š
```mojo
def crawl_js_page(url: String) -> PythonObject:
    let selenium = Python.import_module("selenium")
    let webdriver = selenium.webdriver
    
    let driver = webdriver.Chrome()
    driver.get(url)
    
    # ç­‰å¾…é é¢è¼‰å…¥
    let time = Python.import_module("time")
    time.sleep(3)
    
    let content = driver.page_source
    driver.quit()
    
    return content
```

### Q: å¦‚ä½•é€²è¡Œåˆ†æ•£å¼çˆ¬èŸ²ï¼Ÿ

**A:** å¯ä»¥çµåˆ Celery æˆ– RQï¼š
```mojo
# ä»»å‹™åˆ†ç™¼
def distribute_urls(urls: PythonObject, num_workers: Int) -> PythonObject:
    let chunks = Python.list()
    let chunk_size = len(urls) // num_workers
    
    for i in range(num_workers):
        let start = i * chunk_size
        let end = start + chunk_size if i < num_workers - 1 else len(urls)
        chunks.append(urls[start:end])
    
    return chunks
```

## ğŸ¯ å®Œæ•´ç¯„ä¾‹é‹è¡Œ

```bash
# 1. å‰µå»ºé …ç›®ç›®éŒ„
mkdir mojo-crawler && cd mojo-crawler

# 2. å‰µå»ºä¸¦é‹è¡Œçˆ¬èŸ²
echo '# ä¸Šé¢çš„å®Œæ•´ä»£ç¢¼' > crawler.mojo
mojo crawler.mojo

# 3. æŸ¥çœ‹çµæœ
cat crawl_results.json
```

## ğŸ“š é€²éšå­¸ç¿’è³‡æº

- [Mojo å®˜æ–¹æ–‡æª”](https://docs.modular.com/mojo/)
- [Modular é–‹ç™¼è€…ç¤¾ç¾¤](https://discord.gg/modular)
- [Mojo GitHub ç¯„ä¾‹](https://github.com/modularml/mojo)

---

**æ³¨æ„**: è«‹å‹™å¿…éµå®ˆç›®æ¨™ç¶²ç«™çš„ä½¿ç”¨æ¢æ¬¾å’Œ robots.txt è¦å‰‡ï¼Œé€²è¡Œåˆç†åˆæ³•çš„æ•¸æ“šæ”¶é›†ã€‚