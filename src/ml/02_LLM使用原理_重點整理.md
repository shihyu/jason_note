# LLM 使用原理 - 重點整理與補充說明

## 目錄
1. [LLM 模型原理](#1-llm-模型原理)
2. [LLM 使用原理](#2-llm-使用原理)
3. [LLM 使用原則](#3-llm-使用原則)
4. [新一代 LLM 功能](#4-新一代-llm-功能)
5. [工具使用](#5-工具使用)
6. [多模態能力](#6-多模態能力)

---

## OpenAI 5 AGI 階段

| 階段 | 類型 | 說明 |
|------|------|------|
| Level 1 | 對話式 AI (Chatbot) | 基本聊天功能 |
| Level 2 | 推理式 AI (Reasoner) | 具備邏輯推理能力 |
| Level 3 | 自動化 AI (Agent) | 自主執行任務 |
| Level 4 | 創新化 AI (Innovator) | 具備創新思維 |
| Level 5 | 組織化 AI (Organization) | 完整組織運作能力 |

**補充說明：** 這是 OpenAI 對 AGI (通用人工智慧) 發展的五階段路線圖。目前大多數商用 LLM 處於 Level 1-2，部分最新模型如 GPT-4o 和 Claude 3.5 已開始展現 Level 3 的能力。

---

## 1. LLM 模型原理

### 1.1 數據收集與處理

```
下載全世界資料 → 清理整理 → 純文字呈現
```

**流程：**
- 盡辦法下載全世界所有文字資料
- 整理、去重、去雜訊、去標籤、去識別
- 將資料以純文字方式呈現

**補充說明：** 這個過程涉及爬取網頁、書籍、論文、程式碼等。數據量通常達到 PB (Petabyte) 級別。清理過程非常重要，會影響模型品質。

---

### 1.2 Tokenization (分詞)

**核心概念：**
- 將文字轉換成一系列符號 (Token)
- 從 256 個 token 開始（Unicode Byte）
- 使用 **BPE (Byte Pair Encoding)** 方式增加新 token，直到目標止


**補充說明：**

```python
# BPE 範例
原始文字: "LLM is great"
Token化後: ["LL", "M", " is", " great"]
Token IDs: [9188, 44, 318, 1049]
```

BPE 的原理：
1. 統計字元對出現頻率
2. 合併最常出現的字元對
3. 重複直到達到目標詞彙量（通常 32K-100K tokens）

不同語言的效率不同：英文約 1 token = 4 字元，中文約 1 token = 1-2 字元。

---

### 1.3 預訓練 (Pre-training) - 失真壓縮

**核心類比：**
- 就像將圖片 RAW 壓縮成 JPEG
- 大小會變小，會失去一些訊號
- 但盡量保留圖片本身的資訊
- LLM 使用的壓縮方式為「深度學習神經網路」

**壓縮結果：**
- 原來上千 PB 的文字 → 壓縮成數十億至數千億的機率分佈「參數」
- 模型大小約 1TB（以 GPT-4 級別為例）
- 這就是「模型」本身，有預先儲存的知識

**重要限制：**
- **知識截止日期 (Knowledge Cutoff)**：壓縮之後產生的資料，模型並不知道
- 通常幾個月才能做一次資料更新，非常花錢且花時間
- 解決方案：Post-training（SFT、RLHF、RL 等方法）

**補充說明：** 預訓練的目標是學習「下一個 token 預測」（Next Token Prediction）。模型學習的是語言的統計規律，而非真正的「理解」。這也是為什麼 LLM 會產生幻覺（Hallucination）的原因之一。

---

### 1.4 如果 ChatGPT 是一個人

> "我的大小為 1TB，我的所有知識來自網路。我花了 6 個月閱讀並消化了所有網路資料。我現在只知道大概，無法詳細記住。我的語氣是後訓練出來的，我本身沒個性。我的知識在你問的時候有點過時又模糊。我回答時會用猜的，比較多人討論的知識我會記得較清楚，反之不清楚。如果要我學新知識，我的舊知識也要全部再重新讀一次，除非你讓我可以用工具。"

**重點提示：**
1. 知識是壓縮且模糊的
2. 人格是後訓練的結果
3. 知識會過時
4. 回答基於機率而非確定性
5. 熱門話題記得清楚，冷門話題容易出錯
6. 更新知識代價高昂

---

## 2. LLM 使用原理

### 2.1 對話機制

```
用戶輸入 → Token 序列 → LLM 處理 → Token 序列 → 文字輸出
```

**流程說明：**
1. 在 LLM 中輸入文字
2. 文字被轉換成序列 token
3. 回答也是序列 tokens
4. 將完整對話附加在後面

**補充說明：** 每次對話，LLM 都會看到完整的對話歷史，這是為什麼它能「記住」之前說過的話。但這也意味著長對話會消耗更多 token。

---

### 2.2 一輪聊天的結構

**Token 序列結構：**
```
[系統提示] + [用戶輸入] + [Enter] + [LLM回答] + [<eos>] + [用戶輸入] + ...
```

**關鍵概念：**
- 使用者輸入 Enter 代表使用者結束
- LLM 產生 `<eos>` (End of Sequence) 代表 LLM 輸出結束
- 整個 token 序列被視為「**工作記憶體**」
- 記憶體長度稱為 **Context Window**

**補充說明：**
- GPT-4 Turbo: 128K tokens
- Claude 3: 200K tokens
- Gemini 1.5: 1M-2M tokens

Context Window 越大，能處理的對話歷史和文件就越長，但成本也越高。

---

## 3. LLM 使用原則

### 實用建議

| 原則 | 說明 |
|------|------|
| **多產品測試** | 在多個產品中測試，找出最適合你需求的 |
| **選擇付費方案** | 免費版功能越來越受限，推薦 ChatGPT Plus (20美元/月) |
| **查詢穩定資訊** | 已經不會變的資訊，就直接查 |
| **建立新 Session** | 話題結束後建立新 Chat Session，結果更正確也更快 |
| **使用最新模型** | 盡量使用最新的 Model |
| **不一定要最強** | 根據任務選擇適合的模型，不必總是用最強的 |

**補充說明：**

1. **為什麼新 Session 更好？**
   - 避免上下文汙染
   - 減少 token 消耗
   - 模型回答更聚焦

2. **模型選擇策略：**
   - 簡單問答：GPT-3.5 / Claude Haiku（快速便宜）
   - 複雜推理：GPT-4 / Claude Sonnet（平衡）
   - 專業任務：GPT-4o / Claude Opus（最強）

---

## 4. 新一代 LLM 功能

### 4.1 舊款自迴歸模型

**Auto-regressive 名稱由來：**
- **Regression**：回復、退回
- **Linear Regression**：預測新資料是否回到先前資料集的分佈
- **Regressive 模型**：在機率分佈中，新資料是否回到先前資料集建立的分佈
- **Auto-regressive**：用自己來預測是否有迴歸

**補充說明：** 自迴歸模型的核心是「用已生成的 token 預測下一個 token」。這是一種串行的生成過程，每次只生成一個 token。

---

### 4.2 舊模型的限制與演進

**舊模型問題：**
- 單純不斷利用自己產生新的 token，直到出現 `<eos>` 才結束（文字接龍）
- 利用 Few-shot 方式套出預先壓縮知識
- 使用 COT (Chain of Thought) 方式套出知識
- 如果有錯就更改提示詞（Try and Error）

**演進方向：**
- 如果有一個機制，讓模型不斷用 Try and Error 的方式找出正確答案
- **RL (Reinforcement Learning)** 的介入：Agent → Action → Observation → Reward

---

### 4.3 讓 LLM 思考出答案

**人類解題過程：**
1. 瞭解問題之後，會先試一個方法
2. 不行會重新定向，再換一個方法
3. 如此反覆循環，不斷試驗中間試過的方法
4. 列出思考的過程一一實驗

**訓練會思考的 LLM：**
- 將思考獲得正確答案的完整過程丟給 LLM 作為訓練資料
- 可以得到一個會思考的 LLM
- **Deepseek R1** 的論文首次揭露完整過程

**補充說明：** 這種方法稱為「Process Reward Model」，不只獎勵正確答案，還獎勵正確的思考過程。這是從「結果導向」轉變為「過程導向」的重大突破。

---

### 4.4 LLM 訓練階段

| 階段 | 目的 | 說明 |
|------|------|------|
| **預訓練 (Pre-training)** | 獲得基本知識的概貌 | 學習語言規律 |
| **SFT (Supervised Fine-Tuning)** | 增加新的知識或能力 | 特定任務微調 |
| **RLHF (RL from Human Feedback)** | 讓輸出貼近人類要求 | 對齊人類價值觀 |
| **RL (Reinforcement Learning)** | 讓 LLM 學會人類解題方式 | 學習推理過程 |
| **DRPO** | LLM 學會思考 | 深度推理優化 |

**補充說明：**
- **SFT**：使用標註數據進行監督學習
- **RLHF**：結合人類偏好進行強化學習
- **RL**：使用獎勵信號優化策略
- **DRPO (Direct Reward Policy Optimization)**：更直接的獎勵優化方法

---

### 4.5 會思考的模型

**發展歷程：**
- 首先在 ChatGPT 的 o 開頭模型中出現（o1, o1-mini）
- **Deepseek R1** 提出完整論文
- 將此思考能力稱為 **Aha-Moment**

**思考模型的流程：**
```
1. 生成 <think> token → 開始思考
2. 生成解題步驟
3. 將步驟送回 LLM 產生新內容
4. 不斷循環
5. 輸出 </think> 結束思考
6. 將結果放入 context window
7. 產生最終回答直到 <eos>
```

**特點：**
- 非常花 token（思考過程也消耗 token）
- 也很花時間
- **付費模型才會有**

**補充說明：** 思考模型的核心創新是「讓模型在回答前先思考」。這類似於人類的 System 2 思考（慢思考），而非傳統 LLM 的 System 1（快思考）。

---

## 5. 工具使用

### 5.1 記憶機制

**Context Window 特性：**
- 整個 token 序列被視為「工作記憶體」
- 有大小限制
- 超過限制需要壓縮或截斷

**補充說明：** 這是 LLM 的短期記憶。長期記憶需要透過外部存儲（如向量資料庫）實現。

---

### 5.2 網路搜尋

**為什麼需要網路搜尋？**
- 比較 3.11 和 3.9 哪個大，直接文字接龍失敗
- 用思考模型思考解題過程可行但慢
- 直接用計算機或寫程式又快又準
- **即時知識**無法透過預訓練獲得

**解決方案演進：**
- SFT：不定時加入新知識（慢）
- RAG：轉換向量資料庫（複雜）
- **網路搜尋**：直接上網搜尋，把結果丟回 context window（最快最準）

**補充說明：** 這就是 ChatGPT Plus 的 Browse with Bing、Claude 的 Web Search 等功能的原理。

---

### 5.3 工具使用機制

**工具調用流程：**
```
[用戶提問] → [LLM 回答] → [LLM 生成 <tool> token]
    → [暫停 LLM] → [呼叫外部資源] → [獲得結果]
    → [放入 <output></output></tool> 區段]
    → [繼續 LLM 輸出直到 <eos>]
```

**核心能力：**
- LLM 具有呼叫多種工具的能力
- 可以串接這些工具
- 形成工具鏈 (Tool Chain)

**補充說明：** 這是 Agent 的基礎能力。常見工具包括：
- 計算器
- 程式碼執行器
- 網路搜尋
- 文件讀取
- API 調用

---

### 5.4 深度研究 (Deep Research)

**功能展示：**

輸入：「幫我做增肌減脂的調查及計劃」

自動執行：
- 開啟網頁
- 讀取 PDF
- 執行程式
- 執行計算

輸出：
- 完整的論文
- 報告
- 問卷調查

**技術本質：**
- AI Agent 的基礎
- 先使用工具完成多模態資料讀入和消化
- 最後排版輸出
- 多個 LLM 產品提供這個服務
- **大部分需要付費**

**補充說明：** Deep Research 已經具備基本 Agent 功能，能夠：
1. 自主規劃任務
2. 選擇合適工具
3. 執行多步驟操作
4. 整合結果輸出

---

### 5.5 檔案上傳功能

**功能特點：**
- LLM 具有多模態功能，可在文字之間互轉
- 讀取的圖片也被轉成 token 放入 context window
- 支援：PDF、docx、pptx 等
- **可利用檔案上傳增加 LLM 外部知識，而不一定要經過微調**

**補充說明：** 這是一種輕量級的知識注入方式，適合：
- 臨時文件分析
- 特定文檔問答
- 不需要永久學習的場景

缺點：每次對話都要重新上傳，且受 context window 限制。

---

### 5.6 Python 解譯器功能

**能力突破：**
- 可呼叫瀏覽器，即可呼叫其它工具
- 上一代 LLM 是直接產生 token，問題很多
- 同樣的問題經過深度思考也不見得有答案（如數學、程式設計）
- **為何不直接呼叫計算機、程式編輯器來執行？**
- LLM 開始具備自動執行其它程式，甚至是其它 LLM 的能力

**補充說明：** 這是 LLM 從「思考者」變成「執行者」的關鍵一步。程式碼解譯器讓 LLM 能夠：
- 驗證數學計算
- 執行數據分析
- 生成視覺化圖表
- 測試程式碼

---

## 6. 多模態能力

### 6.1 Vibe Coding 開發

**概念：**
- 沉浸式開發，由 **Andrej Karpathy** 提出
- 使用直接詢問的方式寫程式，由 AI 完成程式碼
- 使用者只是提出需求
- **讓程式設計師變成產品工程師**

**必要條件：**
- 支援 AI 的編輯器（VSCode, Cursor）
- 支援的 LLM（GPT-4o, Claude 3.7 Sonnet 等）
- 編輯器會直接編寫程式、安裝工具、除錯、執行

**常用工具：**
- **Cursor AI**：專為 AI 輔助設計的 IDE
- **Copilot**：GitHub 的 AI 配對程式設計工具
- **Colab AI**：Google 的 Notebook AI 輔助功能

**補充說明：** Vibe Coding 的核心理念是「描述你想要什麼，而不是怎麼做」。這代表程式設計的範式轉移，從「寫程式碼」到「描述意圖」。

---

### 6.2 語音 Token

**文字 Token：**
- 訓練 LLM 前，用語料庫利用 BPE 方式找出基礎詞彙表
- 數量：數萬到數十萬個

**語音 Token：**
- 利用**梅爾頻譜 (Mel Spectrogram)**
- 找到音素級或字元級的發音 Token

**補充說明：** 語音模型如 Whisper、VALL-E 等使用類似的 tokenization 方法。語音 token 可以：
- 表示音素 (Phoneme)
- 表示聲學特徵
- 支持語音生成和識別

---

### 6.3 影像及影片 (視覺)

**視覺 Token 化：**
- 圖片被分割成 patches（小塊）
- 每個 patch 轉換為 token
- 這些 token 與文字 token 一起處理

**補充說明：**
- **Vision Transformer (ViT)**：將圖片分成 16x16 的 patches
- **GPT-4V**、**Claude 3 Vision**：支持圖片理解
- **Gemini**：原生多模態設計

影片處理：
- 抽取關鍵幀
- 每幀作為圖片處理
- 理解時序資訊

---

## 總結

### LLM 的本質
1. **知識壓縮機**：將海量文字壓縮成參數
2. **機率模型**：基於統計預測下一個 token
3. **工具使用者**：能夠調用外部工具增強能力
4. **多模態處理器**：處理文字、圖片、語音、影片

### 使用建議
1. **理解限制**：知識截止、幻覺、token 限制
2. **善用工具**：搜尋、計算、程式執行
3. **選擇適合模型**：平衡成本和效能
4. **新 Session 策略**：保持對話聚焦
5. **付費方案**：獲得更多功能和更好體驗

### 未來趨勢
1. **思考模型**：從快思考到慢思考
2. **Agent 能力**：自主規劃和執行
3. **多模態融合**：統一處理所有模態
4. **工具生態**：更豐富的外部工具整合
5. **程式設計範式轉移**：從寫程式碼到描述意圖

---

## 實用資源

- **ChatGPT Plus**：20 美元/月
- **Cursor IDE**：AI 輔助程式開發
- **Deepseek R1 論文**：思考模型的技術細節
- **OpenAI API**：程式化調用 LLM

---

*本文檔整理自「LLM使用原理」課程投影片，並補充相關技術說明和實用建議。*
