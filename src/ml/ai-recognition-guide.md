# AI èªéŸ³èˆ‡å½±åƒè¾¨è­˜æŠ€è¡“æŒ‡å—

## ğŸ“‹ ç›®éŒ„
- [èªéŸ³è¾¨è­˜æŠ€è¡“](#èªéŸ³è¾¨è­˜æŠ€è¡“)
- [å½±åƒè¾¨è­˜æŠ€è¡“](#å½±åƒè¾¨è­˜æŠ€è¡“)
- [å¤šæ¨¡æ…‹æ‡‰ç”¨](#å¤šæ¨¡æ…‹æ‡‰ç”¨)
- [å®‰è£æŒ‡å—](#å®‰è£æŒ‡å—)
- [å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹](#å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹)

---

## ğŸ¤ èªéŸ³è¾¨è­˜æŠ€è¡“

### 1. OpenAI Whisperï¼ˆæ¨è–¦ï¼‰

**ç‰¹é»ï¼š**
- é›¢ç·šé‹è¡Œï¼Œä¿è­·éš±ç§
- æ”¯æ´ 99 ç¨®èªè¨€
- æº–ç¢ºåº¦æ¥µé«˜
- å…è²»é–‹æº

**å®‰è£ï¼š**
```bash
pip install openai-whisper
```

**åŸºæœ¬ä½¿ç”¨ï¼š**
```python
import whisper

# è¼‰å…¥æ¨¡å‹ (tiny, base, small, medium, large)
model = whisper.load_model("base")

# è¾¨è­˜éŸ³æª”
result = model.transcribe("audio.mp3")
print(result["text"])

# æ”¯æ´ä¸­æ–‡
result = model.transcribe("chinese_audio.mp3", language="zh")
print(result["text"])

# å–å¾—æ™‚é–“æˆ³è¨˜
result = model.transcribe("audio.mp3", verbose=True)
for segment in result["segments"]:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] {segment['text']}")
```

### 2. Google Speech-to-Text

**ç‰¹é»ï¼š**
- é›²ç«¯æœå‹™ï¼Œæº–ç¢ºåº¦é«˜
- æ”¯æ´å³æ™‚ä¸²æµ
- è‡ªå‹•æ¨™é»ç¬¦è™Ÿ

**å®‰è£ï¼š**
```bash
pip install google-cloud-speech
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from google.cloud import speech
import io

def transcribe_file(speech_file):
    """è½‰éŒ„æœ¬åœ°éŸ³æª”"""
    client = speech.SpeechClient()

    with io.open(speech_file, "rb") as audio_file:
        content = audio_file.read()

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="zh-TW",  # ç¹é«”ä¸­æ–‡
        enable_automatic_punctuation=True,
    )

    response = client.recognize(config=config, audio=audio)
    
    for result in response.results:
        print(f"è½‰éŒ„çµæœ: {result.alternatives[0].transcript}")
        print(f"ä¿¡å¿ƒåˆ†æ•¸: {result.alternatives[0].confidence}")
```

### 3. å³æ™‚èªéŸ³è¾¨è­˜

**å®‰è£ï¼š**
```bash
pip install SpeechRecognition pyaudio
```

**å³æ™‚éº¥å…‹é¢¨è¾¨è­˜ï¼š**
```python
import speech_recognition as sr

def live_speech_recognition():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    
    print("èª¿æ•´ç’°å¢ƒå™ªéŸ³...")
    with mic as source:
        recognizer.adjust_for_ambient_noise(source, duration=1)
    
    print("é–‹å§‹èªªè©±...")
    
    while True:
        try:
            with mic as source:
                # è¨­å®šè¶…æ™‚æ™‚é–“
                audio = recognizer.listen(source, timeout=1, phrase_time_limit=5)
                
            # ä½¿ç”¨ Google APIï¼ˆå…è²»ï¼‰
            text = recognizer.recognize_google(audio, language="zh-TW")
            print(f"ä½ èªª: {text}")
            
            # ä¹Ÿå¯ä»¥ä½¿ç”¨ Whisper
            # text = recognizer.recognize_whisper(audio, language="chinese")
            
        except sr.UnknownValueError:
            pass  # ç„¡æ³•è¾¨è­˜
        except sr.RequestError as e:
            print(f"éŒ¯èª¤: {e}")
        except KeyboardInterrupt:
            print("\nåœæ­¢è¾¨è­˜")
            break

if __name__ == "__main__":
    live_speech_recognition()
```

---

## ğŸ“· å½±åƒè¾¨è­˜æŠ€è¡“

### 1. YOLO v8ï¼ˆç‰©ä»¶åµæ¸¬ï¼‰

**ç‰¹é»ï¼š**
- å³æ™‚åµæ¸¬
- é«˜æº–ç¢ºåº¦
- æ”¯æ´å½±ç‰‡ä¸²æµ

**å®‰è£ï¼š**
```bash
pip install ultralytics
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from ultralytics import YOLO
import cv2

# è¼‰å…¥é è¨“ç·´æ¨¡å‹
model = YOLO('yolov8n.pt')  # n=nano, s=small, m=medium, l=large, x=extra large

# åœ–ç‰‡åµæ¸¬
def detect_image(image_path):
    results = model(image_path)
    
    for r in results:
        boxes = r.boxes
        for box in boxes:
            # å–å¾—åº§æ¨™
            x1, y1, x2, y2 = box.xyxy[0]
            # å–å¾—é¡åˆ¥å’Œä¿¡å¿ƒåˆ†æ•¸
            conf = box.conf[0]
            cls = box.cls[0]
            print(f"åµæ¸¬åˆ°: {model.names[int(cls)]} (ä¿¡å¿ƒåº¦: {conf:.2f})")
    
    # å„²å­˜çµæœ
    results[0].save(filename='result.jpg')

# å½±ç‰‡å³æ™‚åµæ¸¬
def detect_video(video_path):
    cap = cv2.VideoCapture(video_path)  # æˆ–ä½¿ç”¨ 0 ç‚ºæ”å½±æ©Ÿ
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        results = model(frame)
        annotated_frame = results[0].plot()
        
        cv2.imshow('YOLOv8 åµæ¸¬', annotated_frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
```

### 2. Transformers å½±åƒåˆ†é¡

**å®‰è£ï¼š**
```bash
pip install transformers torch pillow
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from transformers import pipeline
from PIL import Image

# å»ºç«‹åˆ†é¡å™¨
classifier = pipeline("image-classification", 
                     model="google/vit-base-patch16-224")

def classify_image(image_path):
    image = Image.open(image_path)
    results = classifier(image)
    
    print("å½±åƒåˆ†é¡çµæœ:")
    for item in results[:5]:  # é¡¯ç¤ºå‰5å€‹çµæœ
        print(f"  {item['label']}: {item['score']:.3f}")
    
    return results

# ç‰©ä»¶åµæ¸¬
detector = pipeline("object-detection", 
                   model="facebook/detr-resnet-50")

def detect_objects(image_path):
    image = Image.open(image_path)
    results = detector(image)
    
    print("åµæ¸¬åˆ°çš„ç‰©ä»¶:")
    for item in results:
        print(f"  {item['label']}: {item['score']:.2f}")
        print(f"    ä½ç½®: {item['box']}")
    
    return results
```

### 3. è‡‰éƒ¨è¾¨è­˜

**å®‰è£ï¼š**
```bash
pip install face-recognition opencv-python
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
import face_recognition
import cv2
import numpy as np

def face_detection_and_recognition():
    # è¼‰å…¥å·²çŸ¥äººè‡‰
    known_image = face_recognition.load_image_file("person1.jpg")
    known_encoding = face_recognition.face_encodings(known_image)[0]
    
    known_face_encodings = [known_encoding]
    known_face_names = ["Person 1"]
    
    # é–‹å•Ÿæ”å½±æ©Ÿ
    video_capture = cv2.VideoCapture(0)
    
    while True:
        ret, frame = video_capture.read()
        
        # è½‰æ› BGR (OpenCV) åˆ° RGB (face_recognition)
        rgb_frame = frame[:, :, ::-1]
        
        # æ‰¾å‡ºæ‰€æœ‰è‡‰éƒ¨ä½ç½®å’Œç·¨ç¢¼
        face_locations = face_recognition.face_locations(rgb_frame)
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        
        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
            # æ¯”å°è‡‰éƒ¨
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"
            
            # è¨ˆç®—è·é›¢æ‰¾å‡ºæœ€ä½³åŒ¹é…
            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
            best_match_index = np.argmin(face_distances)
            if matches[best_match_index]:
                name = known_face_names[best_match_index]
            
            # ç•«æ¡†å’Œæ¨™ç±¤
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
            cv2.putText(frame, name, (left, top - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)
        
        cv2.imshow('Video', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    video_capture.release()
    cv2.destroyAllWindows()
```

---

## ğŸ”„ å¤šæ¨¡æ…‹æ‡‰ç”¨

### 1. CLIP - åœ–æ–‡åŒ¹é…

**å®‰è£ï¼š**
```bash
pip install transformers torch pillow
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def image_text_similarity(image_path, text_descriptions):
    """è¨ˆç®—åœ–ç‰‡èˆ‡æ–‡å­—æè¿°çš„ç›¸ä¼¼åº¦"""
    image = Image.open(image_path)
    
    # è™•ç†è¼¸å…¥
    inputs = processor(
        text=text_descriptions, 
        images=image, 
        return_tensors="pt", 
        padding=True
    )
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)
    
    # é¡¯ç¤ºçµæœ
    for i, (desc, prob) in enumerate(zip(text_descriptions, probs[0])):
        print(f"{desc}: {prob:.2%}")
    
    # å›å‚³æœ€å¯èƒ½çš„æè¿°
    max_idx = probs.argmax()
    return text_descriptions[max_idx]

# ä½¿ç”¨ç¯„ä¾‹
descriptions = [
    "ä¸€éš»è²“åœ¨ç¡è¦º",
    "ä¸€éš»ç‹—åœ¨ç©çƒ",
    "ä¸€å€‹äººåœ¨è·‘æ­¥",
    "ä¸€è¼›è»Šåœ¨è·¯ä¸Š"
]

best_match = image_text_similarity("test_image.jpg", descriptions)
print(f"\næœ€ä½³åŒ¹é…: {best_match}")
```

### 2. å½±ç‰‡ç†è§£ï¼ˆçµåˆèªéŸ³å’Œè¦–è¦ºï¼‰

```python
import whisper
import cv2
from ultralytics import YOLO
import numpy as np

class VideoAnalyzer:
    def __init__(self):
        self.whisper_model = whisper.load_model("base")
        self.yolo_model = YOLO('yolov8n.pt')
    
    def analyze_video(self, video_path):
        """å®Œæ•´åˆ†æå½±ç‰‡å…§å®¹"""
        # æå–éŸ³è¨Šä¸¦è½‰éŒ„
        audio_text = self.transcribe_audio(video_path)
        
        # åˆ†æè¦–è¦ºå…§å®¹
        visual_summary = self.analyze_visual(video_path)
        
        return {
            "audio_transcript": audio_text,
            "visual_summary": visual_summary
        }
    
    def transcribe_audio(self, video_path):
        """æå–ä¸¦è½‰éŒ„éŸ³è¨Š"""
        # ä½¿ç”¨ ffmpeg æå–éŸ³è¨Šï¼ˆéœ€è¦å…ˆå®‰è£ ffmpegï¼‰
        import subprocess
        audio_path = "temp_audio.wav"
        subprocess.run([
            "ffmpeg", "-i", video_path, 
            "-vn", "-acodec", "pcm_s16le", 
            "-ar", "16000", "-ac", "1", 
            audio_path, "-y"
        ])
        
        result = self.whisper_model.transcribe(audio_path)
        return result["text"]
    
    def analyze_visual(self, video_path, sample_rate=30):
        """åˆ†æè¦–è¦ºå…§å®¹"""
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        detected_objects = {}
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ¯ sample_rate å¹€åˆ†æä¸€æ¬¡
            if frame_count % sample_rate == 0:
                results = self.yolo_model(frame)
                
                for r in results:
                    for box in r.boxes:
                        cls = int(box.cls[0])
                        label = self.yolo_model.names[cls]
                        
                        if label not in detected_objects:
                            detected_objects[label] = 0
                        detected_objects[label] += 1
            
            frame_count += 1
        
        cap.release()
        return detected_objects

# ä½¿ç”¨ç¯„ä¾‹
analyzer = VideoAnalyzer()
results = analyzer.analyze_video("sample_video.mp4")
print("èªéŸ³å…§å®¹:", results["audio_transcript"])
print("è¦–è¦ºå…§å®¹:", results["visual_summary"])
```

---

## ğŸ“¦ å®‰è£æŒ‡å—

### åŸºç¤ç’°å¢ƒè¨­å®š

```bash
# å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv ai_recognition_env
source ai_recognition_env/bin/activate  # Linux/Mac
# æˆ–
ai_recognition_env\Scripts\activate  # Windows

# å‡ç´š pip
pip install --upgrade pip
```

### å®Œæ•´å®‰è£å¥—ä»¶

```bash
# èªéŸ³è¾¨è­˜å¥—ä»¶
pip install openai-whisper
pip install SpeechRecognition
pip install pyaudio  # å¯èƒ½éœ€è¦é¡å¤–å®‰è£ portaudio

# å½±åƒè¾¨è­˜å¥—ä»¶
pip install ultralytics  # YOLO
pip install transformers  # Hugging Face models
pip install torch torchvision  # PyTorch
pip install opencv-python  # OpenCV
pip install face-recognition  # è‡‰éƒ¨è¾¨è­˜

# å·¥å…·å¥—ä»¶
pip install pillow  # åœ–ç‰‡è™•ç†
pip install numpy  # æ•¸å€¼é‹ç®—
pip install matplotlib  # è¦–è¦ºåŒ–
```

### Docker å®¹å™¨è¨­å®š

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    wget \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£ Python å¥—ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

---

## ğŸ’¡ å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹

### 1. æ™ºæ…§æœƒè­°ç³»çµ±

**åŠŸèƒ½ï¼š**
- å³æ™‚èªéŸ³è½‰æ–‡å­—ç´€éŒ„
- ç™¼è¨€è€…è­˜åˆ¥
- é‡é»æ‘˜è¦ç”Ÿæˆ

```python
class SmartMeetingSystem:
    def __init__(self):
        self.whisper_model = whisper.load_model("medium")
        self.speakers = {}
    
    def process_meeting(self, audio_file):
        # è½‰éŒ„æœƒè­°å…§å®¹
        result = self.whisper_model.transcribe(
            audio_file,
            language="zh",
            verbose=True
        )
        
        # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜çš„é€å­—ç¨¿
        transcript = []
        for segment in result["segments"]:
            transcript.append({
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"],
                "speaker": self.identify_speaker(segment)  # éœ€å¯¦ä½œ
            })
        
        return transcript
```

### 2. æ™ºæ…§å®‰é˜²ç³»çµ±

**åŠŸèƒ½ï¼š**
- äººè‡‰è­˜åˆ¥é–€ç¦
- ç•°å¸¸è¡Œç‚ºåµæ¸¬
- å³æ™‚è­¦å ±é€šçŸ¥

```python
class SecuritySystem:
    def __init__(self):
        self.yolo_model = YOLO('yolov8x.pt')
        self.known_faces = self.load_known_faces()
        self.alert_actions = ['fighting', 'falling', 'running']
    
    def monitor_camera(self, camera_id):
        cap = cv2.VideoCapture(camera_id)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # ç‰©ä»¶å’Œè¡Œç‚ºåµæ¸¬
            results = self.yolo_model(frame)
            
            # æª¢æŸ¥ç•°å¸¸è¡Œç‚º
            for r in results:
                for box in r.boxes:
                    label = self.yolo_model.names[int(box.cls[0])]
                    if label in self.alert_actions:
                        self.send_alert(f"åµæ¸¬åˆ°ç•°å¸¸è¡Œç‚º: {label}")
            
            # äººè‡‰è­˜åˆ¥
            faces = self.detect_faces(frame)
            for face in faces:
                if not self.is_authorized(face):
                    self.send_alert("åµæ¸¬åˆ°æœªæˆæ¬Šäººå“¡")
```

### 3. ç„¡éšœç¤™è¼”åŠ©å·¥å…·

**åŠŸèƒ½ï¼š**
- ç‚ºè¦–éšœè€…æè¿°ç’°å¢ƒ
- æ–‡å­—è½‰èªéŸ³
- æ‰‹èªç¿»è­¯

```python
class AccessibilityAssistant:
    def __init__(self):
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.yolo_model = YOLO('yolov8n.pt')
    
    def describe_scene(self, image_path):
        """ç‚ºè¦–éšœè€…æè¿°å ´æ™¯"""
        image = Image.open(image_path)
        
        # åµæ¸¬ç‰©ä»¶
        results = self.yolo_model(image)
        objects = []
        for r in results:
            for box in r.boxes:
                label = self.yolo_model.names[int(box.cls[0])]
                objects.append(label)
        
        # ç”Ÿæˆå ´æ™¯æè¿°
        description = f"å ´æ™¯ä¸­åŒ…å«: {', '.join(set(objects))}"
        
        # ä½¿ç”¨ CLIP ç²å–æ›´è©³ç´°çš„æè¿°
        scene_types = [
            "å®¤å…§å ´æ™¯", "å®¤å¤–å ´æ™¯", "è¡—é“", "å…¬åœ’", 
            "è¾¦å…¬å®¤", "å®¶åº­ç’°å¢ƒ", "å•†åº—"
        ]
        
        inputs = self.clip_processor(
            text=scene_types, 
            images=image, 
            return_tensors="pt", 
            padding=True
        )
        
        outputs = self.clip_model(**inputs)
        probs = outputs.logits_per_image.softmax(dim=1)
        best_scene = scene_types[probs.argmax()]
        
        description += f"ï¼Œé€™ä¼¼ä¹æ˜¯ä¸€å€‹{best_scene}"
        
        return description
```

### 4. å…§å®¹å‰µä½œåŠ©æ‰‹

**åŠŸèƒ½ï¼š**
- è‡ªå‹•ç”Ÿæˆå½±ç‰‡å­—å¹•
- å…§å®¹æ¨™ç±¤å»ºè­°
- ç²¾å½©ç‰‡æ®µæ“·å–

```python
class ContentCreatorAssistant:
    def __init__(self):
        self.whisper_model = whisper.load_model("base")
        self.yolo_model = YOLO('yolov8n.pt')
    
    def generate_subtitles(self, video_path, output_srt):
        """ç”Ÿæˆ SRT å­—å¹•æª”"""
        result = self.whisper_model.transcribe(video_path)
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result["segments"], 1):
                # SRT æ ¼å¼
                f.write(f"{i}\n")
                f.write(f"{self.format_time(segment['start'])} --> {self.format_time(segment['end'])}\n")
                f.write(f"{segment['text'].strip()}\n\n")
    
    def format_time(self, seconds):
        """è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{seconds:06.3f}".replace('.', ',')
    
    def suggest_tags(self, video_path, num_frames=10):
        """å»ºè­°å½±ç‰‡æ¨™ç±¤"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_interval = total_frames // num_frames
        
        all_objects = {}
        
        for i in range(0, total_frames, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                continue
            
            results = self.yolo_model(frame)
            for r in results:
                for box in r.boxes:
                    label = self.yolo_model.names[int(box.cls[0])]
                    all_objects[label] = all_objects.get(label, 0) + 1
        
        cap.release()
        
        # æ’åºä¸¦å›å‚³æœ€å¸¸å‡ºç¾çš„æ¨™ç±¤
        sorted_tags = sorted(all_objects.items(), key=lambda x: x[1], reverse=True)
        return [tag for tag, _ in sorted_tags[:10]]
```

---

## ğŸ“š åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡æª”
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Ultralytics YOLOv8](https://docs.ultralytics.com/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Google Cloud Speech-to-Text](https://cloud.google.com/speech-to-text/docs)

### æ•™å­¸è³‡æº
- [PyTorch å®˜æ–¹æ•™å­¸](https://pytorch.org/tutorials/)
- [OpenCV Python æ•™å­¸](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)
- [TensorFlow.js ç¯„ä¾‹](https://www.tensorflow.org/js/demos)

### æ¨¡å‹åº«
- [Hugging Face Model Hub](https://huggingface.co/models)
- [Ultralytics Model Zoo](https://github.com/ultralytics/ultralytics)
- [TensorFlow Hub](https://tfhub.dev/)

### è³‡æ–™é›†
- [COCO Dataset](https://cocodataset.org/)
- [ImageNet](https://www.image-net.org/)
- [Common Voice](https://commonvoice.mozilla.org/)

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­°

1. **å…¥é–€ç·´ç¿’ï¼š**
   - å¾ Whisper èªéŸ³è½‰æ–‡å­—é–‹å§‹
   - å˜—è©¦ YOLOv8 ç‰©ä»¶åµæ¸¬
   - çµåˆå…©è€…åšç°¡å–®æ‡‰ç”¨

2. **é€²éšå°ˆæ¡ˆï¼š**
   - å»ºç«‹å³æ™‚ç¿»è­¯ç³»çµ±
   - é–‹ç™¼æ™ºæ…§ç›£æ§æ‡‰ç”¨
   - è£½ä½œç„¡éšœç¤™è¼”åŠ©å·¥å…·

3. **æ•ˆèƒ½å„ªåŒ–ï¼š**
   - å­¸ç¿’æ¨¡å‹é‡åŒ–æŠ€è¡“
   - ä½¿ç”¨ GPU åŠ é€Ÿ
   - éƒ¨ç½²åˆ°é‚Šç·£è£ç½®

4. **æŒçºŒå­¸ç¿’ï¼š**
   - é—œæ³¨æœ€æ–°è«–æ–‡å’ŒæŠ€è¡“
   - åƒèˆ‡é–‹æºå°ˆæ¡ˆ
   - åŠ å…¥ AI ç¤¾ç¾¤è¨è«–

---

## ğŸ“ æˆæ¬Šèˆ‡æ³¨æ„äº‹é …

- ä½¿ç”¨é–‹æºæ¨¡å‹æ™‚æ³¨æ„æˆæ¬Šæ¢æ¬¾
- è™•ç†å€‹äººè³‡æ–™æ™‚éµå®ˆéš±ç§æ³•è¦
- å•†æ¥­ä½¿ç”¨å‰ç¢ºèªæ¨¡å‹æˆæ¬Š
- æ³¨æ„ API ä½¿ç”¨é™åˆ¶å’Œè²»ç”¨