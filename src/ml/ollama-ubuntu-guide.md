# Ollama åœ¨ Ubuntu 24.04 å®Œæ•´ä½¿ç”¨æŒ‡å—

## ç›®éŒ„
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [å®‰è£](#å®‰è£)
- [åŸºæœ¬æ“ä½œ](#åŸºæœ¬æ“ä½œ)
- [ç°¡å–®ç¯„ä¾‹](#ç°¡å–®ç¯„ä¾‹)
- [å¸¸ç”¨æ¨¡å‹æ¨è–¦](#å¸¸ç”¨æ¨¡å‹æ¨è–¦)
- [å¯¦ç”¨æŠ€å·§](#å¯¦ç”¨æŠ€å·§)
- [Web UI è¨­å®š](#web-ui-è¨­å®š)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [é€²éšè¨­å®š](#é€²éšè¨­å®š)
- [API åƒè€ƒ](#api-åƒè€ƒ)

## å¿«é€Ÿé–‹å§‹

```bash
# ä¸€éµå®‰è£
curl -fsSL https://ollama.com/install.sh | sh

# åŸ·è¡Œç¬¬ä¸€å€‹æ¨¡å‹
ollama run tinyllama

# è¼¸å…¥å•é¡Œé–‹å§‹å°è©±
>>> ä½ å¥½ï¼Œè«‹è‡ªæˆ‘ä»‹ç´¹
```

## å®‰è£

### ç³»çµ±éœ€æ±‚
- Ubuntu 24.04 LTS
- æœ€å°‘ 4GB RAMï¼ˆå»ºè­° 8GB ä»¥ä¸Šï¼‰
- 10GB å¯ç”¨ç¡¬ç¢Ÿç©ºé–“
- (é¸ç”¨) NVIDIA GPU with CUDA 11.8+

### å®‰è£æ–¹æ³•

#### æ–¹æ³• 1ï¼šå®˜æ–¹è…³æœ¬ï¼ˆæ¨è–¦ï¼‰
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### æ–¹æ³• 2ï¼šæ‰‹å‹•å®‰è£
```bash
# ä¸‹è¼‰äºŒé€²ä½æª”æ¡ˆ
wget https://github.com/ollama/ollama/releases/latest/download/ollama-linux-amd64

# è³¦äºˆåŸ·è¡Œæ¬Šé™
chmod +x ollama-linux-amd64

# ç§»å‹•åˆ°ç³»çµ±è·¯å¾‘
sudo mv ollama-linux-amd64 /usr/local/bin/ollama

# å»ºç«‹æœå‹™æª”æ¡ˆ
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
```

### é©—è­‰å®‰è£
```bash
# æª¢æŸ¥ç‰ˆæœ¬
ollama --version

# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ollama
```

## åŸºæœ¬æ“ä½œ

### æœå‹™ç®¡ç†
```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ollama

# å•Ÿå‹•æœå‹™
sudo systemctl start ollama

# åœæ­¢æœå‹™
sudo systemctl stop ollama

# é‡å•Ÿæœå‹™
sudo systemctl restart ollama

# è¨­å®šé–‹æ©Ÿè‡ªå‹•å•Ÿå‹•
sudo systemctl enable ollama

# æª¢è¦–æœå‹™æ—¥èªŒ
journalctl -u ollama -f
```

### æ¨¡å‹ç®¡ç†
```bash
# ä¸‹è¼‰ä¸¦åŸ·è¡Œæ¨¡å‹
ollama run llama3.2:3b

# åªä¸‹è¼‰æ¨¡å‹ä¸åŸ·è¡Œ
ollama pull llama3.2:3b

# åˆ—å‡ºå·²å®‰è£çš„æ¨¡å‹
ollama list

# é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
ollama show llama3.2:3b

# åˆªé™¤æ¨¡å‹
ollama rm llama3.2:3b

# è¤‡è£½æ¨¡å‹ï¼ˆç”¨æ–¼è‡ªè¨‚ï¼‰
ollama cp llama3.2:3b my-custom-model
```

## ç°¡å–®ç¯„ä¾‹

### ç¯„ä¾‹ 1ï¼šå‘½ä»¤åˆ—å°è©±
```bash
# äº’å‹•å¼å°è©±
ollama run llama3.2:3b

>>> è§£é‡‹ä»€éº¼æ˜¯å®¹å™¨æŠ€è¡“ï¼Ÿ
>>> ç”¨ Python å¯«ä¸€å€‹å¿«é€Ÿæ’åº
>>> /bye
```

### ç¯„ä¾‹ 2ï¼šä¸€æ¬¡æ€§å•ç­”
```bash
# å–®æ¬¡å•ç­”ï¼ˆä¸é€²å…¥äº’å‹•æ¨¡å¼ï¼‰
echo "ä»€éº¼æ˜¯ RESTful APIï¼Ÿ" | ollama run llama3.2:3b

# æˆ–ä½¿ç”¨åƒæ•¸æ–¹å¼
ollama run llama3.2:3b "åˆ—å‡º 5 å€‹ Git å¸¸ç”¨æŒ‡ä»¤"
```

### ç¯„ä¾‹ 3ï¼šä½¿ç”¨ cURL å‘¼å« API
```bash
# åŸºæœ¬ API å‘¼å«
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "è§£é‡‹ä»€éº¼æ˜¯å¾®æœå‹™æ¶æ§‹",
  "stream": false
}'

# ä¸²æµå›æ‡‰
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "å¯«ä¸€å€‹ Python Flask ç¯„ä¾‹",
  "stream": true
}'
```

### ç¯„ä¾‹ 4ï¼šPython æ•´åˆè…³æœ¬
```python
#!/usr/bin/env python3
"""
Ollama Python æ•´åˆç¯„ä¾‹
å®‰è£: pip install requests
"""

import requests
import json

class OllamaClient:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
    
    def generate(self, prompt, model="llama3.2:3b", stream=False):
        """ç”Ÿæˆå›æ‡‰"""
        url = f"{self.base_url}/api/generate"
        data = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        if not stream:
            response = requests.post(url, json=data)
            if response.status_code == 200:
                return response.json()['response']
        else:
            response = requests.post(url, json=data, stream=True)
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    yield chunk['response']
    
    def chat(self, messages, model="llama3.2:3b"):
        """èŠå¤©ä»‹é¢"""
        url = f"{self.base_url}/api/chat"
        data = {
            "model": model,
            "messages": messages,
            "stream": False
        }
        
        response = requests.post(url, json=data)
        if response.status_code == 200:
            return response.json()['message']['content']
    
    def list_models(self):
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        url = f"{self.base_url}/api/tags"
        response = requests.get(url)
        if response.status_code == 200:
            return response.json()['models']

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    client = OllamaClient()
    
    # ç°¡å–®ç”Ÿæˆ
    print("=== ç°¡å–®ç”Ÿæˆ ===")
    result = client.generate("ç”¨ Python å¯«ä¸€å€‹è²»æ°æ•¸åˆ—")
    print(result)
    
    # ä¸²æµç”Ÿæˆ
    print("\n=== ä¸²æµç”Ÿæˆ ===")
    for chunk in client.generate("è§£é‡‹ Docker çš„å„ªé»", stream=True):
        print(chunk, end='', flush=True)
    print()
    
    # èŠå¤©æ¨¡å¼
    print("\n=== èŠå¤©æ¨¡å¼ ===")
    messages = [
        {"role": "user", "content": "ä½ æ˜¯èª°ï¼Ÿ"},
        {"role": "assistant", "content": "æˆ‘æ˜¯ä¸€å€‹ AI åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ èƒ½åšä»€éº¼ï¼Ÿ"}
    ]
    response = client.chat(messages)
    print(response)
    
    # åˆ—å‡ºæ¨¡å‹
    print("\n=== å¯ç”¨æ¨¡å‹ ===")
    models = client.list_models()
    for model in models:
        print(f"- {model['name']} ({model['size']/1e9:.1f}GB)")
```

### ç¯„ä¾‹ 5ï¼šBash èŠå¤©æ©Ÿå™¨äººè…³æœ¬
```bash
#!/bin/bash
# å„²å­˜ç‚º ollama-chat.sh
# ä½¿ç”¨: chmod +x ollama-chat.sh && ./ollama-chat.sh

# è¨­å®š
MODEL="${1:-llama3.2:3b}"
HISTORY_FILE="$HOME/.ollama_chat_history"

# é¡è‰²è¨­å®š
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# å‡½æ•¸ï¼šé¡¯ç¤ºæ¨™é¡Œ
show_header() {
    clear
    echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${BLUE}â•‘       Ollama èŠå¤©æ©Ÿå™¨äºº v1.0          â•‘${NC}"
    echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${YELLOW}æ¨¡å‹: $MODEL${NC}"
    echo -e "${GREEN}æŒ‡ä»¤: /help, /clear, /model, /save, /quit${NC}"
    echo ""
}

# å‡½æ•¸ï¼šè™•ç†å‘½ä»¤
handle_command() {
    case $1 in
        /help)
            echo -e "${GREEN}å¯ç”¨æŒ‡ä»¤ï¼š${NC}"
            echo "  /help   - é¡¯ç¤ºå¹«åŠ©"
            echo "  /clear  - æ¸…é™¤è¢å¹•"
            echo "  /model  - åˆ‡æ›æ¨¡å‹"
            echo "  /save   - å„²å­˜å°è©±"
            echo "  /quit   - é›¢é–‹ç¨‹å¼"
            ;;
        /clear)
            show_header
            ;;
        /model)
            echo -e "${YELLOW}å¯ç”¨æ¨¡å‹ï¼š${NC}"
            ollama list
            read -p "è¼¸å…¥æ¨¡å‹åç¨±: " new_model
            if [ ! -z "$new_model" ]; then
                MODEL=$new_model
                echo -e "${GREEN}å·²åˆ‡æ›åˆ° $MODEL${NC}"
            fi
            ;;
        /save)
            echo "$conversation" > "$HISTORY_FILE"
            echo -e "${GREEN}å°è©±å·²å„²å­˜åˆ° $HISTORY_FILE${NC}"
            ;;
        /quit|/exit|/bye)
            echo -e "${BLUE}å†è¦‹ï¼${NC}"
            exit 0
            ;;
        *)
            return 1
            ;;
    esac
    return 0
}

# ä¸»ç¨‹å¼
show_header
conversation=""

while true; do
    # è®€å–ä½¿ç”¨è€…è¼¸å…¥
    echo -ne "${GREEN}ğŸ‘¤ ä½ : ${NC}"
    read -r input
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå‘½ä»¤
    if [[ $input == /* ]]; then
        handle_command "$input"
        continue
    fi
    
    # æ–°å¢åˆ°å°è©±è¨˜éŒ„
    conversation="$conversation\nğŸ‘¤: $input"
    
    # å–å¾— AI å›æ‡‰
    echo -ne "${BLUE}ğŸ¤– AI: ${NC}"
    response=$(echo "$input" | ollama run $MODEL 2>/dev/null)
    echo "$response"
    
    # æ–°å¢åˆ°å°è©±è¨˜éŒ„
    conversation="$conversation\nğŸ¤–: $response"
    echo ""
done
```

### ç¯„ä¾‹ 6ï¼šNode.js æ•´åˆ
```javascript
// ollama-client.js
// å®‰è£: npm install axios

const axios = require('axios');

class OllamaClient {
    constructor(baseURL = 'http://localhost:11434') {
        this.baseURL = baseURL;
    }

    async generate(prompt, model = 'llama3.2:3b') {
        try {
            const response = await axios.post(`${this.baseURL}/api/generate`, {
                model: model,
                prompt: prompt,
                stream: false
            });
            return response.data.response;
        } catch (error) {
            console.error('Error:', error.message);
            return null;
        }
    }

    async *generateStream(prompt, model = 'llama3.2:3b') {
        try {
            const response = await axios.post(`${this.baseURL}/api/generate`, {
                model: model,
                prompt: prompt,
                stream: true
            }, {
                responseType: 'stream'
            });

            for await (const chunk of response.data) {
                const lines = chunk.toString().split('\n').filter(Boolean);
                for (const line of lines) {
                    const data = JSON.parse(line);
                    yield data.response;
                }
            }
        } catch (error) {
            console.error('Error:', error.message);
        }
    }
}

// ä½¿ç”¨ç¯„ä¾‹
async function main() {
    const client = new OllamaClient();
    
    // ä¸€èˆ¬ç”Ÿæˆ
    console.log('=== ä¸€èˆ¬ç”Ÿæˆ ===');
    const response = await client.generate('ä»€éº¼æ˜¯ Node.jsï¼Ÿ');
    console.log(response);
    
    // ä¸²æµç”Ÿæˆ
    console.log('\n=== ä¸²æµç”Ÿæˆ ===');
    for await (const chunk of client.generateStream('åˆ—å‡º JavaScript çš„ç‰¹é»')) {
        process.stdout.write(chunk);
    }
    console.log();
}

main();
```

## å¸¸ç”¨æ¨¡å‹æ¨è–¦

### ğŸ¯ è¼•é‡ç´šæ¨¡å‹ (RAM < 4GB)

| æ¨¡å‹åç¨± | åƒæ•¸å¤§å° | è¨˜æ†¶é«”éœ€æ±‚ | ç‰¹é» | å®‰è£æŒ‡ä»¤ |
|---------|---------|-----------|------|---------|
| TinyLlama | 1.1B | ~2GB | è¶…å¿«é€Ÿå›æ‡‰ | `ollama run tinyllama` |
| Phi-3 Mini | 3.8B | ~3GB | å¾®è»Ÿå‡ºå“ï¼Œæ•ˆèƒ½ä½³ | `ollama run phi3:mini` |
| Qwen 0.5B | 0.5B | ~1GB | ä¸­æ–‡æ”¯æ´è‰¯å¥½ | `ollama run qwen:0.5b` |
| Gemma 2B | 2B | ~2.5GB | Google æ¨¡å‹ | `ollama run gemma:2b` |

### ğŸ’ª ä¸­å‹æ¨¡å‹ (RAM 8-16GB)

| æ¨¡å‹åç¨± | åƒæ•¸å¤§å° | è¨˜æ†¶é«”éœ€æ±‚ | ç‰¹é» | å®‰è£æŒ‡ä»¤ |
|---------|---------|-----------|------|---------|
| Llama 3.2 | 3B | ~5GB | Meta æœ€æ–°ï¼Œå¹³è¡¡é¸æ“‡ | `ollama run llama3.2:3b` |
| Mistral | 7B | ~8GB | æ³•åœ‹åœ˜éšŠï¼Œå“è³ªå„ªç§€ | `ollama run mistral` |
| Gemma 2 | 9B | ~10GB | Google å¤§å‹æ¨¡å‹ | `ollama run gemma2:9b` |
| Vicuna | 7B | ~8GB | å°è©±èƒ½åŠ›å¼· | `ollama run vicuna` |

### ğŸ‘¨â€ğŸ’» ç¨‹å¼ç¢¼å°ˆç”¨æ¨¡å‹

| æ¨¡å‹åç¨± | åƒæ•¸å¤§å° | è¨˜æ†¶é«”éœ€æ±‚ | ç‰¹é» | å®‰è£æŒ‡ä»¤ |
|---------|---------|-----------|------|---------|
| CodeLlama | 7B | ~8GB | Meta ç¨‹å¼ç¢¼æ¨¡å‹ | `ollama run codellama` |
| DeepSeek Coder | 1.3B | ~2GB | è¼•é‡ç´šç¨‹å¼ç¢¼ | `ollama run deepseek-coder:1.3b` |
| Starcoder2 | 3B | ~4GB | å¤šèªè¨€ç¨‹å¼ç¢¼ | `ollama run starcoder2:3b` |
| CodeGemma | 7B | ~8GB | Google ç¨‹å¼ç¢¼æ¨¡å‹ | `ollama run codegemma` |

### ğŸŒ ä¸­æ–‡å„ªåŒ–æ¨¡å‹

| æ¨¡å‹åç¨± | åƒæ•¸å¤§å° | è¨˜æ†¶é«”éœ€æ±‚ | ç‰¹é» | å®‰è£æŒ‡ä»¤ |
|---------|---------|-----------|------|---------|
| Qwen | 1.8B | ~3GB | é˜¿é‡Œé€šç¾©åƒå• | `ollama run qwen` |
| Yi | 6B | ~7GB | é›¶ä¸€è¬ç‰© | `ollama run yi` |
| ChatGLM3 | 6B | ~7GB | æ¸…è¯æ™ºè­œ | `ollama run chatglm3` |

## å¯¦ç”¨æŠ€å·§

### æ•ˆèƒ½å„ªåŒ–

#### 1. GPU åŠ é€Ÿè¨­å®š
```bash
# æª¢æŸ¥ GPU æ”¯æ´
nvidia-smi

# è¨­å®šä½¿ç”¨ç‰¹å®š GPU
export CUDA_VISIBLE_DEVICES=0

# ä½¿ç”¨å¤šå€‹ GPU
export CUDA_VISIBLE_DEVICES=0,1
```

#### 2. è¨˜æ†¶é«”å„ªåŒ–
```bash
# ä½¿ç”¨é‡å­åŒ–ç‰ˆæœ¬ï¼ˆé™ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼‰
ollama run llama3.2:3b-q4_0  # 4-bit é‡å­åŒ–
ollama run llama3.2:3b-q5_0  # 5-bit é‡å­åŒ–
ollama run llama3.2:3b-q8_0  # 8-bit é‡å­åŒ–

# é™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
ollama run llama3.2:3b --num-ctx 2048
```

#### 3. CPU å„ªåŒ–
```bash
# è¨­å®šä¸¦è¡Œæ•¸
export OLLAMA_NUM_PARALLEL=2

# è¨­å®šåŸ·è¡Œç·’æ•¸
export OLLAMA_NUM_THREAD=4
```

### æ‰¹æ¬¡è™•ç†

#### æ‰¹æ¬¡å•ç­”è…³æœ¬
```bash
#!/bin/bash
# batch-query.sh

# å•é¡Œåˆ—è¡¨æª”æ¡ˆ
QUESTIONS_FILE="questions.txt"
OUTPUT_FILE="answers.md"
MODEL="llama3.2:3b"

# æ¸…ç©ºè¼¸å‡ºæª”æ¡ˆ
> "$OUTPUT_FILE"

# é€è¡Œè®€å–å•é¡Œä¸¦è™•ç†
while IFS= read -r question; do
    echo "è™•ç†: $question"
    echo "## $question" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    
    answer=$(echo "$question" | ollama run $MODEL)
    echo "$answer" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "---" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
done < "$QUESTIONS_FILE"

echo "å®Œæˆï¼çµæœå·²å„²å­˜åˆ° $OUTPUT_FILE"
```

### æ•´åˆåˆ° VS Code

#### 1. å®‰è£ Continue æ“´å……å¥—ä»¶
```bash
# åœ¨ VS Code ä¸­
# 1. é–‹å•Ÿå»¶ä¼¸æ¨¡çµ„ (Ctrl+Shift+X)
# 2. æœå°‹ "Continue"
# 3. å®‰è£
```

#### 2. è¨­å®š Continue
```json
{
  "models": [
    {
      "title": "Ollama",
      "provider": "ollama",
      "model": "codellama:7b",
      "apiBase": "http://localhost:11434"
    }
  ]
}
```

### å»ºç«‹åˆ¥åå¿«æ·

```bash
# åŠ å…¥åˆ° ~/.bashrc æˆ– ~/.zshrc

# å¿«é€Ÿå•Ÿå‹•å°è©±
alias chat='ollama run llama3.2:3b'

# ç¨‹å¼ç¢¼åŠ©æ‰‹
alias code-ai='ollama run codellama:7b'

# å¿«é€Ÿç¿»è­¯
translate() {
    echo "Translate to Chinese: $1" | ollama run llama3.2:3b
}

# ç¨‹å¼ç¢¼è§£é‡‹
explain() {
    echo "Explain this code: $(cat $1)" | ollama run codellama:7b
}

# å¿«é€Ÿæ‘˜è¦
summarize() {
    echo "Summarize: $(cat $1)" | ollama run llama3.2:3b
}
```

## Web UI è¨­å®š

### é¸é … 1ï¼šOpen WebUIï¼ˆæ¨è–¦ï¼‰

#### Docker å®‰è£
```bash
# æ‹‰å–ä¸¦åŸ·è¡Œ
docker run -d \
  -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

# æª¢æŸ¥ç‹€æ…‹
docker ps | grep open-webui

# æª¢è¦–æ—¥èªŒ
docker logs -f open-webui
```

#### Docker Compose å®‰è£
```yaml
# docker-compose.yml
version: '3.8'

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  open-webui:
```

```bash
# å•Ÿå‹•
docker-compose up -d

# è¨ªå• http://localhost:3000
```

### é¸é … 2ï¼šOllama UI

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/ollama-ui/ollama-ui
cd ollama-ui

# å®‰è£ä¾è³´
npm install

# å»ºç«‹ç’°å¢ƒè¨­å®š
cp .env.example .env
echo "OLLAMA_HOST=http://localhost:11434" >> .env

# å•Ÿå‹•é–‹ç™¼ä¼ºæœå™¨
npm run dev

# å»ºç½®ç”Ÿç”¢ç‰ˆæœ¬
npm run build
npm start
```

### é¸é … 3ï¼šç°¡å–® HTML ä»‹é¢

```html
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ollama Chat</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .container {
            width: 90%;
            max-width: 800px;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        .header {
            background: #4a5568;
            color: white;
            padding: 20px;
            text-align: center;
        }
        .chat-box {
            height: 400px;
            overflow-y: auto;
            padding: 20px;
            background: #f7fafc;
        }
        .message {
            margin: 10px 0;
            padding: 10px 15px;
            border-radius: 10px;
            max-width: 70%;
        }
        .user-message {
            background: #4299e1;
            color: white;
            margin-left: auto;
            text-align: right;
        }
        .ai-message {
            background: #e2e8f0;
            color: #2d3748;
        }
        .input-area {
            display: flex;
            padding: 20px;
            background: white;
            border-top: 1px solid #e2e8f0;
        }
        #messageInput {
            flex: 1;
            padding: 10px 15px;
            border: 2px solid #e2e8f0;
            border-radius: 25px;
            font-size: 16px;
            outline: none;
        }
        #messageInput:focus {
            border-color: #4299e1;
        }
        #sendButton {
            margin-left: 10px;
            padding: 10px 30px;
            background: #4299e1;
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 16px;
            cursor: pointer;
            transition: background 0.3s;
        }
        #sendButton:hover {
            background: #3182ce;
        }
        #sendButton:disabled {
            background: #a0aec0;
            cursor: not-allowed;
        }
        .model-selector {
            padding: 10px 20px;
            background: #edf2f7;
        }
        select {
            padding: 5px 10px;
            border-radius: 5px;
            border: 1px solid #cbd5e0;
        }
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #4299e1;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¤– Ollama Chat</h1>
        </div>
        
        <div class="model-selector">
            <label for="modelSelect">æ¨¡å‹ï¼š</label>
            <select id="modelSelect">
                <option value="llama3.2:3b">Llama 3.2 (3B)</option>
                <option value="tinyllama">TinyLlama</option>
                <option value="codellama:7b">CodeLlama</option>
                <option value="mistral">Mistral</option>
            </select>
        </div>
        
        <div class="chat-box" id="chatBox"></div>
        
        <div class="input-area">
            <input type="text" id="messageInput" placeholder="è¼¸å…¥è¨Šæ¯..." />
            <button id="sendButton">ç™¼é€</button>
        </div>
    </div>

    <script>
        const chatBox = document.getElementById('chatBox');
        const messageInput = document.getElementById('messageInput');
        const sendButton = document.getElementById('sendButton');
        const modelSelect = document.getElementById('modelSelect');
        
        async function sendMessage() {
            const message = messageInput.value.trim();
            if (!message) return;
            
            // é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
            addMessage(message, 'user');
            messageInput.value = '';
            
            // ç¦ç”¨è¼¸å…¥
            sendButton.disabled = true;
            messageInput.disabled = true;
            
            // é¡¯ç¤ºè¼‰å…¥ä¸­
            const loadingId = 'loading-' + Date.now();
            addMessage('<div class="loading"></div>', 'ai', loadingId);
            
            try {
                const response = await fetch('http://localhost:11434/api/generate', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        model: modelSelect.value,
                        prompt: message,
                        stream: false
                    })
                });
                
                const data = await response.json();
                
                // ç§»é™¤è¼‰å…¥å‹•ç•«
                document.getElementById(loadingId).remove();
                
                // é¡¯ç¤º AI å›æ‡‰
                addMessage(data.response, 'ai');
                
            } catch (error) {
                document.getElementById(loadingId).remove();
                addMessage('éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™', 'ai');
            }
            
            // é‡æ–°å•Ÿç”¨è¼¸å…¥
            sendButton.disabled = false;
            messageInput.disabled = false;
            messageInput.focus();
        }
        
        function addMessage(content, sender, id = null) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}-message`;
            if (id) messageDiv.id = id;
            messageDiv.innerHTML = content;
            chatBox.appendChild(messageDiv);
            chatBox.scrollTop = chatBox.scrollHeight;
        }
        
        sendButton.addEventListener('click', sendMessage);
        messageInput.addEventListener('keypress', (e) => {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendMessage();
            }
        });
        
        // åˆå§‹è¨Šæ¯
        addMessage('ä½ å¥½ï¼æˆ‘æ˜¯ Ollama AI åŠ©æ‰‹ï¼Œæœ‰ä»€éº¼å¯ä»¥å¹«åŠ©ä½ çš„å—ï¼Ÿ', 'ai');
    </script>
</body>
</html>
```

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

#### 1. æœå‹™ç„¡æ³•å•Ÿå‹•
```bash
# æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ
journalctl -u ollama -n 50

# å¸¸è¦‹åŸå› ï¼šåŸ è™Ÿè¢«ä½”ç”¨
sudo lsof -i :11434

# è§£æ±ºæ–¹æ¡ˆï¼šæ›´æ”¹åŸ è™Ÿ
OLLAMA_HOST=0.0.0.0:8080 ollama serve
```

#### 2. GPU ä¸è¢«è­˜åˆ¥
```bash
# æª¢æŸ¥ NVIDIA é©…å‹•
nvidia-smi

# å®‰è£ CUDA å·¥å…·åŒ…
sudo apt update
sudo apt install nvidia-cuda-toolkit

# æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# é‡å•Ÿ Ollama
sudo systemctl restart ollama
```

#### 3. è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤
```bash
# è§£æ±ºæ–¹æ¡ˆ 1ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama run tinyllama

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šä½¿ç”¨é‡å­åŒ–ç‰ˆæœ¬
ollama run llama3.2:3b-q4_0

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šé™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
ollama run llama3.2:3b --num-ctx 1024

# è§£æ±ºæ–¹æ¡ˆ 4ï¼šæ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹
ollama list
ollama rm unused-model
```

#### 4. æ¨¡å‹ä¸‹è¼‰å¤±æ•—
```bash
# æª¢æŸ¥ç¶²è·¯é€£ç·š
ping ollama.com

# ä½¿ç”¨ä»£ç†
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# é‡è©¦ä¸‹è¼‰
ollama pull llama3.2:3b

# æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ
wget https://ollama.com/library/llama3.2/blobs/sha256:xxxxx
```

#### 5. API é€£ç·šè¢«æ‹’çµ•
```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ollama

# å…è¨±å¤–éƒ¨é€£ç·š
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# é˜²ç«ç‰†è¨­å®š
sudo ufw allow 11434/tcp
```

### æ•ˆèƒ½èª¿å„ª

#### ç³»çµ±å±¤ç´šå„ªåŒ–
```bash
# å¢åŠ æª”æ¡ˆæè¿°ç¬¦é™åˆ¶
ulimit -n 65536

# èª¿æ•´ swap ä½¿ç”¨
sudo sysctl vm.swappiness=10

# è¨­å®š CPU èª¿æ§å™¨
sudo cpupower frequency-set -g performance
```

#### Ollama ç‰¹å®šå„ªåŒ–
```bash
# ç’°å¢ƒè®Šæ•¸è¨­å®š
export OLLAMA_NUM_PARALLEL=4     # ä¸¦è¡Œè«‹æ±‚æ•¸
export OLLAMA_NUM_GPU=1          # GPU æ•¸é‡
export OLLAMA_MAX_LOADED_MODELS=2 # æœ€å¤§è¼‰å…¥æ¨¡å‹æ•¸
export OLLAMA_KEEP_ALIVE=5m      # æ¨¡å‹ä¿æŒè¼‰å…¥æ™‚é–“
```

## é€²éšè¨­å®š

### è‡ªè¨‚æ¨¡å‹ (Modelfile)

#### åŸºæœ¬ Modelfile
```dockerfile
# Modelfile
FROM llama3.2:3b

# è¨­å®šåƒæ•¸
PARAMETER temperature 0.8
PARAMETER top_k 40
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 4096

# è¨­å®šç³»çµ±æç¤º
SYSTEM """
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æŠ€è¡“é¡§å•ï¼Œå°ˆé–€å”åŠ©è§£æ±ºç¨‹å¼è¨­è¨ˆå’Œç³»çµ±æ¶æ§‹å•é¡Œã€‚
è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦æä¾›è©³ç´°çš„è§£é‡‹å’Œç¯„ä¾‹ã€‚
"""

# è¨­å®šè¨Šæ¯æ¨¡æ¿
TEMPLATE """
{{ if .System }}System: {{ .System }}{{ end }}
{{ if .Prompt }}User: {{ .Prompt }}{{ end }}
Assistant: {{ .Response }}
"""
```

#### å»ºç«‹å’Œä½¿ç”¨è‡ªè¨‚æ¨¡å‹
```bash
# å»ºç«‹æ¨¡å‹
ollama create my-assistant -f ./Modelfile

# ä½¿ç”¨è‡ªè¨‚æ¨¡å‹
ollama run my-assistant

# åˆ†äº«æ¨¡å‹
ollama push username/my-assistant
```

#### é€²éš Modelfile ç¯„ä¾‹
```dockerfile
# ç¨‹å¼ç¢¼åŠ©æ‰‹æ¨¡å‹
FROM codellama:7b

PARAMETER temperature 0.3  # é™ä½éš¨æ©Ÿæ€§
PARAMETER num_predict 2000 # æœ€å¤§ç”Ÿæˆé•·åº¦

SYSTEM """
You are an expert programmer. Follow these rules:
1. Always provide working code examples
2. Include comments explaining complex parts
3. Consider edge cases and error handling
4. Suggest best practices and optimizations
5. Use the most appropriate programming patterns
"""

# åŠ å…¥ç¯„ä¾‹å°è©±
MESSAGE user "Write a function to reverse a string"
MESSAGE assistant """Here's a function to reverse a string in Python:

```python
def reverse_string(s):
    \"\"\"
    Reverse a string using Python's slicing feature.
    
    Args:
        s (str): The string to reverse
    
    Returns:
        str: The reversed string
    \"\"\"
    return s[::-1]

# Example usage
print(reverse_string("hello"))  # Output: "olleh"
```
"""
```

### å¤šæ¨¡å‹ç®¡ç†

#### æ¨¡å‹åˆ‡æ›è…³æœ¬
```python
#!/usr/bin/env python3
"""
Ollama æ¨¡å‹ç®¡ç†å™¨
"""

import subprocess
import json
import sys

class ModelManager:
    def __init__(self):
        self.models = self.get_installed_models()
    
    def get_installed_models(self):
        """å–å¾—å·²å®‰è£çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            result = subprocess.run(
                ['ollama', 'list'], 
                capture_output=True, 
                text=True
            )
            # è§£æè¼¸å‡º
            lines = result.stdout.strip().split('\n')[1:]  # è·³éæ¨™é¡Œ
            models = []
            for line in lines:
                if line:
                    parts = line.split()
                    models.append({
                        'name': parts[0],
                        'size': parts[1] if len(parts) > 1 else 'N/A'
                    })
            return models
        except Exception as e:
            print(f"éŒ¯èª¤: {e}")
            return []
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        print("\nå·²å®‰è£çš„æ¨¡å‹:")
        print("-" * 40)
        for i, model in enumerate(self.models, 1):
            print(f"{i}. {model['name']} ({model['size']})")
    
    def run_model(self, model_name):
        """åŸ·è¡ŒæŒ‡å®šæ¨¡å‹"""
        print(f"\nå•Ÿå‹•æ¨¡å‹: {model_name}")
        subprocess.run(['ollama', 'run', model_name])
    
    def pull_model(self, model_name):
        """ä¸‹è¼‰æ–°æ¨¡å‹"""
        print(f"\nä¸‹è¼‰æ¨¡å‹: {model_name}")
        subprocess.run(['ollama', 'pull', model_name])
    
    def delete_model(self, model_name):
        """åˆªé™¤æ¨¡å‹"""
        confirm = input(f"ç¢ºå®šè¦åˆªé™¤ {model_name}? (y/n): ")
        if confirm.lower() == 'y':
            subprocess.run(['ollama', 'rm', model_name])
            print(f"å·²åˆªé™¤ {model_name}")

def main():
    manager = ModelManager()
    
    while True:
        print("\n" + "="*40)
        print("Ollama æ¨¡å‹ç®¡ç†å™¨")
        print("="*40)
        print("1. åˆ—å‡ºæ¨¡å‹")
        print("2. åŸ·è¡Œæ¨¡å‹")
        print("3. ä¸‹è¼‰æ–°æ¨¡å‹")
        print("4. åˆªé™¤æ¨¡å‹")
        print("5. é›¢é–‹")
        
        choice = input("\né¸æ“‡æ“ä½œ (1-5): ")
        
        if choice == '1':
            manager.list_models()
        
        elif choice == '2':
            manager.list_models()
            model_idx = input("\né¸æ“‡æ¨¡å‹ç·¨è™Ÿ: ")
            try:
                idx = int(model_idx) - 1
                if 0 <= idx < len(manager.models):
                    manager.run_model(manager.models[idx]['name'])
            except (ValueError, IndexError):
                print("ç„¡æ•ˆçš„é¸æ“‡")
        
        elif choice == '3':
            model_name = input("è¼¸å…¥æ¨¡å‹åç¨± (å¦‚ llama3.2:3b): ")
            manager.pull_model(model_name)
            manager.models = manager.get_installed_models()
        
        elif choice == '4':
            manager.list_models()
            model_idx = input("\né¸æ“‡è¦åˆªé™¤çš„æ¨¡å‹ç·¨è™Ÿ: ")
            try:
                idx = int(model_idx) - 1
                if 0 <= idx < len(manager.models):
                    manager.delete_model(manager.models[idx]['name'])
                    manager.models = manager.get_installed_models()
            except (ValueError, IndexError):
                print("ç„¡æ•ˆçš„é¸æ“‡")
        
        elif choice == '5':
            print("å†è¦‹ï¼")
            break
        
        else:
            print("ç„¡æ•ˆçš„é¸æ“‡")

if __name__ == "__main__":
    main()
```

## API åƒè€ƒ

### æ ¸å¿ƒ API ç«¯é»

#### 1. ç”Ÿæˆæ–‡å­— `/api/generate`
```bash
# è«‹æ±‚
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Why is the sky blue?",
    "stream": false,
    "options": {
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 40
    }
  }'

# å›æ‡‰
{
  "model": "llama3.2:3b",
  "created_at": "2024-01-01T00:00:00.000Z",
  "response": "The sky appears blue because...",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 5000000000,
  "load_duration": 1000000000,
  "prompt_eval_duration": 1000000000,
  "eval_duration": 3000000000,
  "eval_count": 100
}
```

#### 2. èŠå¤©ä»‹é¢ `/api/chat`
```bash
# è«‹æ±‚
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"},
      {"role": "assistant", "content": "Hi! How can I help you?"},
      {"role": "user", "content": "Tell me a joke"}
    ],
    "stream": false
  }'
```

#### 3. æ¨¡å‹ç®¡ç†

##### åˆ—å‡ºæ¨¡å‹ `/api/tags`
```bash
curl http://localhost:11434/api/tags

# å›æ‡‰
{
  "models": [
    {
      "name": "llama3.2:3b",
      "modified_at": "2024-01-01T00:00:00.000Z",
      "size": 3825819519,
      "digest": "sha256:xxx"
    }
  ]
}
```

##### é¡¯ç¤ºæ¨¡å‹è³‡è¨Š `/api/show`
```bash
curl -X POST http://localhost:11434/api/show \
  -d '{"name": "llama3.2:3b"}'
```

##### è¤‡è£½æ¨¡å‹ `/api/copy`
```bash
curl -X POST http://localhost:11434/api/copy \
  -d '{
    "source": "llama3.2:3b",
    "destination": "my-model"
  }'
```

##### åˆªé™¤æ¨¡å‹ `/api/delete`
```bash
curl -X DELETE http://localhost:11434/api/delete \
  -d '{"name": "llama3.2:3b"}'
```

##### æ‹‰å–æ¨¡å‹ `/api/pull`
```bash
curl -X POST http://localhost:11434/api/pull \
  -d '{"name": "llama3.2:3b"}'
```

##### æ¨é€æ¨¡å‹ `/api/push`
```bash
curl -X POST http://localhost:11434/api/push \
  -d '{"name": "username/my-model"}'
```

#### 4. åµŒå…¥å‘é‡ `/api/embeddings`
```bash
curl -X POST http://localhost:11434/api/embeddings \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Hello world"
  }'

# å›æ‡‰
{
  "embedding": [0.1, 0.2, 0.3, ...]
}
```

### åƒæ•¸èªªæ˜

#### ç”Ÿæˆåƒæ•¸ (options)
| åƒæ•¸ | é¡å‹ | é è¨­å€¼ | èªªæ˜ |
|-----|------|--------|------|
| temperature | float | 0.8 | æ§åˆ¶éš¨æ©Ÿæ€§ (0-2) |
| top_k | int | 40 | é™åˆ¶è©å½™é¸æ“‡æ•¸é‡ |
| top_p | float | 0.9 | ç´¯ç©æ©Ÿç‡é–¾å€¼ |
| repeat_penalty | float | 1.1 | é‡è¤‡æ‡²ç½° |
| seed | int | 0 | éš¨æ©Ÿç¨®å­ |
| num_predict | int | 128 | æœ€å¤§ç”Ÿæˆé•·åº¦ |
| num_ctx | int | 2048 | ä¸Šä¸‹æ–‡è¦–çª—å¤§å° |
| stop | []string | [] | åœæ­¢åºåˆ— |

### SDK æ•´åˆ

#### Python (ollama-python)
```bash
pip install ollama
```

```python
import ollama

# ç”Ÿæˆ
response = ollama.generate(model='llama3.2:3b', prompt='Why is the sky blue?')
print(response['response'])

# èŠå¤©
messages = [
    {'role': 'user', 'content': 'Why is the sky blue?'}
]
response = ollama.chat(model='llama3.2:3b', messages=messages)
print(response['message']['content'])

# ä¸²æµ
for chunk in ollama.generate(model='llama3.2:3b', prompt='Tell me a story', stream=True):
    print(chunk['response'], end='', flush=True)
```

#### JavaScript/TypeScript
```bash
npm install ollama
```

```javascript
import ollama from 'ollama'

// ç”Ÿæˆ
const response = await ollama.generate({
  model: 'llama3.2:3b',
  prompt: 'Why is the sky blue?'
})
console.log(response.response)

// èŠå¤©
const message = await ollama.chat({
  model: 'llama3.2:3b',
  messages: [{ role: 'user', content: 'Why is the sky blue?' }],
})
console.log(message.message.content)

// ä¸²æµ
const stream = await ollama.generate({
  model: 'llama3.2:3b',
  prompt: 'Tell me a story',
  stream: true,
})
for await (const chunk of stream) {
  process.stdout.write(chunk.response)
}
```

## æœ€ä½³å¯¦è¸

### 1. å®‰å…¨æ€§è¨­å®š
```bash
# é™åˆ¶æœ¬åœ°å­˜å–
OLLAMA_HOST=127.0.0.1:11434 ollama serve

# ä½¿ç”¨ nginx åå‘ä»£ç†
server {
    listen 443 ssl;
    server_name your-domain.com;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location /api/ {
        proxy_pass http://localhost:11434/api/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 2. ç›£æ§å’Œæ—¥èªŒ
```bash
# å³æ™‚ç›£æ§
watch -n 1 'nvidia-smi; echo ""; ollama list'

# æ—¥èªŒåˆ†æ
journalctl -u ollama --since "1 hour ago" | grep ERROR

# æ•ˆèƒ½ç›£æ§è…³æœ¬
#!/bin/bash
while true; do
    echo "$(date): $(ollama list | wc -l) models loaded"
    nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv,noheader
    sleep 5
done
```

### 3. å‚™ä»½å’Œé‚„åŸ
```bash
# å‚™ä»½æ¨¡å‹
tar -czf ollama-models-backup.tar.gz ~/.ollama/models

# é‚„åŸæ¨¡å‹
tar -xzf ollama-models-backup.tar.gz -C ~/

# å‚™ä»½è¨­å®š
cp -r ~/.ollama ollama-config-backup
```

## è³‡æºé€£çµ

### å®˜æ–¹è³‡æº
- [Ollama å®˜æ–¹ç¶²ç«™](https://ollama.com)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [æ¨¡å‹åº«](https://ollama.com/library)
- [API æ–‡ä»¶](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [Modelfile æ–‡ä»¶](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

### ç¤¾ç¾¤è³‡æº
- [Ollama Discord](https://discord.gg/ollama)
- [Reddit r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)
- [Hugging Face Models](https://huggingface.co/models)

### ç›¸é—œå·¥å…·
- [Open WebUI](https://github.com/open-webui/open-webui)
- [Continue (VS Code)](https://continue.dev/)
- [LangChain](https://langchain.com/)
- [LlamaIndex](https://www.llamaindex.ai/)

### å­¸ç¿’è³‡æº
- [Ollama æ•™å­¸å½±ç‰‡](https://www.youtube.com/results?search_query=ollama+tutorial)
- [LLM èª²ç¨‹](https://www.deeplearning.ai/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

---

## æ›´æ–°æ—¥èªŒ

- **2024.01**: åˆå§‹ç‰ˆæœ¬
- **2024.02**: æ–°å¢ Web UI è¨­å®š
- **2024.03**: æ–°å¢é€²éšè¨­å®šå’Œ API åƒè€ƒ
- **2024.04**: æ–°å¢æ•…éšœæ’é™¤å’Œæœ€ä½³å¯¦è¸

---

ğŸ’¡ **å°æç¤º**: 
- é–‹å§‹ä½¿ç”¨æ™‚å…ˆå˜—è©¦è¼ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ TinyLlamaï¼‰
- å®šæœŸæ›´æ–° Ollama ä»¥ç²å¾—æœ€æ–°åŠŸèƒ½å’Œæ•ˆèƒ½æ”¹é€²
- åŠ å…¥ç¤¾ç¾¤ç²å¾—æ”¯æ´å’Œåˆ†äº«ç¶“é©—

ğŸ“ **æˆæ¬Š**: MIT License

ğŸ¤ **è²¢ç»**: æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼