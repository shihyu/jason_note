# 機器學習常見術語白話解釋 🎓
## 完整版：功能 + 解決的問題

---

## 📊 1. 資料相關

### 應變數（Dependent Variable）
**是什麼：** 你想要預測的目標

**白話：** 就是「答案」

**功能：** 
- 定義機器學習任務的目標
- 模型訓練的依據

**解決什麼問題：**
- ❌ 沒有明確目標 → 不知道要預測什麼
- ✅ 定義應變數 → 清楚知道模型要學什麼

**例子：**
- 預測房價 → 應變數是「房價」
- 預測會不會下雨 → 應變數是「下雨/不下雨」
- 預測花的品種 → 應變數是「品種（A/B/C）」
- 預測學生成績 → 應變數是「考試分數」

**程式碼：**
```python
y = df['房價']  # 這是應變數（目標）
```

---

### 自變數（Independent Variable）
**是什麼：** 用來預測答案的線索

**白話：** 就是「參考資料」

**功能：**
- 提供預測所需的資訊
- 影響應變數的因素

**解決什麼問題：**
- ❌ 沒有自變數 → 無法預測
- ✅ 選對自變數 → 預測更準確
- ⚠️ 自變數太少 → 資訊不足，預測不準
- ⚠️ 自變數太多 → 雜訊干擾，反而變差

**例子：**
- 預測房價 → 自變數是「坪數、房間數、地段、屋齡」
- 預測會不會下雨 → 自變數是「溫度、濕度、氣壓、風速」
- 預測花的品種 → 自變數是「花瓣長度、花瓣寬度、花萼長度」
- 預測學生成績 → 自變數是「讀書時間、出席率、作業完成度」

**程式碼：**
```python
X = df[['坪數', '房間數', '屋齡']]  # 這些是自變數（特徵）
```

**關係圖：**
```
自變數（線索）          應變數（答案）
坪數 = 30 坪
房間數 = 3 房      →    房價 = 1500 萬
屋齡 = 5 年
地段 = 市中心
```

---

## 🧠 2. 模型架構相關

### 激活函數（Activation Function）
**是什麼：** 決定神經元要不要「興奮」的開關

**白話：** 就像是「判斷規則」，決定訊號要不要傳下去

**功能：**
1. 引入非線性（讓神經網絡能學習複雜模式）
2. 控制訊號傳遞
3. 影響梯度傳播

**解決什麼問題：**
- ❌ 沒有激活函數 → 多層網絡等於單層（無法學複雜規律）
- ✅ 使用激活函數 → 可以學習非線性關係（如曲線、複雜分類邊界）
- ✅ 選對激活函數 → 避免梯度消失，深層網絡才能訓練

---

#### ReLU（隱藏層最常用）⭐⭐⭐⭐⭐
```python
activation='relu'
```

**規則：** 負數變 0，正數不變
- `ReLU(5) = 5`
- `ReLU(-3) = 0`

**比喻：** 像考試「不及格的通通算 0 分」

**功能：**
- 加速訓練（計算簡單）
- 避免梯度消失
- 產生稀疏性（有些神經元輸出 0）

**解決什麼問題：**
- ❌ Sigmoid/Tanh 梯度消失 → 深層網絡訓練困難
- ✅ ReLU 梯度不衰減 → 可以訓練 50+ 層的深度網絡
- ✅ 計算速度快 → 訓練時間大幅縮短

**缺點（Dying ReLU）：**
- 神經元輸出變負數後，梯度永遠是 0，神經元「死掉」

**用在：** 隱藏層（無論分類或回歸）

---

#### Leaky ReLU（改良版 ReLU）⭐⭐⭐⭐
```python
from keras.layers import LeakyReLU
model.add(Dense(64))
model.add(LeakyReLU(alpha=0.01))
```

**規則：** 
- `x > 0: y = x` （梯度 = 1）
- `x < 0: y = 0.01x` （梯度 = 0.01）

**功能：**
- 解決 Dying ReLU 問題
- 負數區域仍有小梯度

**解決什麼問題：**
- ❌ ReLU 神經元會死掉 → 浪費模型容量
- ✅ Leaky ReLU 永遠有梯度 → 所有神經元都能學習

**用在：** 隱藏層（比 ReLU 更穩定）

---

#### ELU（Exponential Linear Unit）⭐⭐⭐⭐
```python
activation='elu'
```

**功能：**
- 負數區域平滑曲線
- 輸出均值接近 0
- 訓練更穩定

**解決什麼問題：**
- ❌ ReLU 輸出均值 > 0 → 訓練不穩定
- ✅ ELU 均值接近 0 → 加速收斂

**缺點：** 計算比 ReLU 慢

**用在：** 隱藏層（要求高穩定性時）

---

#### Sigmoid（二分類輸出層用）⭐⭐
```python
activation='sigmoid'
```

**規則：** 把任何數字壓縮到 0~1 之間

**比喻：** 像「信心程度」，0 = 完全沒信心，1 = 100% 確定

**功能：**
- 輸出機率值（0 到 1）
- 適合二分類問題

**解決什麼問題：**
- ❌ 輸出是任意數字 → 無法解釋為機率
- ✅ Sigmoid 輸出 0~1 → 可解釋為「是」的機率

**缺點（梯度消失）：**
- 梯度最大只有 0.25
- 多層相乘：0.25 × 0.25 × 0.25 × ... → 接近 0
- 深層網絡（>5 層）訓練困難

**比喻：**
```
每層都打 7 折：
100 → 70 → 49 → 34 → 24 → 17 → 12 → 8 → 6 → 4 → 3
傳到第 10 層只剩 3%！
```

**用在：** 二分類問題的輸出層（例：垃圾郵件判斷）

**輸出例子：** 0.8 表示「80% 機率是垃圾郵件」

---

#### Tanh⭐⭐
```python
activation='tanh'
```

**規則：** 把數字壓縮到 -1~1 之間

**功能：**
- 輸出零中心（比 Sigmoid 好）
- 適合 RNN

**解決什麼問題：**
- ❌ Sigmoid 輸出 0~1，不是零中心 → 訓練較慢
- ✅ Tanh 輸出 -1~1，零中心 → 訓練稍快

**缺點：** 仍有梯度消失問題（但比 Sigmoid 輕微）

**用在：** RNN（循環神經網絡）的隱藏層

---

#### Softmax（多分類輸出層用）⭐⭐⭐⭐⭐
```python
activation='softmax'
```

**規則：** 把數字變成機率，加起來 = 1

**比喻：** 像「投票結果」，每個選項都有機率

**功能：**
- 輸出多個類別的機率分佈
- 所有機率總和 = 1

**解決什麼問題：**
- ❌ 輸出任意數字 → 無法比較類別
- ✅ Softmax 輸出機率 → 可以看出「最可能的類別」

**用在：** 多分類問題的輸出層（例：Iris 花種類）

**輸出例子：** [0.7, 0.2, 0.1] 表示「70% 是 A、20% 是 B、10% 是 C」

---

#### Linear / None（回歸輸出層用）⭐⭐⭐⭐⭐
```python
activation=None  # 或 'linear'
```

**規則：** 數字直接輸出，不做轉換

**比喻：** 像「實際測量值」

**功能：**
- 輸出任意實數
- 不限制範圍

**解決什麼問題：**
- ❌ 用 Sigmoid/Softmax → 輸出被限制範圍
- ✅ 不用激活函數 → 可以預測任意數值（如房價 1523.5 萬）

**用在：** 回歸問題的輸出層

**輸出例子：** 1523.5（房價 1523.5 萬）

---

### 📋 激活函數快速選擇表

| 問題類型 | 輸出層激活函數 | 隱藏層激活函數 | 解決的核心問題 |
|---------|--------------|--------------|--------------|
| 回歸（預測數值） | None / linear | ReLU | 預測連續數值 |
| 二分類（是/否） | sigmoid | ReLU | 輸出機率（0~1） |
| 多分類（選一個） | softmax | ReLU | 輸出機率分佈 |

| 激活函數 | 梯度消失問題 | 適用場景 | 推薦度 |
|---------|------------|---------|--------|
| **Sigmoid** | ❌ 嚴重 | 輸出層（二分類） | ⭐⭐ |
| **Tanh** | ❌ 中等 | RNN | ⭐⭐ |
| **ReLU** | ✅ 解決 | 隱藏層（最常用） | ⭐⭐⭐⭐⭐ |
| **Leaky ReLU** | ✅ 解決 | 隱藏層（更穩定） | ⭐⭐⭐⭐ |
| **ELU** | ✅ 解決 | 隱藏層（計算較慢） | ⭐⭐⭐⭐ |
| **Softmax** | N/A | 輸出層（多分類） | ⭐⭐⭐⭐⭐ |

---

### 🔍 深入理解：梯度消失問題

**什麼是梯度消失？**

**白話：** 深層神經網絡訓練時，越前面的層「學不到東西」

**比喻：**
```
傳話遊戲（10 個人排成一列）：
第 1 個人說：「今天天氣很好」
傳到第 10 個人：「...什麼？聽不清楚」

→ 訊息越傳越弱，最後消失了
→ 這就是梯度消失！
```

**技術原因：**
反向傳播時，梯度（調整幅度）會一層層相乘，如果每層都乘以 < 1 的數字，最後會變得超級小，接近 0。

**為什麼 Sigmoid/Tanh 容易梯度消失：**
```
Sigmoid 梯度最大 = 0.25
10 層網絡：0.25¹⁰ = 0.00000095（幾乎是 0！）

前面的層收到的梯度 ≈ 0
→ 權重幾乎不更新
→ 學不到東西
```

**為什麼 ReLU 能解決：**
```
ReLU 正數區域梯度 = 1
10 層網絡：1¹⁰ = 1（不衰減！）

前面的層仍能收到有效梯度
→ 權重正常更新
→ 可以學習
```

---

### 損失函數（Loss Function）
**是什麼：** 衡量模型預測有多爛的分數

**白話：** 就是「錯誤程度」，越小越好（0 分最好）

**功能：**
- 量化模型的預測誤差
- 提供優化方向
- 引導模型改進

**解決什麼問題：**
- ❌ 不知道模型好不好 → 無法改進
- ✅ 有損失函數 → 知道哪裡錯了，怎麼改
- ✅ 選對損失函數 → 模型學習更有效

**常見類型：**

---

#### MSE（均方誤差）- 回歸用 ⭐⭐⭐⭐⭐
```python
loss='mse'
```

**算法：** (預測值 - 真實值)² 的平均

**功能：**
- 懲罰大錯誤（誤差會被平方放大）
- 數學性質好，容易優化

**解決什麼問題：**
- ❌ 大小錯誤同等對待 → 不合理
- ✅ MSE 懲罰大錯誤 → 模型更重視離群值

**例子：**
```
預測 3 間房價：
- 房 1：真實 1500 萬，預測 1600 萬 → (100)² = 10,000
- 房 2：真實 2000 萬，預測 1950 萬 → (50)² = 2,500
- 房 3：真實 1800 萬，預測 1800 萬 → (0)² = 0
MSE = (10,000 + 2,500 + 0) ÷ 3 = 4,166.67
```

**特點：** 對異常值敏感（差很多會被放大懲罰）

---

#### MAE（平均絕對誤差）- 回歸用 ⭐⭐⭐⭐
```python
loss='mae'
```

**算法：** |預測值 - 真實值| 的平均

**功能：**
- 平等對待所有誤差
- 對異常值不敏感

**解決什麼問題：**
- ❌ MSE 對異常值太敏感 → 可能被極端值主導
- ✅ MAE 穩健 → 不會被少數極端值影響太多

**例子：**
```
預測 3 間房價：
- 房 1：真實 1500 萬，預測 1600 萬 → |100| = 100
- 房 2：真實 2000 萬，預測 1950 萬 → |50| = 50
- 房 3：真實 1800 萬，預測 1800 萬 → |0| = 0
MAE = (100 + 50 + 0) ÷ 3 = 50 萬
```

**白話：** 「平均錯了 50 萬」

**選擇建議：**
- 資料有離群值 → 用 MAE
- 希望懲罰大錯誤 → 用 MSE

---

#### Binary Crossentropy（二分類用）⭐⭐⭐⭐⭐
```python
loss='binary_crossentropy'
```

**功能：**
- 衡量機率預測的準確度
- 懲罰「很有信心但猜錯」

**解決什麼問題：**
- ❌ 用 MSE 做分類 → 不適合機率預測
- ✅ 用 Crossentropy → 針對機率分佈優化

**用在：** 是/否問題

**例子：** 判斷郵件是不是垃圾郵件

**懲罰機制：**
```
真實：垃圾郵件（1）
預測：0.9（90% 信心） → 損失小 ✓
預測：0.1（10% 信心） → 損失大 ✗（很有信心但錯了！）
```

---

#### Categorical Crossentropy（多分類用）⭐⭐⭐⭐⭐
```python
loss='categorical_crossentropy'
```

**功能：**
- 衡量多類別機率預測的準確度
- 引導模型輸出正確類別的高機率

**解決什麼問題：**
- ❌ 二分類損失無法處理多類別
- ✅ Categorical Crossentropy 處理 N 個類別

**用在：** 多個類別選一個

**例子：** 判斷 Iris 花是 A、B、C 哪一種

**懲罰機制：**
```
真實：類別 A
預測：[0.8, 0.1, 0.1] → 損失小 ✓（正確類別機率高）
預測：[0.2, 0.5, 0.3] → 損失大 ✗（正確類別機率低）
```

---

### 📋 損失函數快速選擇表

| 問題類型 | 損失函數 | 何時用 |
|---------|---------|-------|
| 回歸（預測數值） | MSE | 標準選擇，懲罰大錯誤 |
| 回歸（有離群值） | MAE | 對異常值穩健 |
| 二分類（是/否） | binary_crossentropy | 機率預測 |
| 多分類（選一個） | categorical_crossentropy | 機率分佈預測 |

**生活比喻：**
```
考試考 100 題：
- 錯 5 題 → 損失函數 = 5（越小越好）
- 錯 50 題 → 損失函數 = 50（很糟）
- 全對 → 損失函數 = 0（完美！）
```

---

## 🎯 3. 訓練過程相關

### 優化器（Optimizer）
**是什麼：** 決定模型如何學習、改進

**白話：** 就是「學習方法」

**功能：**
- 根據損失函數更新權重
- 決定學習的路徑和速度
- 影響訓練穩定性和速度

**解決什麼問題：**
- ❌ 不知道怎麼更新權重 → 模型無法學習
- ✅ 使用優化器 → 自動找到最佳參數
- ✅ 選對優化器 → 更快收斂、更穩定

**常見類型：**

---

#### Adam（最常用）⭐⭐⭐⭐⭐
```python
optimizer='adam'
```

**功能：**
- 自動調整每個參數的學習率
- 結合動量（Momentum）和 RMSprop 的優點
- 適應性強

**解決什麼問題：**
- ❌ SGD 需要手動調參 → 費時費力
- ✅ Adam 自動適應 → 開箱即用
- ✅ 對學習率不敏感 → 容錯率高

**比喻：** 像「聰明的學生」，知道哪裡該多花時間，哪裡可以快速通過

**為什麼最常用：**
- 效果好
- 不需要太多調參
- 適用於大部分問題

---

#### SGD（隨機梯度下降）⭐⭐⭐
```python
optimizer='sgd'
optimizer=SGD(learning_rate=0.01, momentum=0.9)
```

**功能：**
- 最基礎的優化方法
- 每次隨機選一批資料更新權重

**解決什麼問題：**
- ❌ 全部資料一起算 → 記憶體不夠、速度慢
- ✅ SGD 分批計算 → 速度快、記憶體省

**缺點：**
- 需要手動調整學習率
- 可能卡在局部最優解
- 訓練不穩定

**比喻：** 像「按部就班的學生」，一步一步來，但可能走錯路

**改進版：SGD + Momentum**
```python
optimizer=SGD(learning_rate=0.01, momentum=0.9)
```
- 加入「慣性」，減少震盪
- 更容易跳出局部最優解

---

#### RMSprop ⭐⭐⭐⭐
```python
optimizer='rmsprop'
```

**功能：**
- 自適應學習率
- 適合處理 RNN（循環神經網絡）

**解決什麼問題：**
- ❌ 固定學習率 → 不同參數需要不同速度
- ✅ RMSprop 自適應 → 每個參數有自己的學習率

**比喻：** 專門處理序列資料的學習方式

---

### 📋 優化器選擇建議

| 優化器 | 特點 | 適用場景 | 推薦度 |
|-------|------|---------|--------|
| **Adam** | 自適應、容錯高 | 大部分問題（首選） | ⭐⭐⭐⭐⭐ |
| **SGD** | 簡單、需調參 | 經典模型、研究 | ⭐⭐⭐ |
| **SGD + Momentum** | 穩定、較快 | 圖像分類 | ⭐⭐⭐⭐ |
| **RMSprop** | 適合 RNN | 時間序列、NLP | ⭐⭐⭐⭐ |

**新手建議：** 先用 `Adam`，99% 的情況都夠用

---

### 學習率（Learning Rate）
**是什麼：** 每次學習時調整的幅度

**白話：** 就是「每次改進的步伐大小」

**功能：**
- 控制參數更新的大小
- 影響訓練速度和穩定性

**解決什麼問題：**
- ❌ 學習率太大 → 震盪不收斂，找不到最佳解
- ❌ 學習率太小 → 訓練超級慢，可能卡住
- ✅ 學習率適中 → 穩定且快速收斂

**例子：**
```python
optimizer=Adam(learning_rate=0.001)  # 常用值
```

**比喻：**
```
想像你在找山谷最低點（最佳解）：

學習率太大（0.5）：
→ 步伐太大，一直跨過最低點
→ 像「走太快會摔倒」
  ╱╲     ╱╲     ╱╲
 ↓  ↑   ↓  ↑   ↓  ↑  （來回震盪）

學習率太小（0.00001）：
→ 步伐太小，要走很久
→ 像「龜速前進」
  ╱         ╲
 ↓→→→→→→→→→↓  （太慢了）

學習率剛好（0.001 ~ 0.01）：
→ 穩定前進，順利找到最低點 ✓
  ╱    ╲
 ↓→→→↓  （穩定下降）
```

**常用值：**
- Adam：0.001（預設）
- SGD：0.01 到 0.1
- 微調預訓練模型：0.0001（更小更穩）

**進階技巧：學習率衰減**
```python
# 訓練過程中逐漸降低學習率
from keras.callbacks import ReduceLROnPlateau
```

---

### Epoch（訓練輪數）
**是什麼：** 整個資料集訓練幾次

**白話：** 就是「複習次數」

**功能：**
- 讓模型多次看過所有資料
- 提高學習充分性

**解決什麼問題：**
- ❌ Epoch 太少 → 學習不充分（欠擬合）
- ❌ Epoch 太多 → 背答案（過擬合）
- ✅ Epoch 適中 → 學得剛好

**例子：**
```python
model.fit(X, y, epochs=100)
```

**比喻：**
```
準備考試有 100 題練習題：

epochs=1：
→ 每題只做 1 次
→ 可能記不熟

epochs=10：
→ 每題做 10 次
→ 開始有印象 ✓

epochs=100：
→ 每題做 100 次
→ 滾瓜爛熟 ✓

epochs=1000：
→ 每題做 1000 次
→ 過度背答案，不會舉一反三 ✗
```

**如何選擇：**
1. 觀察訓練曲線
2. 使用 Early Stopping（表現不再提升就停止）
3. 一般從 50-200 開始嘗試

---

### Batch Size（批次大小）
**是什麼：** 一次餵多少筆資料給模型

**白話：** 就是「一次看幾題」

**功能：**
- 平衡計算效率和學習品質
- 影響梯度估計的準確性
- 影響記憶體使用

**解決什麼問題：**
- ❌ 每次只看 1 筆 → 梯度不穩定、超級慢
- ❌ 一次看全部 → 記憶體爆炸、梯度不準
- ✅ 適中批次 → 速度和效果平衡

**例子：**
```python
model.fit(X, y, batch_size=32)
```

**比喻：**
```
有 1000 題練習題：

batch_size=1（逐筆學習）：
→ 做 1 題，馬上檢討 1 題
→ 優點：學習精細
→ 缺點：超級慢（檢討 1000 次）

batch_size=32（小批次）：
→ 做 32 題，檢討一次
→ 優點：速度適中，效果好 ✓
→ 檢討 1000÷32 ≈ 31 次

batch_size=1000（全部一起）：
→ 做完 1000 題，檢討一次
→ 優點：速度快（只檢討 1 次）
→ 缺點：可能學不好、記憶體爆炸
```

**常用值：** 16、32、64、128

**如何選擇：**
- 資料少 → 較小 batch (16, 32)
- 資料多 → 較大 batch (64, 128)
- GPU 記憶體限制 → 調小直到不爆記憶體

**記憶體關係：**
```
Batch Size 越大 → GPU 記憶體用越多
如果出現 OOM（Out of Memory）→ 調小 batch size
```

---

## 📈 4. 評估指標相關

### 分類問題指標

#### Accuracy（準確率）⭐⭐⭐⭐⭐
```python
metrics=['accuracy']
```

**算法：** 預測對的數量 ÷ 總數量

**功能：**
- 最直觀的分類指標
- 快速了解整體表現

**解決什麼問題：**
- ❌ 不知道模型好不好 → 無法評估
- ✅ 看準確率 → 立刻知道對錯比例

**例子：** 100 封郵件，判對 95 封 → 準確率 = 95%

**白話：** 「答對率」

**缺點（類別不平衡時）：**
```
100 封郵件：95 封正常、5 封垃圾
模型全猜「正常」→ 準確率 = 95%
但垃圾郵件完全沒抓到！
```

**何時不適用：**
- 類別不平衡（如癌症檢測：99% 正常、1% 有癌）
- 此時應看 Precision、Recall、F1-score

---

#### Precision（精確率）⭐⭐⭐⭐
**算法：** 預測是垃圾郵件的，真的是垃圾郵件的比例

**功能：**
- 衡量「誤判」的程度
- 重視「不要冤枉好人」

**解決什麼問題：**
- ❌ 準確率高但誤判多 → Accuracy 無法反映
- ✅ Precision 低 → 知道誤判嚴重

**例子：** 
```
判斷 100 封是垃圾郵件，其中 90 封真的是
→ Precision = 90/100 = 90%
```

**白話：** 「不要誤判好人」

**適用場景：**
- 垃圾郵件過濾（不希望誤判正常郵件）
- 推薦系統（推薦的商品最好都是使用者想要的）

---

#### Recall（召回率）⭐⭐⭐⭐
**算法：** 真的是垃圾郵件的，被抓出來的比例

**功能：**
- 衡量「漏掉」的程度
- 重視「不要漏掉壞人」

**解決什麼問題：**
- ❌ 準確率高但漏掉很多 → Accuracy 無法反映
- ✅ Recall 低 → 知道漏掉太多

**例子：** 
```
有 100 封垃圾郵件，抓到 80 封
→ Recall = 80/100 = 80%
```

**白話：** 「不要漏掉壞人」

**適用場景：**
- 疾病檢測（不希望漏掉病人）
- 欺詐檢測（不希望漏掉詐騙案件）

---

#### F1-Score ⭐⭐⭐⭐⭐
**算法：** Precision 和 Recall 的調和平均

**功能：**
- 平衡 Precision 和 Recall
- 單一指標綜合評估

**解決什麼問題：**
- ❌ Precision 和 Recall 互相矛盾 → 難以取捨
- ✅ F1-Score 綜合考量 → 一個數字看整體

**公式：**
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

**適用場景：**
- 類別不平衡問題
- 需要同時兼顧「不誤判」和「不漏掉」

---

### 🎯 分類指標生活比喻（抓小偷）

```
街上有 10 個小偷，100 個路人：

模型 A：抓了 8 個人，全是小偷
→ Precision = 100%（抓到的都是小偷）✓
→ Recall = 80%（10 個小偷只抓到 8 個）
→ 適合：不想冤枉好人（如法律判決）

模型 B：抓了 50 個人，包含 10 個小偷和 40 個路人
→ Precision = 20%（抓錯很多路人）
→ Recall = 100%（小偷全抓到了）✓
→ 適合：不想漏掉壞人（如疾病篩檢）

模型 C：抓了 9 個人，9 個都是小偷
→ Precision = 100%（完美！）
→ Recall = 90%（很好！）
→ F1-Score = 94.7%（綜合最佳）✓
```

---

### 回歸問題指標

#### MSE（均方誤差）⭐⭐⭐⭐⭐
```python
metrics=['mse']
```

**算法：** (預測值 - 真實值)² 的平均

**功能：**
- 衡量預測誤差
- 懲罰大錯誤

**解決什麼問題：**
- ❌ 不知道預測有多準 → 無法評估
- ✅ MSE 量化誤差 → 知道平均錯多少

**例子：**
```
預測 3 間房價：
- 房 1：真實 1500 萬，預測 1600 萬 → (100)² = 10,000
- 房 2：真實 2000 萬，預測 1950 萬 → (50)² = 2,500
- 房 3：真實 1800 萬，預測 1800 萬 → (0)² = 0
MSE = (10,000 + 2,500 + 0) ÷ 3 = 4,166.67
```

**特點：** 對異常值非常敏感（誤差會被平方放大）

---

#### RMSE（均方根誤差）⭐⭐⭐⭐⭐
**算法：** MSE 開根號

**功能：**
- 和原始資料同單位
- 更容易解釋

**解決什麼問題：**
- ❌ MSE 單位是「平方」→ 不直觀（房價平方？）
- ✅ RMSE 單位和原始資料一樣 → 容易理解

**例子：**
```
MSE = 4,166.67
RMSE = √4,166.67 ≈ 64.5 萬

白話：「平均誤差約 64.5 萬」
```

---

#### MAE（平均絕對誤差）⭐⭐⭐⭐
```python
metrics=['mae']
```

**算法：** |預測值 - 真實值| 的平均

**功能：**
- 平等對待所有誤差
- 直觀易懂

**解決什麼問題：**
- ❌ MSE 對異常值太敏感 → 被極端值主導
- ✅ MAE 穩健 → 反映整體誤差

**例子：**
```
預測 3 間房價：
- 房 1：真實 1500 萬，預測 1600 萬 → |100| = 100
- 房 2：真實 2000 萬，預測 1950 萬 → |50| = 50
- 房 3：真實 1800 萬，預測 1800 萬 → |0| = 0
MAE = (100 + 50 + 0) ÷ 3 = 50 萬
```

**白話：** 「平均錯了 50 萬」

---

#### R²（決定係數）⭐⭐⭐⭐⭐
**範圍：** 0 到 1（越接近 1 越好）

**功能：**
- 衡量模型解釋能力
- 和基準模型（預測平均值）比較

**解決什麼問題：**
- ❌ MSE/MAE 無法比較不同問題 → 不知道算好還是壞
- ✅ R² 標準化 → 容易比較

**意義：** 模型能解釋多少變異

**例子：**
```
R² = 0.9 → 模型能解釋 90% 的變化，很好！
R² = 0.3 → 模型只能解釋 30% 的變化，不太好
R² = 0.0 → 模型和瞎猜一樣差
R² < 0.0 → 模型比瞎猜還差（這很糟）
```

**白話：** 「模型有多準」

---

### 📋 評估指標選擇建議

**分類問題：**
| 情境 | 推薦指標 | 原因 |
|-----|---------|------|
| 類別平衡 | Accuracy | 最直觀 |
| 類別不平衡 | F1-Score | 綜合考量 |
| 不想誤判（如法律） | Precision | 寧可漏掉，不要冤枉 |
| 不想漏掉（如醫療） | Recall | 寧可誤判，不要漏掉 |

**回歸問題：**
| 情境 | 推薦指標 | 原因 |
|-----|---------|------|
| 標準回歸 | RMSE | 易解釋，懲罰大錯 |
| 有離群值 | MAE | 穩健 |
| 比較模型 | R² | 標準化，易比較 |

---

## 🔧 5. 資料處理相關

### 標準化（Standardization）⭐⭐⭐⭐⭐
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

**是什麼：** 把數據變成「平均 0、標準差 1」

**功能：**
- 統一不同特徵的量級
- 加速模型收斂
- 避免某些特徵主導模型

**解決什麼問題：**
- ❌ 不同特徵範圍差很大 → 模型學習不平衡
- ❌ 坪數 (10-100) vs 距離 (0-5000) → 距離主導一切
- ✅ 標準化後範圍相近 → 模型公平對待所有特徵

**例子：**
```
原始資料：
- 坪數：30（範圍 10~100，平均 50）
- 屋齡：5（範圍 0~50，平均 10）
- 距捷運：500 公尺（範圍 0~5000，平均 1000）

標準化後（平均 0，標準差 1）：
- 坪數：-0.5
- 屋齡：-0.3
- 距捷運：-0.4

→ 大家都在類似的範圍，模型學習更平衡
```

**何時用：**
- 神經網絡（必用！）
- SVM、邏輯迴歸
- 梯度下降算法

---

### 正規化（Normalization）⭐⭐⭐⭐
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

**是什麼：** 把數據壓縮到 0~1 之間

**功能：**
- 統一數據範圍
- 保持原始分佈形狀

**解決什麼問題：**
- ❌ 數據範圍不一致 → 影響某些算法
- ✅ 壓縮到 0~1 → 統一處理

**例子：**
```
原始資料：
- 溫度：[10, 20, 30, 40]

正規化後：
- 溫度：[0, 0.33, 0.67, 1.0]

公式：(x - 最小值) ÷ (最大值 - 最小值)
```

**何時用：**
- 圖像處理（像素 0-255 → 0-1）
- 神經網絡輸入
- KNN、K-means

**標準化 vs 正規化：**
| 特性 | 標準化 | 正規化 |
|-----|--------|--------|
| 範圍 | 不固定（多在 -3~3） | 0~1 |
| 平均 | 0 | 不一定 |
| 適用 | 神經網絡、SVM | 圖像、距離算法 |

---

### One-Hot Encoding（獨熱編碼）⭐⭐⭐⭐⭐
```python
from keras.utils import to_categorical
y_encoded = to_categorical(y, num_classes=3)
```

**是什麼：** 把類別變成 0 和 1 的組合

**功能：**
- 將類別資料轉成數字
- 避免類別間的「順序」誤解

**解決什麼問題：**
- ❌ 直接用數字編碼 → 模型誤以為有大小關係
```
顏色：紅(1) < 綠(2) < 藍(3)
→ 模型會認為「藍色比紅色大」（錯誤！）
```
- ✅ One-Hot 編碼 → 類別平等，無大小關係

**例子：**
```
原始類別：
['紅色', '綠色', '藍色', '紅色']

錯誤做法（Label Encoding）：
[1, 2, 3, 1]
→ 模型認為：3 > 2 > 1（錯！）

正確做法（One-Hot Encoding）：
[[1, 0, 0],   ← 紅色
 [0, 1, 0],   ← 綠色
 [0, 0, 1],   ← 藍色
 [1, 0, 0]]   ← 紅色
→ 模型知道：這些是不同類別，沒有大小關係 ✓
```

**比喻：** 像「複選題」，每個選項只能勾一個

**何時用：**
- 分類問題的標籤（必用！）
- 類別型特徵（如：城市、顏色、品牌）

---

### Label Encoding ⭐⭐⭐
```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
```

**是什麼：** 把類別轉成數字

**功能：**
- 簡單快速
- 節省記憶體

**解決什麼問題：**
- ❌ 模型不能處理文字 → 轉成數字
- ✅ Label Encoding → 快速轉換

**例子：**
```
['紅', '綠', '藍'] → [0, 1, 2]
```

**何時用：**
- 決策樹、隨機森林（可以用，不會誤解）
- 目標變數（y）

**何時不用：**
- 神經網絡、線性模型（會誤解大小關係）
- 類別特徵（應該用 One-Hot）

---

## ⚠️ 6. 常見問題

### 過擬合（Overfitting）
**是什麼：** 模型「背答案」，只記住訓練資料

**功能：** 提醒你模型太複雜了

**解決什麼問題的反面：**
- ❌ 模型太複雜 → 過擬合
- ✅ 適當複雜度 → 泛化良好

**症狀：**
```
訓練集準確率：99% ✓
測試集準確率：60% ✗

→ 這就是過擬合！
```

**比喻：**
```
學生只背考古題的答案：
- 考古題：100 分（因為背過）
- 新的題目：30 分（沒背過就不會）

→ 這就是過擬合！
```

**原因：**
1. 模型太複雜（層數太多、神經元太多）
2. 訓練資料太少
3. 訓練太久（epochs 太多）
4. 沒有正規化

**解決方法：**

#### 1. Dropout ⭐⭐⭐⭐⭐
```python
model.add(Dropout(0.5))
```
- 隨機關閉 50% 神經元
- 防止過度依賴特定神經元

#### 2. Early Stopping ⭐⭐⭐⭐⭐
```python
from keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=10)
model.fit(X, y, callbacks=[early_stop])
```
- 驗證集表現不再提升就停止
- 避免過度訓練

#### 3. 增加訓練資料 ⭐⭐⭐⭐⭐
- 更多資料 → 更難背答案
- Data Augmentation（資料增強）

#### 4. 正規化（L1/L2）⭐⭐⭐⭐
```python
from keras.regularizers import l2
model.add(Dense(64, kernel_regularizer=l2(0.01)))
```
- 懲罰過大的權重
- 強迫模型簡化

#### 5. 減少模型複雜度 ⭐⭐⭐
```python
# 原本：5 層，每層 128 神經元
# 改成：3 層，每層 64 神經元
```

---

### 欠擬合（Underfitting）
**是什麼：** 模型太笨，學不到規律

**功能：** 提醒你模型太簡單了

**解決什麼問題的反面：**
- ❌ 模型太簡單 → 欠擬合
- ✅ 適當複雜度 → 學得好

**症狀：**
```
訓練集準確率：60% ✗
測試集準確率：58% ✗

→ 都很爛，這就是欠擬合！
```

**比喻：**
```
學生完全不讀書：
- 考古題：30 分
- 新的題目：32 分

→ 都很爛，這就是欠擬合！
```

**原因：**
1. 模型太簡單（層數太少、神經元太少）
2. 訓練不夠（epochs 太少）
3. 正規化太強
4. 學習率太小

**解決方法：**

#### 1. 增加模型複雜度 ⭐⭐⭐⭐⭐
```python
# 加更多層、更多神經元
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
```

#### 2. 訓練更久 ⭐⭐⭐⭐
```python
model.fit(X, y, epochs=200)  # 增加 epochs
```

#### 3. 減少正規化 ⭐⭐⭐
```python
model.add(Dropout(0.2))  # 從 0.5 降到 0.2
```

#### 4. 增加特徵 ⭐⭐⭐⭐
- 加入更多有用的自變數
- 特徵工程

---

### Dropout（隨機丟棄）⭐⭐⭐⭐⭐
```python
model.add(Dropout(0.5))
```

**是什麼：** 訓練時隨機「關閉」一些神經元

**功能：**
- 防止過擬合
- 增強泛化能力
- 模擬集成學習

**解決什麼問題：**
- ❌ 神經元過度依賴彼此 → 過擬合
- ✅ Dropout 強迫獨立學習 → 泛化更好

**比喻：**
```
考試練習時：
- 正常：100 個神經元一起工作
- Dropout(0.5)：隨機關掉 50 個，只用 50 個工作

→ 強迫模型不能依賴特定神經元
→ 就像「蒙眼練習」，增強適應能力
```

**數字意義：**
- Dropout(0.5)：關掉 50%
- Dropout(0.3)：關掉 30%
- Dropout(0.2)：關掉 20%
- 常用範圍：0.2 ~ 0.5

**何時用：**
- 隱藏層之後（防止過擬合）
- 模型出現過擬合時

**注意：**
- 測試時自動關閉 Dropout（用全部神經元）
- 輸出層不要用 Dropout

---

## 📋 7. 資料分割

### 訓練集（Training Set）⭐⭐⭐⭐⭐
**功能：** 讓模型學習

**比例：** 通常 60~80%

**比喻：** 「練習題」

**解決什麼問題：**
- ❌ 沒有訓練資料 → 模型無法學習
- ✅ 足夠訓練資料 → 模型學到規律

---

### 驗證集（Validation Set）⭐⭐⭐⭐⭐
**功能：** 
- 調整超參數
- 監控過擬合
- 決定何時停止訓練

**比例：** 通常 10~20%

**比喻：** 「模擬考」

**解決什麼問題：**
- ❌ 沒有驗證集 → 不知道何時停止，可能過擬合
- ✅ 有驗證集 → 即時監控，適時停止

---

### 測試集（Test Set）⭐⭐⭐⭐⭐
**功能：** 最終評估模型

**比例：** 通常 10~20%

**比喻：** 「正式考試」

**解決什麼問題：**
- ❌ 沒有測試集 → 不知道真實表現
- ✅ 有測試集 → 客觀評估泛化能力

---

### 為什麼要分三份？

```
只有訓練集和測試集：
→ 問題：不知道什麼時候該停止訓練
→ 風險：可能過擬合

有訓練集、驗證集、測試集：
→ 訓練集：學習 📚
→ 驗證集：調整、監控（邊學邊檢查）🔍
→ 測試集：最終評分（完全沒看過的題目）✅

完美！
```

**程式碼：**
```python
from sklearn.model_selection import train_test_split

# 先分出訓練集和測試集（80% vs 20%）
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 從訓練集再分出驗證集（訓練 75% vs 驗證 25%）
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.25, random_state=42
)

# 最終比例：訓練集 60%、驗證集 20%、測試集 20%
```

---

## 🎓 8. 完整範例：預測房價

```python
# ========== 1. 載入套件 ==========
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
import matplotlib.pyplot as plt

# ========== 2. 載入資料 ==========
# 假設有房屋資料
df = pd.DataFrame({
    '坪數': [30, 25, 40, 35, 28, 45, 32, 38, 27, 33],
    '房間數': [3, 2, 4, 3, 2, 4, 3, 4, 2, 3],
    '屋齡': [5, 10, 3, 8, 12, 2, 7, 4, 15, 6],
    '距捷運': [500, 1000, 300, 800, 1500, 200, 600, 400, 2000, 700],
    '房價': [1500, 1200, 2000, 1700, 1100, 2200, 1600, 1900, 1000, 1550]
})

# ========== 3. 定義自變數和應變數 ==========
X = df[['坪數', '房間數', '屋齡', '距捷運']]  # 自變數（特徵）
y = df['房價']                                # 應變數（目標）

# ========== 4. 資料分割 ==========
# 先分出訓練+驗證 vs 測試（80% vs 20%）
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 再分出訓練 vs 驗證（75% vs 25%）
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42
)

# ========== 5. 標準化 ==========
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # 訓練集：fit + transform
X_val = scaler.transform(X_val)          # 驗證集：只 transform
X_test = scaler.transform(X_test)        # 測試集：只 transform

# ========== 6. 建立模型（回歸問題）==========
model = Sequential([
    # 輸入層 + 第一隱藏層
    Dense(64, input_dim=4, activation='relu'),
    Dropout(0.3),                    # 防止過擬合
    
    # 第二隱藏層
    Dense(32, activation='relu'),
    Dropout(0.2),
    
    # 輸出層（回歸不需激活函數）
    Dense(1)
])

# ========== 7. 編譯模型 ==========
model.compile(
    loss='mse',                      # 損失函數：均方誤差
    optimizer=Adam(learning_rate=0.001),  # 優化器：Adam
    metrics=['mae', 'mse']           # 評估指標
)

# ========== 8. 設定 Early Stopping ==========
early_stop = EarlyStopping(
    monitor='val_loss',      # 監控驗證集損失
    patience=20,             # 20 輪沒改善就停止
    restore_best_weights=True  # 恢復最佳權重
)

# ========== 9. 訓練模型 ==========
history = model.fit(
    X_train, y_train,
    epochs=200,                      # 最多訓練 200 輪
    batch_size=4,                    # 每次 4 筆資料
    validation_data=(X_val, y_val),  # 驗證集
    callbacks=[early_stop],          # 使用 Early Stopping
    verbose=1                        # 顯示訓練過程
)

# ========== 10. 評估模型 ==========
# 測試集評估
test_loss, test_mae, test_mse = model.evaluate(X_test, y_test)
print(f'\n測試集結果：')
print(f'MAE: {test_mae:.2f} 萬（平均誤差）')
print(f'RMSE: {np.sqrt(test_mse):.2f} 萬（均方根誤差）')

# ========== 11. 預測新房子的價格 ==========
new_house = [[35, 3, 6, 500]]  # 35坪、3房、屋齡6年、距捷運500m
new_house_scaled = scaler.transform(new_house)
predicted_price = model.predict(new_house_scaled, verbose=0)
print(f'\n新房子預測房價: {predicted_price[0][0]:.2f} 萬')

# ========== 12. 視覺化訓練過程 ==========
plt.figure(figsize=(12, 4))

# 損失曲線
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='訓練集損失')
plt.plot(history.history['val_loss'], label='驗證集損失')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title('訓練過程 - 損失函數')

# MAE 曲線
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='訓練集 MAE')
plt.plot(history.history['val_mae'], label='驗證集 MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.title('訓練過程 - 平均絕對誤差')

plt.tight_layout()
plt.show()
```

**這段代碼示範了：**
✅ 完整的資料處理流程  
✅ 三份資料分割（訓練/驗證/測試）  
✅ 標準化處理  
✅ 模型建立與編譯  
✅ 使用 Dropout 防止過擬合  
✅ Early Stopping 自動停止  
✅ 模型評估與預測  
✅ 訓練過程視覺化  

---

## 🗂️ 完整術語快速查詢表

| 類別 | 術語 | 白話 | 解決的問題 | 推薦度 |
|-----|------|------|-----------|--------|
| **資料** | 應變數 | 答案 | 定義預測目標 | ⭐⭐⭐⭐⭐ |
| **資料** | 自變數 | 線索 | 提供預測資訊 | ⭐⭐⭐⭐⭐ |
| **激活** | ReLU | 負數變0 | 解決梯度消失 | ⭐⭐⭐⭐⭐ |
| **激活** | Sigmoid | 輸出0~1 | 二分類機率 | ⭐⭐⭐⭐ |
| **激活** | Softmax | 機率分佈 | 多分類機率 | ⭐⭐⭐⭐⭐ |
| **損失** | MSE | 均方誤差 | 量化回歸誤差 | ⭐⭐⭐⭐⭐ |
| **損失** | MAE | 絕對誤差 | 穩健誤差估計 | ⭐⭐⭐⭐ |
| **損失** | Crossentropy | 分類誤差 | 量化分類錯誤 | ⭐⭐⭐⭐⭐ |
| **優化** | Adam | 自適應學習 | 自動調整速度 | ⭐⭐⭐⭐⭐ |
| **訓練** | Learning Rate | 步伐大小 | 控制更新幅度 | ⭐⭐⭐⭐⭐ |
| **訓練** | Epoch | 複習次數 | 充分學習 | ⭐⭐⭐⭐⭐ |
| **訓練** | Batch Size | 一次幾題 | 平衡效率品質 | ⭐⭐⭐⭐⭐ |
| **評估** | Accuracy | 答對率 | 整體表現 | ⭐⭐⭐⭐⭐ |
| **評估** | Precision | 不誤判 | 減少誤報 | ⭐⭐⭐⭐ |
| **評估** | Recall | 不漏掉 | 減少漏報 | ⭐⭐⭐⭐ |
| **評估** | R² | 解釋力 | 評估回歸品質 | ⭐⭐⭐⭐⭐ |
| **處理** | 標準化 | 統一量級 | 平衡特徵 | ⭐⭐⭐⭐⭐ |
| **處理** | One-Hot | 類別編碼 | 避免順序誤解 | ⭐⭐⭐⭐⭐ |
| **問題** | 過擬合 | 背答案 | 泛化不良 | ⭐⭐⭐⭐⭐ |
| **問題** | 欠擬合 | 學不會 | 模型太簡單 | ⭐⭐⭐⭐ |
| **技巧** | Dropout | 隨機丟棄 | 防止過擬合 | ⭐⭐⭐⭐⭐ |

---

## 💡 新手建議

### 學習順序
1. ✅ **先搞懂應變數和自變數**（這是基礎）
2. ✅ **記住激活函數和損失函數的搭配**
3. ✅ **優化器用 Adam，學習率用 0.001**
4. ✅ **注意過擬合問題**（訓練久 ≠ 好）
5. ✅ **多實作，多看別人的 code**

### 常用組合（直接套用）

**分類問題（如：Iris 花分類）**
```python
model = Sequential([
    Dense(64, activation='relu'),    # 隱藏層
    Dropout(0.3),
    Dense(3, activation='softmax')   # 輸出層（3類）
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
```

**回歸問題（如：房價預測）**
```python
model = Sequential([
    Dense(64, activation='relu'),    # 隱藏層
    Dropout(0.3),
    Dense(1)                         # 輸出層（回歸）
])

model.compile(
    loss='mse',
    optimizer='adam',
    metrics=['mae']
)
```

### 除錯清單
- ❌ 訓練集 99%，測試集 60% → **過擬合**，加 Dropout
- ❌ 訓練集 60%，測試集 58% → **欠擬合**，增加複雜度
- ❌ 損失不下降 → 檢查學習率、檢查資料標準化
- ❌ 損失是 NaN → 學習率太大，調小 10 倍

---

## 🎉 總結

機器學習雖然術語多，但核心概念不複雜：

1. **資料**：定義問題（應變數、自變數）
2. **模型**：學習規律（激活函數、層數）
3. **訓練**：優化參數（損失函數、優化器）
4. **評估**：檢驗效果（各種指標）
5. **改進**：解決問題（過擬合、欠擬合）

**記住：多動手實作，比看一百遍理論有用！** 💪

希望這份筆記對你有幫助！🚀
