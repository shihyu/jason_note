#11.4 C++ AMP與OpenCL對比

為了將OpenCL映射到一個新的編程模型中，先將一些重要的步驟進行映射。表11.1中就展示了OpenCL相關步驟與`C++ AMP`的對應關係。

表11.1 OpenCL的相關步驟在`C++ AMP`的對應情況

OpenCL|C++ AMP
----|----
內核|parallel_for_each中定義Lambda函數，或將一個函數直接指定給parallel_for_each
內核名|使用C++函數操作符直接調用給定函數或Lambda函數
內核啟動|parallel_for_each
內核參數|使用Lambda函數獲取變量
cl_mem內存|concurrency::array_view或array

圖11.1中使用了Lambda函數作為並行執行的具體實現。這就類似於OpenCL中的內核函數，其會並行執行在各個工作項上。

通常會使用內核名稱生成具體的OpenCL內核對象，這裡可以直接使用`c++`的函數操作符直接運行Lambda函數或給定的函數。`C++`中為了避免不命名衝突，使用了作用域規則生成對應的內核對象。

其餘的映射就是有關於主機端和設備端交互的。`C++ AMP`使用parallel_for_each來替代OpenCL中使用API進行傳遞內核函數和啟動內核函數的過程。圖11.1中使用的Lambda函數可以自動獲取相關的參數，也就是自動的完成了向內核傳遞參數這一步驟。另外，Lambda中所是用到的array_view，都可以認為是顯式的cl_mem內存。

從概念上了解了二者的映射關係後，對於`C++ AMP`編譯器來說只需要提供如下支持就可以：

1. 如何從周圍的代碼中獲取OpenCL內核所需要的參數。
2. 主機端需要在一條語句中完成創建內核、準備參數和內存，以及加載內核並運行的操作。

`C++`Lambda函數可以當做為匿名函數，為了更加貼近於`C++`我們會重寫這個Lambda表達式(如圖11.3所示)。

{%ace edit=false, lang='c_cpp'%}
class vecAdd{
  private:
  	array_view<const float, 1>va, vb;
    array_view<float, 1> vc;
  public:
  	vecAdd(array_view<const float, 1> a,
    	array_view<const float, 1> b,
        array_view<float, 1> c) restric(cpu)
      :va(a), vb(b), vc(c){}
    void operator()(index<1> idx) restrict(amp){
      cv[idx] = av[idx] + bv[idx];
    }
};
{%endace%}

圖11.3 仿函數版本的`C++ AMP`向量相加。

這段代碼中，顯式的將Lambda函數攜程一個仿函數版本，其可以獲取變量，va、vb和vc作為這個類的輸出，並且將Lambda作為函數操作符的函數體。最後，構造函數通過主機端傳入的參數生成對應的輸出。

不過，我們含有一些東西漏掉了：

1. 函數操作符在這裡與OpenCL內核相對應。`C++ AMP`中使用parallel_for_each執行對應內核函數。不過，該仿函數是一個類，需要我們先創建一個實例。
2. 運行時如何對內核的名稱進行推斷？
3. array_view在主機端可能包含有cl_mem，不過在OpenCL設備端只能操作原始cl_mem指針，而不允許在主機端對cl_mem指針進行操作。這種關係需要理清楚，以便滿足主機端和設備端不同的操作。

為了彌合這些漏洞，我們需要在圖11.3中的類中添加更多的東西。圖11.4中的第1、21、29和31行。

{%ace edit=false, lang='c_cpp'%}
// This is used to close the gap #3
template<class T>
class array_view{
#ifdef HOST_CODE
  cl_mem _backing_storage;
  T *_host_ptr;
#else
  T *_backing_storage;
#endif
  size_t _sz;
};

class vecAdd{
private:
  array_view<const float, 1> va, vb;
  array_view<float, 1> vc;
public:
  vecAdd(
    array_view<const float, 1> a,
    array_view<const float, 1> b,
    array_view<float, 1> c)restrict(cpu)
   :va(a), vb(b), vc(c){}
  // This new constructor is for closing gap #1
#ifndef HOST_CODE
  vecAdd(__global float *a, size_t as, __global float *b, size_t bs, __global float *c, size_t cs) restrict(amp)
   :va(a, as), vb(b, bs), vc(c, cs){}
  void operator()(index<1> idx) restrict(amp){
    cv[idx] = av[idx] + bv[idx];
  }
#endif
  // The following parts are added to close the gap #1 and #2
#ifndef HOST_CODE
  // This is to close the gap #2
  static const char *__get_kernel_name(void){
    return mangled name of "vecAdd::trampoline(const __global float *va, const __global float *vb, __global float *vc)"
  }
#else // This is to close the gap #1
  _kernel void trampoline(const __global float *va, size_t vas,   const __global float *vb, size_t vbs, __global float *vc, size_t vcs){
    vecAdd tmp(va, vas, vb ,vbs, vc, vcs);// Calls the new constructor at line 20
    index<1> i(get_gloabl_id(0));
    tmp(i);
  }
#endif
};
{%endace%}

圖11.4 擴展之後的向量相加——`C++ AMP`版本

圖11.4中的版本中，將三個遺留的問題進行彌補。第一行簡單的定義了一個`concurrency::array_view`，這個簡單定義並不表示其就是標準`concurrency::array_view`的實現。這裡使用的方式就是使用宏的方式，使得主機端和設備端所在同種容器中，使用不同的宏定義情況下，具有不同的類型數據成員。注意這裡我們將array_view看作為一個OpenCL內存，這裡我們就需要將OpenCL內存對象放入array_view中(命名為backing_storage)。同樣，我們也需要添加兩個成員函數，並且需要再定義一個新的構造函數，所以有些功能需要`C++ AMP`編譯器在編譯翻譯過程中進行添加：

- 需要在編譯主機端代碼階段獲取內核名稱
- 其次就是需要一個OpenCL內核，並且內核代碼只能在設備端編譯。並且運行時主機端就需要使用到這些內核的名字。編譯器需要將這些函數對象拷貝一份，並連同內核參數傳入設備端。
- 第22行的新構造函數會編譯成只有設備端能使用的代碼。其目的是使用新的構造函數，在GPU上夠造出一個Lambda函數的副本(攜帶參數)，參數通常使用的地址並不是外部使用的那些。這就給了編譯器相對自由的空間，並且可以在這之後進行一些優化。

不過，11.4圖中的代碼主要描繪了CLamp如何獲取每個函數的名稱(調用函數操作符)，新的構造函數會將Lambda表達式構造成適合於GPU端使用的函數，並且使用宏的方式讓同一個數組容器可以包含主機端或設備端的內存。最後，輸出的OpenCL代碼中，我們將數組容器中的cl_mem對象通過`clSetKernelArg()`API以一定的順序設置入OpenCL內核當中。為了滿足這些要求，需要實現的編譯器具有如下的功能：

- 從主機端的Lambda函數中獲取cl_mem對象，並通過OpenCL`clSetKernelArg()` API對內存對象進行設置。
- 將對應的內核參數地址進行查詢，並且調用新構造函數實例化與Lambda不太相同的設備端內核，以及內核參數。
- 參數的獲取應該不會受到影響，並且參數的傳遞過程是不透明的。例如，array_view中_sz的值就需要通過主機端傳遞給設備端。

為了系統的實現這些功能，需要清晰的指明，Lambda函數中需要哪種類型的數據。表11.2中描述了圖11.4中哪些數據需要在設備端和主機端使用。

表11.2 將主機端的數據成員與設備端進行映射

數據成員|主機端|設備端|注意
----|----|----|----
array_view<const float, 1> va| cl_mem va._backing_storage|__global float *va_backing_storage|通過clSetKernelArg進行傳遞
va的尺寸| size_t va._sz|size_t va._sz|字面方式傳遞
array_view<const float, 1> vb|cl_mem vb._backing_storage|__global float *vb._backing_storage|通過clSetKernelArg進行傳遞
vb的尺寸| size_t vb._sz|size_t vb._sz|字面方式傳遞
array_view<float, 1> vc|cl_mem vc._backing_storage|__global float *vc._backing_storage|通過clSetKernelArg進行傳遞
vc的尺寸| size_t vc._sz|size_t vc._sz|字面方式傳遞

根據對應關係，可以通過`C++ AMP`的parallel_for_each生成OpenCL內核。這可以通過圖11.4中的`C++`模板進行實現。基於表中的對應關係，可以實現對應的parallel_for_each，如圖11.5所示。

{%ace edit=false, lang='c_cpp'%}
template<class T>
void parallel_for_each(T k){
  // Locate the kernel source file or SPIR
  // Construct an OpenCL Kernel named k::__get_kernel_name()
  // We need to look into the objects
  clSetKernelArg(..., 0, k.va._backing_storage);// cf. line5 of Figure 3
  clSetKernelArg(..., 1, k.va._sz);
  clSetKernelArg(..., 2, k.vb._backing_storage);
  clSetKernelArg(..., 3, k.vb._sz);
  clSetKernelArg(..., 4, k.vc._backing_storage);
  clSetKernelArg(..., 5, k.vc._sz);
  // Invoke the kernel
  // We need to copy the results back if necessary from vc
}
{%endace%}

圖11.5 主機端的parallel_for_each實現(概念代碼)

為了產生適合圖11.5中的OpenCL內核點，我們需要將所要執行的對象進行遍歷，並篩選出相關的對象供內核函數調用(第6-11行)。另外，生成的內核代碼的參數順序也要和主機端實現對應起來。

之前，仿函數是一種向`C++ AMP`傳遞數據的方式，不過對於OpenCL內核來說，需要將相關的數據與內核參數順序進行對應。基本上CPU端的地址，都會要拷貝並轉化成GPU端能使用的地址：函數中有太多數據成員需要拷貝，這樣的拷貝最好放在初始化的時候去做。

我們之前通過值的方式進行OpenCL內存的傳遞。不過，複雜的地方在於我們如何將內存通過不透明的方式傳遞給對應句柄，需要依賴於OpenCL運行時創建對應的GPU內存空間，並將主機端的內存拷貝到GPU上。

這裡有的讀者可能會發現，圖11.5中的實例代碼有點類似於對象的[序列化](http://en.wikipedia.org/wiki/Serialization)。因為在實現的時候，我們不希望將相應的數據在外部進行存儲和檢索，所以我們需要壓縮更多的數據，並通過`clSetKernelArg()`將相關數據設置到GPU內部。

需要注意的是，其他語言中(比如：java)序列化和反序列通過代碼的順序進行反映，這種反映並不是`C++`源碼級別的，所以無需對編譯器進行很大的改動。`C++ AMP`編譯器中，序列化和反序列化代碼的順序，都會通過枚舉數據變量的方式，進行內核參數的設置。





