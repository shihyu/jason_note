#11.11 完整例子:二項式期權

本節中，我們以一個應用開發者的角度來看一個較為複雜的例子。這裡的代碼需要使用之前章節所提到的編譯技術進行轉換，轉換成一個正確，且有較高性能OpenCL實現。應用我們選擇了二項式期權。注意，我們不會從數學和經濟學的角度深度探討該問題，只是對於編譯器作者來說，將其做為一個完整的例子。

```c++
void binomial_options_gpu(
  std::vector<float> &v_s,
  std::vector<float> &v_x,
  std::vector<float> &v_vdt,
  std::vector<float> &v_pu_by_df,
  std::vector<float> &v_pd_by_df,
  std::vector<float> &call_value)
```

上面的代碼就是二項式期權函數的聲明，其中call_value作為存儲最終結果的對象，其他的參數都僅作為輸入參數。

```c++
extent<1> e(data_size);
arrar_view<float, 1> av_call_value(e, call_value);
av_call_value.discard_data();
```

為了將輸入數據輸入內核函數，數據需要通過`C++ AMP`的容器進行包裝。本例中，使用`concurrency::array_view`。av_call_value對象調用discar_data，就是用來告訴運行時，這段數據無需從主機端拷貝到設備端。

```c++
array_view<const float, 1> av_s(e, v_s);
array_view<const float, 1> av_x(e, v_x);
array_view<const float, 1> av_vdt(e, v_vdt);
array_view<const float, 1> av_pu_by_df(e, v_pu_by_df);
array_view<const float, 1> av_pd_by_df(e, v_pd_by_df);

exten<1> ebuf(MAX_OPTIONS * (NUM_STEPS + 16));
array<float, 1> a_call_buffer(ebuf);
```

注意這裡av_s，av_x，av_vdt，av_pu_by_df，av_pd_by_df均由array_view包裝，也就是在計算完成後不需要拷貝回主機。

```c++
extent<1> compute_extent(CACHE_SIZE * MAX_OPTIONS);
parallel_for_each(compute_extent.tile<CACHE_SIZE>(),
  [=, &a_call_buffer]tile_index<CACHE_SIZE> ti)restrict(amp){
    binomial_options_gpu(ti, av_s, av_x, av_vdt, av_pu_by_df, av_pd_by_df, av_call_value, a_call_buffer);
  });
av_call_value.synchronize();
```

`C++ AMP`使用parallel_for_each完成計算。在計算完成之後，使用同步成員函數對計算結果進行同步，以確保所有計算結果都已經保存在容器中。所有使用到的數據都會在運行時進行隱式處理。編程者不需要顯式的在設備和主機之間進行數據的傳遞或拷貝。注意parallel_for_each使用顯式線程劃分進行線程局部控制。

```c++
void binomial_options_kernel(
  tiled_index<CACHE_SIZE> &tidx,
  array_view<const float, 1> s,
  array_view<const float, 1> x,
  array_view<const float, 1> vdt,
  array_view<const float, 1> pu_by_df,
  array_view<const float, 1> pd_by_df,
  array_view<float, 1> call_value,
  array<float, 1> &call_buffer) restrict(amp){
  
  index<1> tile_idx = tidx.tile;
  index<1> local_idx = tidx.local;
  
  tile_static float call_a[CACHE_SIZE + 1];
  tile_static float call_b[CACHE_SIZE + 1];
  
  int tid = local_idx[0];
  int i;
  
  for (i = tid; i <= NUM_STEPS; i+= CACHE_SIZE){
    index<1> idx(tile_idx[0] * (NUM_STEPS + 16) + (i));
    call_buffer[idx] = expiry_call_value(s[tile_idx], x[tile_idx], vdt[tile_idx], i);
  }
  
  for (i = NUM_STEPS; i > 0; i -= CACHE_DELTA){
    for (int c_base = 0; c_base < i; c_base += CACHE_STEP){
      int c_start = min(CACHE_SIZE - 1, i - c_base);
      int c_end = c_start - CACHE_DELTA;
      
      tidx.barrier.wait();
      if (tid <= c_start){
        index<1> idx(tile_idx[0] * (NUM_STEPS + 16) + (c_base + tid));
        call_a[tid] = call_buffer[idx];
      }
      
      for (int k = c_start - 1; k >= c_end;){
        tidx.barrier.wait();
        call_b[tid] = pu_by_df[tile_idx] * call_a[tid + 1] + pd_by_df[tile_idx] * call_a[tid];
        k--;
        
        tidx.barrier.wait();
        call_a[tid] = pu_by_df[tile_idx] * call_b[tid + 1] + pd_by_df[tile_idx] * call_b[tid];
        k--;
    }
    
    tidx.barrier.wait();
    if (tid <= c_end){
      index<1> idx(tile_idx[0] * (NUM_STEPS + 16) + (c_base + tid));
      call_buffer[idx] = call_a[tid];
    }
  }
  
  if (tid == 0){
    call_value[tile_idx] = call_a[0];
  }
}
```

聲明為tile_static類型的數據將在同一工作組內進行共享。為了確保共享數據的一致性，我們這裡使用了tidx.barrier.wait函數。在同一工作組內的工作項將會在這個調用點進行等待，直到工作組內所有線程都到達該調用點為止。



