#11.7 OpenCL 2.0提出共享虛擬內存的原因

OpenCL 2.0中SVM共享虛擬內存特性的添加尤為重要。這讓主機端和設備端可以訪問同一上下文中的地址空間。其也支持在主機端和內核間傳遞自定義結構體的指針。這個特性可以理解成將設備端的全局內存擴張到了主機端內存空間中，因此OpenCL中的工作項就能夠訪問主機端地址空間的數據。

根據OpenCL 2.0標準，SVM分為三種：

1. 粗粒度SVM：數據以OpenCL內存對象的形式共享。其具有一些同步點——內核執行、映射和逆映射。
2. 細粒度SVM：數據以OpenCL內存對象的形式共享。不需要進行顯式同步，不過在主機端和設備端對相應內存進行原子操作時，內存數據需要進行同步。
3. 系統細粒度SVM：設備端和主機端使用的內存可以認為是完全一樣的，內存對象使用的方式和C/C++一樣。

表11.3展示了2.0之前、粗粒度SVM和細粒度SVM間的不同。

表11.3 OpenCL共享數據的行為對比

操作|2.0之前|粗粒度SVM|細粒度SVM
----|----|----
數據拷貝到設備端|clEnqueueWriteBuffer|不需要|不需要
設備端執行原子操作對主機端可見|不適用|不可見|可見
設備端對數據進行修改對主機端可見|需要進行拷貝之後可見|內核執行完成時|內核執行完成後，或在設備端執行原子操作之後
數據從設備端拷貝到主機端|clEnqueueReadBuffer|不需要|不需要

細粒度SVM具有一個特殊能力，就是支持主機端和設備同時對同一塊內存地址進行原子操作。直方圖統計的例子中，使用細粒度SVM就可以和主機或其他設備合作完成該工作；他們都共享同一塊內存，並且都能看到相應的數據，在原子操作完成後，每個設備上都會看到最新的數據。

OpenCL 2.0中，粗粒度SVM是強制要求支持，其他兩種類型可以選擇支持。本節中，我們就來聊一下如何在`C++ AMP`中使用粗粒度SVM。注意，粗粒度SVM有點類似於OpenCL 1.x中的內存對象，不過其不需要顯式的通過`clEnqueueWriteBuffer()`進行寫入。因為相似，CLamp將組粒度SVM認為是一種性能有所提升的`concurrency::array_view`實現。

為了利用粗粒度SVM，主機端需要調用`clSVMAlloc()`分配出SVM內存，這段內存可以在主機端和設備端共享。cl_mem對象是通過`clCreateBuffer()`通過傳入`CL_MEM_USE_HOST_PTR`參數和主機端內存指針進行創建。這裡`clSVMAlloc()`分配出的指針就類似於傳入的主機端指針。

同樣，內存中的內容設備端和主機端也是能夠自動共享的。這裡就不需要再去調用`clEnqueueWriteBuffer()`和`clEnqueueReadBuffer()`，來完成設備端和主機端數據進行共享了。

當不在需要SVM內存，可以通過`clReleaseMemObject()`API對粗粒度SVM對象進行釋放。之後，還需要通過`clSVMFree()`API銷燬SVM內存所開闢的空間。

