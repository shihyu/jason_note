# 8.1 線程間劃分工作

假設作為建造一座房子總工程師，為了完成房屋的建造，需要挖地基、砌牆、添置水暖、接入電線等等。理論上，如果你很擅長建造屋子，這些事情都可以由自己來完成，但是這樣就要花費很長時間，並且需要不斷的切換任務。你也可以僱傭一些人來幫助你完成房子的建造。那麼現在需要決定僱多少人，以及僱傭人員具有什麼樣的技能。僱傭的人到了後，進展速度就要比之前快很多。

當然，你也可以僱傭個包工隊(專家組)，由瓦工、木匠、電工和水管工組成。隊員們只做自己擅長的，所以當沒有水暖任務時，水管工會休息。因為人多的緣故，要比之前一個人的速度快很多，並且水管工在收拾廁所的時候，電工可以將電線連接到廚房，不過當沒有屬於自己的任務時就會休息。即使有人在休息，你可能還是能感覺到包工隊要比僱傭一個什麼都會的人快。包工隊不需要更換工具，並且每個人的任務都要比會的人做的快。是快還是慢，取決於特定的情況——需要嘗試和觀察。

即使僱傭包工隊，你依舊可以選擇人數不同的團隊(可能在一個團隊中，瓦工的數量超過電工)。同樣，這會是一種補足，並且在建造不止一座房子的時候，會改變整體效率。即使水管工沒有太多的任務，在建造過一次房子後，你依舊能讓他總是處於忙碌的狀態。當然，即使每次工作只有那麼幾個人工作，你還需要負擔整個團隊的開銷。

建造例子已經足夠說明問題，這與線程有什麼關係呢？這些問題也會發生在線程上。需要決定使用多少個線程，並且這些線程應該去做什麼。需要決定是使用“全能”線程去完成所有的任務，還是使用“專業”線程去完成一件事情，或將兩種方法混合。使用併發時，需要作出諸多選擇來驅動併發，選擇會決定代碼的性能和可讀性。因此選擇至關重要，所以設計應用結構時，需要作出適當的決定。本節中，將看到很多劃分任務的技術。

## 8.1.1 對數據進行預處理劃分

最簡單的並行算法就是並行化的`std::for_each`，會對數據集中每個元素執行同一個操作。為了並行化該算法，可為數據集中每個元素分配一個線程。如何劃分才能獲得最佳的性能，很大程度上取決於數據結構實現的細節，之後有關性能問題的章節會再提及。

最簡單的分配方式：第一組N個元素分配一個線程，下一組N個元素再分配一個線程，以此類推，如圖8.1所示。不管數據怎麼分，每個線程都會對分配給它的元素進行操作，但不會和其他線程進行溝通，直到處理完成。

![](../../images/chapter8/8-1.png)

圖8.1 向線程分發連續的數據塊

使用過*MPI*(Message Passing Interface)[1]和OpenMP[2]的同學對這個結構一定很熟悉：一項任務被分割成多個，放入一個並行任務集中，執行線程獨立的執行這些任務，結果在主線程中合併。這種方式在2.4節中的accumulate的例子中使用過，所有並行任務和主線程的任務都是累加和。對於for_each來說，主線程將無事可做，因為這個計算不需要處理最終結果。

最後一步對於並行程序來說十分重要，(如代碼2.8中的實現)最後一步就是串行的。不過，這一步同樣也是能並行化。accumulate實際上是一個遞減操作，所以當線程數量大於一個線程上最小處理項時，可以對accumulate進行遞歸調用。或者工作線程就像做一個完整的任務一樣，對步驟進行遞減。

雖然這個技術十分強大，但是並不是哪裡都適用。有時不能像之前那樣，對任務進行整齊的劃分，因為只有對數據進行處理後，才能進行明確的劃分。這裡的方式特別適用了遞歸算法，下面就來看看這種特別的方式。

## 8.1.2 遞歸劃分

快速排序有兩個最基本的步驟：將數據劃分到中樞元素之前或之後，然後對中樞元素之前和之後的兩半數組再次進行快速排序。這裡不能通過對數據的簡單劃分達到並行，因為只有在一次排序結束後，才能知道哪些項在中樞元素之前和之後。當要對這種算法進行並行化，很自然的會想到使用遞歸。每一級的遞歸都會多次調用quick_sort函數，因為需要知道哪些元素在中樞元素之前和之後。遞歸調用是完全獨立的，因為其訪問的是不同的數據集，並且每次迭代都能併發執行。圖8.2展示了這樣的遞歸劃分。

![](../../images/chapter8/8-2.png)

圖 8.2 遞歸劃分數據

第4章中已經見過這種實現，比起對數據塊遞歸調用函數，使用`std::async()`可以為每一級生成小於數據塊的異步任務。使用`std::async()`時，C++線程庫就能決定何時讓一個新線程執行任務，並對任務進行同步。

對一個很大的數據集進行排序時，每層遞歸都產生新線程，最後就會產生大量的線程。大量線程會對性能有影響，如果線程太多，應用將會運行的很慢。如果數據集過於龐大，會將資源耗盡。所以在遞歸的基礎上進行任務的劃分，就是一個不錯的主意。只需要將一定數量的數據打包後，交給線程即可。`std::async()`可以處理這種簡單的情況，當然也有其他組選擇。

另一種選擇是使用`std::thread::hardware_concurrency()`函數來確定線程的數量，就像代碼2.8中的accumulate()一樣。將已排序的數據推到線程安全的棧上(如第6、7章中提及的棧)。線程無所事事時，不是已經完成對自己數據塊的梳理，就是在等待一組排序數據的產生。所以，線程可以從棧上獲取這組數據，並且對其排序。

下面的代碼就是使用以上方式實現。與大多數示例一樣，只是演示，而非工業代碼。如果編譯器支持C++17，最好使用標準庫提供的並行算法，這些會在第10章再來討論。

代碼8.1 使用棧的並行快速排序算法——等待數據塊排序

```c++
template<typename T>
struct sorter  // 1
{
  struct chunk_to_sort
  {
    std::list<T> data;
    std::promise<std::list<T> > promise;
  };

  thread_safe_stack<chunk_to_sort> chunks;  // 2
  std::vector<std::thread> threads;  // 3
  unsigned const max_thread_count;
  std::atomic<bool> end_of_data;

  sorter():
    max_thread_count(std::thread::hardware_concurrency()-1),
    end_of_data(false)
  {}

  ~sorter()  // 4
  {
    end_of_data=true;  // 5

    for(unsigned i=0;i<threads.size();++i)
    {
      threads[i].join();  // 6
    }
  }

  void try_sort_chunk()
  {
    boost::shared_ptr<chunk_to_sort > chunk=chunks.pop();  // 7
    if(chunk)
    {
      sort_chunk(chunk);  // 8
    }
  }

  std::list<T> do_sort(std::list<T>& chunk_data)  // 9
  {
    if(chunk_data.empty())
    {
      return chunk_data;
    }

    std::list<T> result;
    result.splice(result.begin(),chunk_data,chunk_data.begin());
    T const& partition_val=*result.begin();

    typename std::list<T>::iterator divide_point=  // 10
       std::partition(chunk_data.begin(),chunk_data.end(),
        [&](T const& val){return val<partition_val;});

    chunk_to_sort new_lower_chunk;
    new_lower_chunk.data.splice(new_lower_chunk.data.end(),
       chunk_data,chunk_data.begin(),
       divide_point);

    std::future<std::list<T> > new_lower=
      new_lower_chunk.promise.get_future();
    chunks.push(std::move(new_lower_chunk));  // 11
    if(threads.size()<max_thread_count)  // 12
    {
      threads.push_back(std::thread(&sorter<T>::sort_thread,this));
    }

    std::list<T> new_higher(do_sort(chunk_data));

    result.splice(result.end(),new_higher);
    while(new_lower.wait_for(std::chrono::seconds(0)) !=
       std::future_status::ready)  // 13
    {
      try_sort_chunk();  // 14
    }

    result.splice(result.begin(),new_lower.get());
    return result;
  }

  void sort_chunk(boost::shared_ptr<chunk_to_sort> const& chunk)
  {
    chunk->promise.set_value(do_sort(chunk->data));  // 15
  }

  void sort_thread()
  {
    while(!end_of_data)  // 16
    {
      try_sort_chunk();  // 17
      std::this_thread::yield();  // 18
    }
  }
};

template<typename T>
std::list<T> parallel_quick_sort(std::list<T> input)  // 19
{
  if(input.empty())
  {
    return input;
  }
  sorter<T> s;

  return s.do_sort(input);  // 20
}
```

parallel_quick_sort函數⑲代表了sorter類①的功能，支持在棧上簡單的存儲無序數據塊②，並且對線程進行設置③。do_sort成員函數⑨主要是對數據進行劃分⑩。相較於對每個數據塊產生新線程，這次會將數據塊推到棧上⑪，並使用備用處理器⑫產生新線程。因為小於部分的數據塊可能由其他線程進行處理，就需要等待線程完成⑬。為了讓所有事情順利進行(只有一個線程和其他所有線程都忙碌時)，線程處於等待狀態時⑭，就讓當前線程嘗試處理棧上的數據。try_sort_chunk只是從棧上彈出一個數據塊⑦，並且對其進行排序⑧，將結果存在promise中，讓線程對已存在於棧上的數據塊進行提取⑮。

當沒有設置end_of_data時⑯，新生成的線程還在嘗試從棧上獲取需要排序的數據塊⑰。循環檢查中，也要給其他線程機會⑱，可以從棧上取下數據塊進行操作。這裡的實現依賴於sorter類④對線程的清理。當所有數據都已完成排序，do_sort將會返回(即使還有工作線程在運行)，所以主線程將會從parallel_quick_sort⑳中返回，之後會銷燬sorter對象。析構函數會設置end_of_data標誌⑤，以及等待所有線程完成工作⑥，標誌將決定是否終止線程函數的內部循環⑯。

這個方案中，不用為spawn_task產生的無數線程所困擾，也不用再依賴C++線程庫，這裡可以選擇執行線程的數量。該方案的線程數量就是`std::thread::hardware_concurrency()`，這樣就能避免任務過於頻繁的切換。不過，這裡還有兩個問題：線程管理和通訊。要解決這兩個問題就要增加代碼的複雜度，雖然線程對數據項分開處理，不過所有對棧的訪問都可以向棧添加新的數據塊，並且移出數據塊以作處理。重度競爭會降低性能，原因會在後面給出。

這個方案使用到了特殊的線程池——所有線程任務都來源於一個等待鏈表，然後線程會去完成任務，任務完成後會再來鏈表提取任務。這個線程池會出問題(包括對工作鏈表的競爭)，問題的解決方案將在第9章提到。關於多處理器的問題，將會在本章後面的章節中做出更為詳細的介紹(詳見8.2.1)。

任務幾種劃分方法：處理前劃分和遞歸劃分(都需要事先知道數據的長度固定)，還有上面的劃分方式。事情並非總是這樣好解決，當數據是動態生成或是通過外部輸入，那這裡的辦法就不適用了。這種情況下，基於任務類型的劃分方式，就要好於基於數據的劃分方式。

## 8.1.3 通過任務類型劃分

雖然會為每個線程分配不同的數據塊，因為這裡每個線程對每個數據塊的操作是相同的，所以工作的劃分(無論是之前就劃分好，還是使用遞歸的方式劃分)仍停留在理論階段。另一種選擇是讓線程做專門的工作，就是每個線程做不同的工作，就像水管工和電工在建造一所屋子的時候，所做的不同工作那樣。線程可能會對同一段數據進行操作，但對數據進行不同的操作。

對分工的排序，也就是分離關注點。每個線程都有不同的任務，這意味著真正意義上的線程獨立。其他線程偶爾會向特定線程交付數據，或是通過觸發事件的方式來進行處理。不過總體而言，每個線程只需要關注自己所要做的事情即可。其本身就是良好的設計，每一段代碼只對自己的部分負責。

**分離關注**

當有多個任務需要持續運行一段時間，或需要及時進行處理的事件(比如，按鍵事件或傳入網絡數據)，且還有其他任務正在運行時，單線程應用採用的是單責任原則處理衝突。單線程中代碼會執行任務A(部分)後，再去執行任務B(部分)，再檢查按鈕事件，再檢查傳入的網絡包，然後在循環回去，執行任務A。這將會使得任務A複雜化，因為需要存儲完成狀態，以及定期從主循環中返回。如果在循環中添加了很多任務，那麼程序將運行的很慢。並且用戶會發現，在按下按鍵後，很久之後才會有反應。我確定你已經在一些程序中見過這種情況：給程序分配一項任務後，發現接口會封鎖，直到這項任務完成。

當使用獨立線程執行任務時，操作系統會幫忙處理接口問題。執行任務A時，線程可以專注於執行任務，而不用為保存狀態從主循環中返回。操作系統會自動保存狀態，當需要的時候將線程切換到任務B或任務C。如果目標系統是帶有多核或多處理器，任務A和任務B可很可能真正的併發執行。處理按鍵時間或網絡包的代碼，就能及時執行了。所有事情都完成的很好，用戶得到了及時的響應。當然，作為開發者只需要寫具體操作的代碼即可，不用將控制分支和用戶交互混在一起了。

聽起來不錯，玫瑰色的願景呀。事實真會如上面所說的那樣簡單？一切取決於細節。如果每件事都是獨立的，線程間就不需要交互，這樣的話一切都很簡單。不幸的是，現實沒那麼美好。後臺那些優雅的任務，經常會被用戶要求做一些事情，並且它們需要通過更新用戶接口的方式，來讓用戶知道它們完成了任務。或者，用戶可能想要取消任務，就需要用戶向接口發送一條消息，告知後臺任務停止運行。這兩種情況都需要認真考慮、設計、以及適當的同步，不過這裡還是會對分離的部分產生擔心。用戶接口線程只能處理用戶接口，當其他線程告訴該線程要做什麼時，用戶接口線程會進行更新。同樣，後臺線程只運行它們所關注的任務，有時會發生“允許任務被其他線程所停止”的情況。這兩種情況下，後臺線程需要照顧來自其他線程的請求，線程本身只知道它們的請求與自己的任務有所關聯。

多線程下有兩個危險需要分離。第一個是對錯誤的擔憂(主要表現為線程間共享著很多的數據)，第二是不同的線程要相互等待，這兩種情況都是因為線程間很密切的交互。這種情況發生時，就需要看一下為什麼需要這麼多交互。當所有交互都有同樣的問題，就應該使用單線程來解決，並將引用同一源的線程提取出來。或者當有兩個線程需要頻繁的交流，在沒有其他線程時，就可以將這兩個線程合為一個線程。

當通過任務類型對線程間的任務進行劃分時，不應該讓線程處於隔離態。當多個輸入數據集需要使用同樣的操作序列，可以將序列中的操作分成多個階段讓線程執行。

**劃分任務序列**

當任務會應用到相同操作序列，去處理獨立的數據項時，就可以使用*流水線*(pipeline)系統進行併發。這好比一個物理管道：數據流從管道一端進入，進行一系列操作後，從管道另一端出去。

使用這種方式劃分工作，可以為流水線中的每一階段操作創建一個獨立線程。當一個操作完成，數據元素會放在隊列中，供下一階段的線程使用。這就允許第一個線程在完成對於第一個數據塊的操作時，第二個線程可以對第一個數據塊執行管線中的第二個操作。

這就是線程間劃分數據的一種替代方案(如8.1.1描述)，這種方式適合於操作開始前，且對輸入數據處長度不清楚的情況。例如：數據來源可能是從網絡，或者可能是通過掃描文件系統來確定要處理的文件。

流水線對於隊列中的耗時操作處理也很合理，通過對線程間任務的劃分，就能對應用的性能有所改善。假設有20個數據項，需要在四核的機器上處理，並且每一個數據項需要四個步驟來完成操作，每一步都需要3秒來完成。如果將數據分給了四個線程，每個線程上就有5個數據項要處理。假設在處理時，沒有其他線程對處理過程進行影響，在12秒後4個數據項處理完成，24秒後8個數據項處理完成，以此類推。當20個數據項都完成操作，就需要1分鐘的時間。管線中就會完全不同，四步可以交給四個內核，第一個數據項可以被每一個核進行處理，所以其還是會消耗12秒。在12秒後你就能得到一個處理過的數據項，這相較於數據劃分並沒有好多少。不過，當流水線動起來，事情就會不一樣了。第一個核處理第一個數據項後，數據項就會交給下一個內核，所以第一個核在處理完第一個數據項後，其還可以對第二個數據項進行處理。在12秒後，每3秒將會得到一個已處理的數據項，這就要好於每隔12秒完成4個數據項。

為什麼整批處理的時間要長於流水線呢？因為最後一個核在開始處理第一個元素時，等待了9秒。更平滑的操作能在某些情況下獲益更多，考慮如下情況：當一個系統用來播放高清數字視頻。為了讓視頻能夠播放，至少要保證25幀每秒的解碼速度。這些圖像需要有均勻的間隔，才會給觀眾留有連續播放的感覺。一個應用可以在1秒解碼100幀，不過在解完就需要暫停1s的時候，這個應用就沒有意義了。另一方面，觀眾能接受在視頻開始播放的時候有一定的延遲。這種情況，並行使用流水線就能得到穩定的解碼率。

看了這麼多線程間劃分工作的技術，接下來讓我們來看一下在多線程系統中有哪些因素會影響性能，並瞭解一下這些因素是如何影響劃分方案的。

----------

[1] http://www.mpi-forum.org/

[2] http://www.openmp.org/
