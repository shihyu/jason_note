不仅仅是在做出权衡的决定时，我们必须站在性能数据的基础上。毕竟，如果不知道以缓存最优顺序和随机顺序访问数据要花费多少成本时，要如何才能决定以实现高效的内存访问呢？这又回到了性能的第一条规则，现在应该已经记住了:\textit{永远不要猜测性能}。如果程序是散布在白板上的设计图，那么这就是说起来容易做起来难了。

无法运行设计，如何获得测试数据来指导和支持设计决策呢？有些知识来自于经验。并不是指那种“我们一直都是这么做的”的体验。开发者可能已经设计和实现了类似的组件和新系统的其他部分。如果可以重用，则会提供可靠的性能信息。但即使必须修改它们或设计类似的东西，也已经有了高度相关的性能指标数据，并且可以很好地转移到新设计中。 

如果没有可以用来衡量性能的相关程序，是否应该这样做呢？这时需要依靠模型和原型。模型需要人工构造，据我们所知，它可以模拟未来程序某些部分的预期工作的负载和性能。一种模型是，若必须决定在内存中组织大量的数据，需要知道经常处理整个数据语料库，微基准。另一种模型，也可能会使用为链表和数组处理相同体积的数据组织。不是对未来项目性能的精确测试，但它提供了有价值的信息，并为决策提供了良好的数据支持。记住，模型越接近，预测就越不准确。若对两个可选设计进行建模，并且得出的性能测量值相差不超过10\%时，可能会认为这是完全正确的。顺便说一下，这并不是浪费时间。这样做会获得了重要的信息，两种设计选项都提供了类似的性能，所以可以根据其他标准自由选择。 

并非所有的模型都是微基准测试，可以使用现有的程序来建模新的行为。假设有一个分布式程序，它对某些数据的操作与下一个程序需要处理的数据类似。新程序将有更多的数据，而且相似性只是表面上的(可能两个程序都处理字符串)，因此旧程序不能用于处理新数据的实际测试。没关系，可以修改代码来发送和接收更长的字符串。如果现有的程序不使用它们怎么办？也没关系，可以编写一些代码以一种比较实际的方式生成和使用这些字符串，并将其嵌入到程序中。现在可以启动程序中进行分布式计算的部分，看看发送和接收预期的数据量需要多长时间(假设它需要足够长的时间来压缩)。这里可以做得更好，向代码中添加压缩，并比较网络传输速度与压缩和解压缩的成本。如果不想花费大量时间为特定数据编写实际的压缩算法，那么可以尝试使用现有的压缩库。在免费的库中比较几种压缩算法可以提供更有价值的数据，以便在以后根据数据量决定使用哪个压缩库。 

仔细注意刚才所做的。使用一个现有程序作为框架来运行一些新代码，这些新代码近似于未来程序的行为。换句话说，我们已经构建了一个原型。这是另一种获得原型设计性能评估的方法。当然，为性能构建原型与基于功能的原型有所不同。在后一种情况下，希望快速地组合一个系统来演示所需的行为，通常不考虑实现的性能或质量。性能原型应该给我们提供合理的性能数字，因此底层实现必须高效，可以忽略特殊情况和错误处理。也可以跳过许多特性，只要原型能够执行想要进行基准测试代码即可。有时，原型根本没有任何功能。相反，在代码的某个地方，可以硬编码一个条件。在实际系统中，在执行某些功能时，会发生这种情况。在这样的原型创建过程中，创建的高性能代码通常会形成底层库的基础。

所有的模型都是近似的，而且即使有一个完整的、最终的代码实现，仍然是近似。微基准测试通常不如大型框架准确，这就产生了像“微基准测试是谎言”这样吸引眼球的标题。微基准测试和其他性能模型，并不总是与最终结果匹配的主要原因是，程序的性能都受其环境的影响。若为最佳内存访问对一段代码进行基准测试，结果却发现它通常与其他完全饱和内存总线的线程一起运行。 

正如理解模型的局限性很重要一样，不要\textit{反应过度}也很重要。基准测试确实提供了有用的信息。测试软件越完整、越真实，测量结果越准确。如果基准测试显示一段代码比另一段快几倍，那么当代码在最终上下文中运行，这种差异完全消失的可能性就很低。除了运行在真实数据上的代码的最终版本之外，尝试从其他任何地方获得最后5\%的效率就是很愚蠢的想法了。

原型——模拟真实程序以某种方式再现我们感兴趣的特性的方法——允许从不同的设计决策中获得合理的性能评估。可以是微观基准册测试，也可以是大型的、已经存在的项目的实验，但它们都有一个目标:将性能设计从猜测的领域，转移到基于测试驱动的决策上。 
