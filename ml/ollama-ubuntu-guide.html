<!DOCTYPE HTML>
<html lang="zh" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Ollama Ubuntu æŒ‡å— - Jason&#x27;s Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Jason&#x27;s Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/shihyu/jason_note" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="ollama-åœ¨-ubuntu-2404-å®Œæ•´ä½¿ç”¨æŒ‡å—"><a class="header" href="#ollama-åœ¨-ubuntu-2404-å®Œæ•´ä½¿ç”¨æŒ‡å—">Ollama åœ¨ Ubuntu 24.04 å®Œæ•´ä½¿ç”¨æŒ‡å—</a></h1>
<h2 id="ç›®éŒ„"><a class="header" href="#ç›®éŒ„">ç›®éŒ„</a></h2>
<ul>
<li><a href="#%E5%BF%AB%E9%80%9F%E9%96%8B%E5%A7%8B">å¿«é€Ÿé–‹å§‹</a></li>
<li><a href="#%E5%AE%89%E8%A3%9D">å®‰è£</a></li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C">åŸºæœ¬æ“ä½œ</a></li>
<li><a href="#%E7%B0%A1%E5%96%AE%E7%AF%84%E4%BE%8B">ç°¡å–®ç¯„ä¾‹</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9E%8B%E6%8E%A8%E8%96%A6">å¸¸ç”¨æ¨¡å‹æ¨è–¦</a></li>
<li><a href="#%E5%AF%A6%E7%94%A8%E6%8A%80%E5%B7%A7">å¯¦ç”¨æŠ€å·§</a></li>
<li><a href="#web-ui-%E8%A8%AD%E5%AE%9A">Web UI è¨­å®š</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4">æ•…éšœæ’é™¤</a></li>
<li><a href="#%E9%80%B2%E9%9A%8E%E8%A8%AD%E5%AE%9A">é€²éšè¨­å®š</a></li>
<li><a href="#api-%E5%8F%83%E8%80%83">API åƒè€ƒ</a></li>
</ul>
<h2 id="å¿«é€Ÿé–‹å§‹"><a class="header" href="#å¿«é€Ÿé–‹å§‹">å¿«é€Ÿé–‹å§‹</a></h2>
<pre><code class="language-bash"># ä¸€éµå®‰è£
curl -fsSL https://ollama.com/install.sh | sh

# åŸ·è¡Œç¬¬ä¸€å€‹æ¨¡å‹
ollama run tinyllama

# è¼¸å…¥å•é¡Œé–‹å§‹å°è©±
&gt;&gt;&gt; ä½ å¥½ï¼Œè«‹è‡ªæˆ‘ä»‹ç´¹
</code></pre>
<h2 id="å®‰è£"><a class="header" href="#å®‰è£">å®‰è£</a></h2>
<h3 id="ç³»çµ±éœ€æ±‚"><a class="header" href="#ç³»çµ±éœ€æ±‚">ç³»çµ±éœ€æ±‚</a></h3>
<ul>
<li>Ubuntu 24.04 LTS</li>
<li>æœ€å°‘ 4GB RAMï¼ˆå»ºè­° 8GB ä»¥ä¸Šï¼‰</li>
<li>10GB å¯ç”¨ç¡¬ç¢Ÿç©ºé–“</li>
<li>(é¸ç”¨) NVIDIA GPU with CUDA 11.8+</li>
</ul>
<h3 id="å®‰è£æ–¹æ³•"><a class="header" href="#å®‰è£æ–¹æ³•">å®‰è£æ–¹æ³•</a></h3>
<h4 id="æ–¹æ³•-1å®˜æ–¹è…³æœ¬æ¨è–¦"><a class="header" href="#æ–¹æ³•-1å®˜æ–¹è…³æœ¬æ¨è–¦">æ–¹æ³• 1ï¼šå®˜æ–¹è…³æœ¬ï¼ˆæ¨è–¦ï¼‰</a></h4>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<h4 id="æ–¹æ³•-2æ‰‹å‹•å®‰è£"><a class="header" href="#æ–¹æ³•-2æ‰‹å‹•å®‰è£">æ–¹æ³• 2ï¼šæ‰‹å‹•å®‰è£</a></h4>
<pre><code class="language-bash"># ä¸‹è¼‰äºŒé€²ä½æª”æ¡ˆ
wget https://github.com/ollama/ollama/releases/latest/download/ollama-linux-amd64

# è³¦äºˆåŸ·è¡Œæ¬Šé™
chmod +x ollama-linux-amd64

# ç§»å‹•åˆ°ç³»çµ±è·¯å¾‘
sudo mv ollama-linux-amd64 /usr/local/bin/ollama

# å»ºç«‹æœå‹™æª”æ¡ˆ
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama
</code></pre>
<h3 id="é©—è­‰å®‰è£"><a class="header" href="#é©—è­‰å®‰è£">é©—è­‰å®‰è£</a></h3>
<pre><code class="language-bash"># æª¢æŸ¥ç‰ˆæœ¬
ollama --version

# æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ollama
</code></pre>
<h2 id="åŸºæœ¬æ“ä½œ"><a class="header" href="#åŸºæœ¬æ“ä½œ">åŸºæœ¬æ“ä½œ</a></h2>
<h3 id="æœå‹™ç®¡ç†"><a class="header" href="#æœå‹™ç®¡ç†">æœå‹™ç®¡ç†</a></h3>
<pre><code class="language-bash"># æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ollama

# å•Ÿå‹•æœå‹™
sudo systemctl start ollama

# åœæ­¢æœå‹™
sudo systemctl stop ollama

# é‡å•Ÿæœå‹™
sudo systemctl restart ollama

# è¨­å®šé–‹æ©Ÿè‡ªå‹•å•Ÿå‹•
sudo systemctl enable ollama

# æª¢è¦–æœå‹™æ—¥èªŒ
journalctl -u ollama -f
</code></pre>
<h3 id="æ¨¡å‹ç®¡ç†"><a class="header" href="#æ¨¡å‹ç®¡ç†">æ¨¡å‹ç®¡ç†</a></h3>
<pre><code class="language-bash"># ä¸‹è¼‰ä¸¦åŸ·è¡Œæ¨¡å‹
ollama run llama3.2:3b

# åªä¸‹è¼‰æ¨¡å‹ä¸åŸ·è¡Œ
ollama pull llama3.2:3b

# åˆ—å‡ºå·²å®‰è£çš„æ¨¡å‹
ollama list

# é¡¯ç¤ºæ¨¡å‹è³‡è¨Š
ollama show llama3.2:3b

# åˆªé™¤æ¨¡å‹
ollama rm llama3.2:3b

# è¤‡è£½æ¨¡å‹ï¼ˆç”¨æ–¼è‡ªè¨‚ï¼‰
ollama cp llama3.2:3b my-custom-model
</code></pre>
<h2 id="ç°¡å–®ç¯„ä¾‹"><a class="header" href="#ç°¡å–®ç¯„ä¾‹">ç°¡å–®ç¯„ä¾‹</a></h2>
<h3 id="ç¯„ä¾‹-1å‘½ä»¤åˆ—å°è©±"><a class="header" href="#ç¯„ä¾‹-1å‘½ä»¤åˆ—å°è©±">ç¯„ä¾‹ 1ï¼šå‘½ä»¤åˆ—å°è©±</a></h3>
<pre><code class="language-bash"># äº’å‹•å¼å°è©±
ollama run llama3.2:3b

&gt;&gt;&gt; è§£é‡‹ä»€éº¼æ˜¯å®¹å™¨æŠ€è¡“ï¼Ÿ
&gt;&gt;&gt; ç”¨ Python å¯«ä¸€å€‹å¿«é€Ÿæ’åº
&gt;&gt;&gt; /bye
</code></pre>
<h3 id="ç¯„ä¾‹-2ä¸€æ¬¡æ€§å•ç­”"><a class="header" href="#ç¯„ä¾‹-2ä¸€æ¬¡æ€§å•ç­”">ç¯„ä¾‹ 2ï¼šä¸€æ¬¡æ€§å•ç­”</a></h3>
<pre><code class="language-bash"># å–®æ¬¡å•ç­”ï¼ˆä¸é€²å…¥äº’å‹•æ¨¡å¼ï¼‰
echo "ä»€éº¼æ˜¯ RESTful APIï¼Ÿ" | ollama run llama3.2:3b

# æˆ–ä½¿ç”¨åƒæ•¸æ–¹å¼
ollama run llama3.2:3b "åˆ—å‡º 5 å€‹ Git å¸¸ç”¨æŒ‡ä»¤"
</code></pre>
<h3 id="ç¯„ä¾‹-3ä½¿ç”¨-curl-å‘¼å«-api"><a class="header" href="#ç¯„ä¾‹-3ä½¿ç”¨-curl-å‘¼å«-api">ç¯„ä¾‹ 3ï¼šä½¿ç”¨ cURL å‘¼å« API</a></h3>
<pre><code class="language-bash"># åŸºæœ¬ API å‘¼å«
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "è§£é‡‹ä»€éº¼æ˜¯å¾®æœå‹™æ¶æ§‹",
  "stream": false
}'

# ä¸²æµå›æ‡‰
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "å¯«ä¸€å€‹ Python Flask ç¯„ä¾‹",
  "stream": true
}'
</code></pre>
<h3 id="ç¯„ä¾‹-4python-æ•´åˆè…³æœ¬"><a class="header" href="#ç¯„ä¾‹-4python-æ•´åˆè…³æœ¬">ç¯„ä¾‹ 4ï¼šPython æ•´åˆè…³æœ¬</a></h3>
<pre><code class="language-python">#!/usr/bin/env python3
"""
Ollama Python æ•´åˆç¯„ä¾‹
å®‰è£: pip install requests
"""

import requests
import json

class OllamaClient:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
    
    def generate(self, prompt, model="llama3.2:3b", stream=False):
        """ç”Ÿæˆå›æ‡‰"""
        url = f"{self.base_url}/api/generate"
        data = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        if not stream:
            response = requests.post(url, json=data)
            if response.status_code == 200:
                return response.json()['response']
        else:
            response = requests.post(url, json=data, stream=True)
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line)
                    yield chunk['response']
    
    def chat(self, messages, model="llama3.2:3b"):
        """èŠå¤©ä»‹é¢"""
        url = f"{self.base_url}/api/chat"
        data = {
            "model": model,
            "messages": messages,
            "stream": False
        }
        
        response = requests.post(url, json=data)
        if response.status_code == 200:
            return response.json()['message']['content']
    
    def list_models(self):
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        url = f"{self.base_url}/api/tags"
        response = requests.get(url)
        if response.status_code == 200:
            return response.json()['models']

# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    client = OllamaClient()
    
    # ç°¡å–®ç”Ÿæˆ
    print("=== ç°¡å–®ç”Ÿæˆ ===")
    result = client.generate("ç”¨ Python å¯«ä¸€å€‹è²»æ°æ•¸åˆ—")
    print(result)
    
    # ä¸²æµç”Ÿæˆ
    print("\n=== ä¸²æµç”Ÿæˆ ===")
    for chunk in client.generate("è§£é‡‹ Docker çš„å„ªé»", stream=True):
        print(chunk, end='', flush=True)
    print()
    
    # èŠå¤©æ¨¡å¼
    print("\n=== èŠå¤©æ¨¡å¼ ===")
    messages = [
        {"role": "user", "content": "ä½ æ˜¯èª°ï¼Ÿ"},
        {"role": "assistant", "content": "æˆ‘æ˜¯ä¸€å€‹ AI åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ èƒ½åšä»€éº¼ï¼Ÿ"}
    ]
    response = client.chat(messages)
    print(response)
    
    # åˆ—å‡ºæ¨¡å‹
    print("\n=== å¯ç”¨æ¨¡å‹ ===")
    models = client.list_models()
    for model in models:
        print(f"- {model['name']} ({model['size']/1e9:.1f}GB)")
</code></pre>
<h3 id="ç¯„ä¾‹-5bash-èŠå¤©æ©Ÿå™¨äººè…³æœ¬"><a class="header" href="#ç¯„ä¾‹-5bash-èŠå¤©æ©Ÿå™¨äººè…³æœ¬">ç¯„ä¾‹ 5ï¼šBash èŠå¤©æ©Ÿå™¨äººè…³æœ¬</a></h3>
<pre><code class="language-bash">#!/bin/bash
# å„²å­˜ç‚º ollama-chat.sh
# ä½¿ç”¨: chmod +x ollama-chat.sh &amp;&amp; ./ollama-chat.sh

# è¨­å®š
MODEL="${1:-llama3.2:3b}"
HISTORY_FILE="$HOME/.ollama_chat_history"

# é¡è‰²è¨­å®š
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# å‡½æ•¸ï¼šé¡¯ç¤ºæ¨™é¡Œ
show_header() {
    clear
    echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${BLUE}â•‘       Ollama èŠå¤©æ©Ÿå™¨äºº v1.0          â•‘${NC}"
    echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${YELLOW}æ¨¡å‹: $MODEL${NC}"
    echo -e "${GREEN}æŒ‡ä»¤: /help, /clear, /model, /save, /quit${NC}"
    echo ""
}

# å‡½æ•¸ï¼šè™•ç†å‘½ä»¤
handle_command() {
    case $1 in
        /help)
            echo -e "${GREEN}å¯ç”¨æŒ‡ä»¤ï¼š${NC}"
            echo "  /help   - é¡¯ç¤ºå¹«åŠ©"
            echo "  /clear  - æ¸…é™¤è¢å¹•"
            echo "  /model  - åˆ‡æ›æ¨¡å‹"
            echo "  /save   - å„²å­˜å°è©±"
            echo "  /quit   - é›¢é–‹ç¨‹å¼"
            ;;
        /clear)
            show_header
            ;;
        /model)
            echo -e "${YELLOW}å¯ç”¨æ¨¡å‹ï¼š${NC}"
            ollama list
            read -p "è¼¸å…¥æ¨¡å‹åç¨±: " new_model
            if [ ! -z "$new_model" ]; then
                MODEL=$new_model
                echo -e "${GREEN}å·²åˆ‡æ›åˆ° $MODEL${NC}"
            fi
            ;;
        /save)
            echo "$conversation" &gt; "$HISTORY_FILE"
            echo -e "${GREEN}å°è©±å·²å„²å­˜åˆ° $HISTORY_FILE${NC}"
            ;;
        /quit|/exit|/bye)
            echo -e "${BLUE}å†è¦‹ï¼${NC}"
            exit 0
            ;;
        *)
            return 1
            ;;
    esac
    return 0
}

# ä¸»ç¨‹å¼
show_header
conversation=""

while true; do
    # è®€å–ä½¿ç”¨è€…è¼¸å…¥
    echo -ne "${GREEN}ğŸ‘¤ ä½ : ${NC}"
    read -r input
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå‘½ä»¤
    if [[ $input == /* ]]; then
        handle_command "$input"
        continue
    fi
    
    # æ–°å¢åˆ°å°è©±è¨˜éŒ„
    conversation="$conversation\nğŸ‘¤: $input"
    
    # å–å¾— AI å›æ‡‰
    echo -ne "${BLUE}ğŸ¤– AI: ${NC}"
    response=$(echo "$input" | ollama run $MODEL 2&gt;/dev/null)
    echo "$response"
    
    # æ–°å¢åˆ°å°è©±è¨˜éŒ„
    conversation="$conversation\nğŸ¤–: $response"
    echo ""
done
</code></pre>
<h3 id="ç¯„ä¾‹-6nodejs-æ•´åˆ"><a class="header" href="#ç¯„ä¾‹-6nodejs-æ•´åˆ">ç¯„ä¾‹ 6ï¼šNode.js æ•´åˆ</a></h3>
<pre><code class="language-javascript">// ollama-client.js
// å®‰è£: npm install axios

const axios = require('axios');

class OllamaClient {
    constructor(baseURL = 'http://localhost:11434') {
        this.baseURL = baseURL;
    }

    async generate(prompt, model = 'llama3.2:3b') {
        try {
            const response = await axios.post(`${this.baseURL}/api/generate`, {
                model: model,
                prompt: prompt,
                stream: false
            });
            return response.data.response;
        } catch (error) {
            console.error('Error:', error.message);
            return null;
        }
    }

    async *generateStream(prompt, model = 'llama3.2:3b') {
        try {
            const response = await axios.post(`${this.baseURL}/api/generate`, {
                model: model,
                prompt: prompt,
                stream: true
            }, {
                responseType: 'stream'
            });

            for await (const chunk of response.data) {
                const lines = chunk.toString().split('\n').filter(Boolean);
                for (const line of lines) {
                    const data = JSON.parse(line);
                    yield data.response;
                }
            }
        } catch (error) {
            console.error('Error:', error.message);
        }
    }
}

// ä½¿ç”¨ç¯„ä¾‹
async function main() {
    const client = new OllamaClient();
    
    // ä¸€èˆ¬ç”Ÿæˆ
    console.log('=== ä¸€èˆ¬ç”Ÿæˆ ===');
    const response = await client.generate('ä»€éº¼æ˜¯ Node.jsï¼Ÿ');
    console.log(response);
    
    // ä¸²æµç”Ÿæˆ
    console.log('\n=== ä¸²æµç”Ÿæˆ ===');
    for await (const chunk of client.generateStream('åˆ—å‡º JavaScript çš„ç‰¹é»')) {
        process.stdout.write(chunk);
    }
    console.log();
}

main();
</code></pre>
<h2 id="å¸¸ç”¨æ¨¡å‹æ¨è–¦"><a class="header" href="#å¸¸ç”¨æ¨¡å‹æ¨è–¦">å¸¸ç”¨æ¨¡å‹æ¨è–¦</a></h2>
<h3 id="-è¼•é‡ç´šæ¨¡å‹-ram--4gb"><a class="header" href="#-è¼•é‡ç´šæ¨¡å‹-ram--4gb">ğŸ¯ è¼•é‡ç´šæ¨¡å‹ (RAM &lt; 4GB)</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹åç¨±</th><th>åƒæ•¸å¤§å°</th><th>è¨˜æ†¶é«”éœ€æ±‚</th><th>ç‰¹é»</th><th>å®‰è£æŒ‡ä»¤</th></tr></thead><tbody>
<tr><td>TinyLlama</td><td>1.1B</td><td>~2GB</td><td>è¶…å¿«é€Ÿå›æ‡‰</td><td><code>ollama run tinyllama</code></td></tr>
<tr><td>Phi-3 Mini</td><td>3.8B</td><td>~3GB</td><td>å¾®è»Ÿå‡ºå“ï¼Œæ•ˆèƒ½ä½³</td><td><code>ollama run phi3:mini</code></td></tr>
<tr><td>Qwen 0.5B</td><td>0.5B</td><td>~1GB</td><td>ä¸­æ–‡æ”¯æ´è‰¯å¥½</td><td><code>ollama run qwen:0.5b</code></td></tr>
<tr><td>Gemma 2B</td><td>2B</td><td>~2.5GB</td><td>Google æ¨¡å‹</td><td><code>ollama run gemma:2b</code></td></tr>
</tbody></table>
</div>
<h3 id="-ä¸­å‹æ¨¡å‹-ram-8-16gb"><a class="header" href="#-ä¸­å‹æ¨¡å‹-ram-8-16gb">ğŸ’ª ä¸­å‹æ¨¡å‹ (RAM 8-16GB)</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹åç¨±</th><th>åƒæ•¸å¤§å°</th><th>è¨˜æ†¶é«”éœ€æ±‚</th><th>ç‰¹é»</th><th>å®‰è£æŒ‡ä»¤</th></tr></thead><tbody>
<tr><td>Llama 3.2</td><td>3B</td><td>~5GB</td><td>Meta æœ€æ–°ï¼Œå¹³è¡¡é¸æ“‡</td><td><code>ollama run llama3.2:3b</code></td></tr>
<tr><td>Mistral</td><td>7B</td><td>~8GB</td><td>æ³•åœ‹åœ˜éšŠï¼Œå“è³ªå„ªç§€</td><td><code>ollama run mistral</code></td></tr>
<tr><td>Gemma 2</td><td>9B</td><td>~10GB</td><td>Google å¤§å‹æ¨¡å‹</td><td><code>ollama run gemma2:9b</code></td></tr>
<tr><td>Vicuna</td><td>7B</td><td>~8GB</td><td>å°è©±èƒ½åŠ›å¼·</td><td><code>ollama run vicuna</code></td></tr>
</tbody></table>
</div>
<h3 id="-ç¨‹å¼ç¢¼å°ˆç”¨æ¨¡å‹"><a class="header" href="#-ç¨‹å¼ç¢¼å°ˆç”¨æ¨¡å‹">ğŸ‘¨â€ğŸ’» ç¨‹å¼ç¢¼å°ˆç”¨æ¨¡å‹</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹åç¨±</th><th>åƒæ•¸å¤§å°</th><th>è¨˜æ†¶é«”éœ€æ±‚</th><th>ç‰¹é»</th><th>å®‰è£æŒ‡ä»¤</th></tr></thead><tbody>
<tr><td>CodeLlama</td><td>7B</td><td>~8GB</td><td>Meta ç¨‹å¼ç¢¼æ¨¡å‹</td><td><code>ollama run codellama</code></td></tr>
<tr><td>DeepSeek Coder</td><td>1.3B</td><td>~2GB</td><td>è¼•é‡ç´šç¨‹å¼ç¢¼</td><td><code>ollama run deepseek-coder:1.3b</code></td></tr>
<tr><td>Starcoder2</td><td>3B</td><td>~4GB</td><td>å¤šèªè¨€ç¨‹å¼ç¢¼</td><td><code>ollama run starcoder2:3b</code></td></tr>
<tr><td>CodeGemma</td><td>7B</td><td>~8GB</td><td>Google ç¨‹å¼ç¢¼æ¨¡å‹</td><td><code>ollama run codegemma</code></td></tr>
</tbody></table>
</div>
<h3 id="-ä¸­æ–‡å„ªåŒ–æ¨¡å‹"><a class="header" href="#-ä¸­æ–‡å„ªåŒ–æ¨¡å‹">ğŸŒ ä¸­æ–‡å„ªåŒ–æ¨¡å‹</a></h3>
<div class="table-wrapper"><table><thead><tr><th>æ¨¡å‹åç¨±</th><th>åƒæ•¸å¤§å°</th><th>è¨˜æ†¶é«”éœ€æ±‚</th><th>ç‰¹é»</th><th>å®‰è£æŒ‡ä»¤</th></tr></thead><tbody>
<tr><td>Qwen</td><td>1.8B</td><td>~3GB</td><td>é˜¿é‡Œé€šç¾©åƒå•</td><td><code>ollama run qwen</code></td></tr>
<tr><td>Yi</td><td>6B</td><td>~7GB</td><td>é›¶ä¸€è¬ç‰©</td><td><code>ollama run yi</code></td></tr>
<tr><td>ChatGLM3</td><td>6B</td><td>~7GB</td><td>æ¸…è¯æ™ºè­œ</td><td><code>ollama run chatglm3</code></td></tr>
</tbody></table>
</div>
<h2 id="å¯¦ç”¨æŠ€å·§"><a class="header" href="#å¯¦ç”¨æŠ€å·§">å¯¦ç”¨æŠ€å·§</a></h2>
<h3 id="æ•ˆèƒ½å„ªåŒ–"><a class="header" href="#æ•ˆèƒ½å„ªåŒ–">æ•ˆèƒ½å„ªåŒ–</a></h3>
<h4 id="1-gpu-åŠ é€Ÿè¨­å®š"><a class="header" href="#1-gpu-åŠ é€Ÿè¨­å®š">1. GPU åŠ é€Ÿè¨­å®š</a></h4>
<pre><code class="language-bash"># æª¢æŸ¥ GPU æ”¯æ´
nvidia-smi

# è¨­å®šä½¿ç”¨ç‰¹å®š GPU
export CUDA_VISIBLE_DEVICES=0

# ä½¿ç”¨å¤šå€‹ GPU
export CUDA_VISIBLE_DEVICES=0,1
</code></pre>
<h4 id="2-è¨˜æ†¶é«”å„ªåŒ–"><a class="header" href="#2-è¨˜æ†¶é«”å„ªåŒ–">2. è¨˜æ†¶é«”å„ªåŒ–</a></h4>
<pre><code class="language-bash"># ä½¿ç”¨é‡å­åŒ–ç‰ˆæœ¬ï¼ˆé™ä½è¨˜æ†¶é«”ä½¿ç”¨ï¼‰
ollama run llama3.2:3b-q4_0  # 4-bit é‡å­åŒ–
ollama run llama3.2:3b-q5_0  # 5-bit é‡å­åŒ–
ollama run llama3.2:3b-q8_0  # 8-bit é‡å­åŒ–

# é™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
ollama run llama3.2:3b --num-ctx 2048
</code></pre>
<h4 id="3-cpu-å„ªåŒ–"><a class="header" href="#3-cpu-å„ªåŒ–">3. CPU å„ªåŒ–</a></h4>
<pre><code class="language-bash"># è¨­å®šä¸¦è¡Œæ•¸
export OLLAMA_NUM_PARALLEL=2

# è¨­å®šåŸ·è¡Œç·’æ•¸
export OLLAMA_NUM_THREAD=4
</code></pre>
<h3 id="æ‰¹æ¬¡è™•ç†"><a class="header" href="#æ‰¹æ¬¡è™•ç†">æ‰¹æ¬¡è™•ç†</a></h3>
<h4 id="æ‰¹æ¬¡å•ç­”è…³æœ¬"><a class="header" href="#æ‰¹æ¬¡å•ç­”è…³æœ¬">æ‰¹æ¬¡å•ç­”è…³æœ¬</a></h4>
<pre><code class="language-bash">#!/bin/bash
# batch-query.sh

# å•é¡Œåˆ—è¡¨æª”æ¡ˆ
QUESTIONS_FILE="questions.txt"
OUTPUT_FILE="answers.md"
MODEL="llama3.2:3b"

# æ¸…ç©ºè¼¸å‡ºæª”æ¡ˆ
&gt; "$OUTPUT_FILE"

# é€è¡Œè®€å–å•é¡Œä¸¦è™•ç†
while IFS= read -r question; do
    echo "è™•ç†: $question"
    echo "## $question" &gt;&gt; "$OUTPUT_FILE"
    echo "" &gt;&gt; "$OUTPUT_FILE"
    
    answer=$(echo "$question" | ollama run $MODEL)
    echo "$answer" &gt;&gt; "$OUTPUT_FILE"
    echo "" &gt;&gt; "$OUTPUT_FILE"
    echo "---" &gt;&gt; "$OUTPUT_FILE"
    echo "" &gt;&gt; "$OUTPUT_FILE"
done &lt; "$QUESTIONS_FILE"

echo "å®Œæˆï¼çµæœå·²å„²å­˜åˆ° $OUTPUT_FILE"
</code></pre>
<h3 id="æ•´åˆåˆ°-vs-code"><a class="header" href="#æ•´åˆåˆ°-vs-code">æ•´åˆåˆ° VS Code</a></h3>
<h4 id="1-å®‰è£-continue-æ“´å……å¥—ä»¶"><a class="header" href="#1-å®‰è£-continue-æ“´å……å¥—ä»¶">1. å®‰è£ Continue æ“´å……å¥—ä»¶</a></h4>
<pre><code class="language-bash"># åœ¨ VS Code ä¸­
# 1. é–‹å•Ÿå»¶ä¼¸æ¨¡çµ„ (Ctrl+Shift+X)
# 2. æœå°‹ "Continue"
# 3. å®‰è£
</code></pre>
<h4 id="2-è¨­å®š-continue"><a class="header" href="#2-è¨­å®š-continue">2. è¨­å®š Continue</a></h4>
<pre><code class="language-json">{
  "models": [
    {
      "title": "Ollama",
      "provider": "ollama",
      "model": "codellama:7b",
      "apiBase": "http://localhost:11434"
    }
  ]
}
</code></pre>
<h3 id="å»ºç«‹åˆ¥åå¿«æ·"><a class="header" href="#å»ºç«‹åˆ¥åå¿«æ·">å»ºç«‹åˆ¥åå¿«æ·</a></h3>
<pre><code class="language-bash"># åŠ å…¥åˆ° ~/.bashrc æˆ– ~/.zshrc

# å¿«é€Ÿå•Ÿå‹•å°è©±
alias chat='ollama run llama3.2:3b'

# ç¨‹å¼ç¢¼åŠ©æ‰‹
alias code-ai='ollama run codellama:7b'

# å¿«é€Ÿç¿»è­¯
translate() {
    echo "Translate to Chinese: $1" | ollama run llama3.2:3b
}

# ç¨‹å¼ç¢¼è§£é‡‹
explain() {
    echo "Explain this code: $(cat $1)" | ollama run codellama:7b
}

# å¿«é€Ÿæ‘˜è¦
summarize() {
    echo "Summarize: $(cat $1)" | ollama run llama3.2:3b
}
</code></pre>
<h2 id="web-ui-è¨­å®š"><a class="header" href="#web-ui-è¨­å®š">Web UI è¨­å®š</a></h2>
<h3 id="é¸é …-1open-webuiæ¨è–¦"><a class="header" href="#é¸é …-1open-webuiæ¨è–¦">é¸é … 1ï¼šOpen WebUIï¼ˆæ¨è–¦ï¼‰</a></h3>
<h4 id="docker-å®‰è£"><a class="header" href="#docker-å®‰è£">Docker å®‰è£</a></h4>
<pre><code class="language-bash"># æ‹‰å–ä¸¦åŸ·è¡Œ
docker run -d \
  -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main

# æª¢æŸ¥ç‹€æ…‹
docker ps | grep open-webui

# æª¢è¦–æ—¥èªŒ
docker logs -f open-webui
</code></pre>
<h4 id="docker-compose-å®‰è£"><a class="header" href="#docker-compose-å®‰è£">Docker Compose å®‰è£</a></h4>
<pre><code class="language-yaml"># docker-compose.yml
version: '3.8'

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: always

volumes:
  open-webui:
</code></pre>
<pre><code class="language-bash"># å•Ÿå‹•
docker-compose up -d

# è¨ªå• http://localhost:3000
</code></pre>
<h3 id="é¸é …-2ollama-ui"><a class="header" href="#é¸é …-2ollama-ui">é¸é … 2ï¼šOllama UI</a></h3>
<pre><code class="language-bash"># å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/ollama-ui/ollama-ui
cd ollama-ui

# å®‰è£ä¾è³´
npm install

# å»ºç«‹ç’°å¢ƒè¨­å®š
cp .env.example .env
echo "OLLAMA_HOST=http://localhost:11434" &gt;&gt; .env

# å•Ÿå‹•é–‹ç™¼ä¼ºæœå™¨
npm run dev

# å»ºç½®ç”Ÿç”¢ç‰ˆæœ¬
npm run build
npm start
</code></pre>
<h3 id="é¸é …-3ç°¡å–®-html-ä»‹é¢"><a class="header" href="#é¸é …-3ç°¡å–®-html-ä»‹é¢">é¸é … 3ï¼šç°¡å–® HTML ä»‹é¢</a></h3>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang="zh-TW"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;Ollama Chat&lt;/title&gt;
    &lt;style&gt;
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .container {
            width: 90%;
            max-width: 800px;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        .header {
            background: #4a5568;
            color: white;
            padding: 20px;
            text-align: center;
        }
        .chat-box {
            height: 400px;
            overflow-y: auto;
            padding: 20px;
            background: #f7fafc;
        }
        .message {
            margin: 10px 0;
            padding: 10px 15px;
            border-radius: 10px;
            max-width: 70%;
        }
        .user-message {
            background: #4299e1;
            color: white;
            margin-left: auto;
            text-align: right;
        }
        .ai-message {
            background: #e2e8f0;
            color: #2d3748;
        }
        .input-area {
            display: flex;
            padding: 20px;
            background: white;
            border-top: 1px solid #e2e8f0;
        }
        #messageInput {
            flex: 1;
            padding: 10px 15px;
            border: 2px solid #e2e8f0;
            border-radius: 25px;
            font-size: 16px;
            outline: none;
        }
        #messageInput:focus {
            border-color: #4299e1;
        }
        #sendButton {
            margin-left: 10px;
            padding: 10px 30px;
            background: #4299e1;
            color: white;
            border: none;
            border-radius: 25px;
            font-size: 16px;
            cursor: pointer;
            transition: background 0.3s;
        }
        #sendButton:hover {
            background: #3182ce;
        }
        #sendButton:disabled {
            background: #a0aec0;
            cursor: not-allowed;
        }
        .model-selector {
            padding: 10px 20px;
            background: #edf2f7;
        }
        select {
            padding: 5px 10px;
            border-radius: 5px;
            border: 1px solid #cbd5e0;
        }
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #4299e1;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="container"&gt;
        &lt;div class="header"&gt;
            &lt;h1&gt;ğŸ¤– Ollama Chat&lt;/h1&gt;
        &lt;/div&gt;
        
        &lt;div class="model-selector"&gt;
            &lt;label for="modelSelect"&gt;æ¨¡å‹ï¼š&lt;/label&gt;
            &lt;select id="modelSelect"&gt;
                &lt;option value="llama3.2:3b"&gt;Llama 3.2 (3B)&lt;/option&gt;
                &lt;option value="tinyllama"&gt;TinyLlama&lt;/option&gt;
                &lt;option value="codellama:7b"&gt;CodeLlama&lt;/option&gt;
                &lt;option value="mistral"&gt;Mistral&lt;/option&gt;
            &lt;/select&gt;
        &lt;/div&gt;
        
        &lt;div class="chat-box" id="chatBox"&gt;&lt;/div&gt;
        
        &lt;div class="input-area"&gt;
            &lt;input type="text" id="messageInput" placeholder="è¼¸å…¥è¨Šæ¯..." /&gt;
            &lt;button id="sendButton"&gt;ç™¼é€&lt;/button&gt;
        &lt;/div&gt;
    &lt;/div&gt;

    &lt;script&gt;
        const chatBox = document.getElementById('chatBox');
        const messageInput = document.getElementById('messageInput');
        const sendButton = document.getElementById('sendButton');
        const modelSelect = document.getElementById('modelSelect');
        
        async function sendMessage() {
            const message = messageInput.value.trim();
            if (!message) return;
            
            // é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
            addMessage(message, 'user');
            messageInput.value = '';
            
            // ç¦ç”¨è¼¸å…¥
            sendButton.disabled = true;
            messageInput.disabled = true;
            
            // é¡¯ç¤ºè¼‰å…¥ä¸­
            const loadingId = 'loading-' + Date.now();
            addMessage('&lt;div class="loading"&gt;&lt;/div&gt;', 'ai', loadingId);
            
            try {
                const response = await fetch('http://localhost:11434/api/generate', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        model: modelSelect.value,
                        prompt: message,
                        stream: false
                    })
                });
                
                const data = await response.json();
                
                // ç§»é™¤è¼‰å…¥å‹•ç•«
                document.getElementById(loadingId).remove();
                
                // é¡¯ç¤º AI å›æ‡‰
                addMessage(data.response, 'ai');
                
            } catch (error) {
                document.getElementById(loadingId).remove();
                addMessage('éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™', 'ai');
            }
            
            // é‡æ–°å•Ÿç”¨è¼¸å…¥
            sendButton.disabled = false;
            messageInput.disabled = false;
            messageInput.focus();
        }
        
        function addMessage(content, sender, id = null) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}-message`;
            if (id) messageDiv.id = id;
            messageDiv.innerHTML = content;
            chatBox.appendChild(messageDiv);
            chatBox.scrollTop = chatBox.scrollHeight;
        }
        
        sendButton.addEventListener('click', sendMessage);
        messageInput.addEventListener('keypress', (e) =&gt; {
            if (e.key === 'Enter' &amp;&amp; !e.shiftKey) {
                e.preventDefault();
                sendMessage();
            }
        });
        
        // åˆå§‹è¨Šæ¯
        addMessage('ä½ å¥½ï¼æˆ‘æ˜¯ Ollama AI åŠ©æ‰‹ï¼Œæœ‰ä»€éº¼å¯ä»¥å¹«åŠ©ä½ çš„å—ï¼Ÿ', 'ai');
    &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h2 id="æ•…éšœæ’é™¤"><a class="header" href="#æ•…éšœæ’é™¤">æ•…éšœæ’é™¤</a></h2>
<h3 id="å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ"><a class="header" href="#å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ">å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ</a></h3>
<h4 id="1-æœå‹™ç„¡æ³•å•Ÿå‹•"><a class="header" href="#1-æœå‹™ç„¡æ³•å•Ÿå‹•">1. æœå‹™ç„¡æ³•å•Ÿå‹•</a></h4>
<pre><code class="language-bash"># æª¢æŸ¥éŒ¯èª¤æ—¥èªŒ
journalctl -u ollama -n 50

# å¸¸è¦‹åŸå› ï¼šåŸ è™Ÿè¢«ä½”ç”¨
sudo lsof -i :11434

# è§£æ±ºæ–¹æ¡ˆï¼šæ›´æ”¹åŸ è™Ÿ
OLLAMA_HOST=0.0.0.0:8080 ollama serve
</code></pre>
<h4 id="2-gpu-ä¸è¢«è­˜åˆ¥"><a class="header" href="#2-gpu-ä¸è¢«è­˜åˆ¥">2. GPU ä¸è¢«è­˜åˆ¥</a></h4>
<pre><code class="language-bash"># æª¢æŸ¥ NVIDIA é©…å‹•
nvidia-smi

# å®‰è£ CUDA å·¥å…·åŒ…
sudo apt update
sudo apt install nvidia-cuda-toolkit

# æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# é‡å•Ÿ Ollama
sudo systemctl restart ollama
</code></pre>
<h4 id="3-è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤"><a class="header" href="#3-è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤">3. è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤</a></h4>
<pre><code class="language-bash"># è§£æ±ºæ–¹æ¡ˆ 1ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama run tinyllama

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šä½¿ç”¨é‡å­åŒ–ç‰ˆæœ¬
ollama run llama3.2:3b-q4_0

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šé™åˆ¶ä¸Šä¸‹æ–‡é•·åº¦
ollama run llama3.2:3b --num-ctx 1024

# è§£æ±ºæ–¹æ¡ˆ 4ï¼šæ¸…ç†æœªä½¿ç”¨çš„æ¨¡å‹
ollama list
ollama rm unused-model
</code></pre>
<h4 id="4-æ¨¡å‹ä¸‹è¼‰å¤±æ•—"><a class="header" href="#4-æ¨¡å‹ä¸‹è¼‰å¤±æ•—">4. æ¨¡å‹ä¸‹è¼‰å¤±æ•—</a></h4>
<pre><code class="language-bash"># æª¢æŸ¥ç¶²è·¯é€£ç·š
ping ollama.com

# ä½¿ç”¨ä»£ç†
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# é‡è©¦ä¸‹è¼‰
ollama pull llama3.2:3b

# æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ
wget https://ollama.com/library/llama3.2/blobs/sha256:xxxxx
</code></pre>
<h4 id="5-api-é€£ç·šè¢«æ‹’çµ•"><a class="header" href="#5-api-é€£ç·šè¢«æ‹’çµ•">5. API é€£ç·šè¢«æ‹’çµ•</a></h4>
<pre><code class="language-bash"># æª¢æŸ¥æœå‹™ç‹€æ…‹
sudo systemctl status ollama

# å…è¨±å¤–éƒ¨é€£ç·š
OLLAMA_HOST=0.0.0.0:11434 ollama serve

# é˜²ç«ç‰†è¨­å®š
sudo ufw allow 11434/tcp
</code></pre>
<h3 id="æ•ˆèƒ½èª¿å„ª"><a class="header" href="#æ•ˆèƒ½èª¿å„ª">æ•ˆèƒ½èª¿å„ª</a></h3>
<h4 id="ç³»çµ±å±¤ç´šå„ªåŒ–"><a class="header" href="#ç³»çµ±å±¤ç´šå„ªåŒ–">ç³»çµ±å±¤ç´šå„ªåŒ–</a></h4>
<pre><code class="language-bash"># å¢åŠ æª”æ¡ˆæè¿°ç¬¦é™åˆ¶
ulimit -n 65536

# èª¿æ•´ swap ä½¿ç”¨
sudo sysctl vm.swappiness=10

# è¨­å®š CPU èª¿æ§å™¨
sudo cpupower frequency-set -g performance
</code></pre>
<h4 id="ollama-ç‰¹å®šå„ªåŒ–"><a class="header" href="#ollama-ç‰¹å®šå„ªåŒ–">Ollama ç‰¹å®šå„ªåŒ–</a></h4>
<pre><code class="language-bash"># ç’°å¢ƒè®Šæ•¸è¨­å®š
export OLLAMA_NUM_PARALLEL=4     # ä¸¦è¡Œè«‹æ±‚æ•¸
export OLLAMA_NUM_GPU=1          # GPU æ•¸é‡
export OLLAMA_MAX_LOADED_MODELS=2 # æœ€å¤§è¼‰å…¥æ¨¡å‹æ•¸
export OLLAMA_KEEP_ALIVE=5m      # æ¨¡å‹ä¿æŒè¼‰å…¥æ™‚é–“
</code></pre>
<h2 id="é€²éšè¨­å®š"><a class="header" href="#é€²éšè¨­å®š">é€²éšè¨­å®š</a></h2>
<h3 id="è‡ªè¨‚æ¨¡å‹-modelfile"><a class="header" href="#è‡ªè¨‚æ¨¡å‹-modelfile">è‡ªè¨‚æ¨¡å‹ (Modelfile)</a></h3>
<h4 id="åŸºæœ¬-modelfile"><a class="header" href="#åŸºæœ¬-modelfile">åŸºæœ¬ Modelfile</a></h4>
<pre><code class="language-dockerfile"># Modelfile
FROM llama3.2:3b

# è¨­å®šåƒæ•¸
PARAMETER temperature 0.8
PARAMETER top_k 40
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 4096

# è¨­å®šç³»çµ±æç¤º
SYSTEM """
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æŠ€è¡“é¡§å•ï¼Œå°ˆé–€å”åŠ©è§£æ±ºç¨‹å¼è¨­è¨ˆå’Œç³»çµ±æ¶æ§‹å•é¡Œã€‚
è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦æä¾›è©³ç´°çš„è§£é‡‹å’Œç¯„ä¾‹ã€‚
"""

# è¨­å®šè¨Šæ¯æ¨¡æ¿
TEMPLATE """
{{ if .System }}System: {{ .System }}{{ end }}
{{ if .Prompt }}User: {{ .Prompt }}{{ end }}
Assistant: {{ .Response }}
"""
</code></pre>
<h4 id="å»ºç«‹å’Œä½¿ç”¨è‡ªè¨‚æ¨¡å‹"><a class="header" href="#å»ºç«‹å’Œä½¿ç”¨è‡ªè¨‚æ¨¡å‹">å»ºç«‹å’Œä½¿ç”¨è‡ªè¨‚æ¨¡å‹</a></h4>
<pre><code class="language-bash"># å»ºç«‹æ¨¡å‹
ollama create my-assistant -f ./Modelfile

# ä½¿ç”¨è‡ªè¨‚æ¨¡å‹
ollama run my-assistant

# åˆ†äº«æ¨¡å‹
ollama push username/my-assistant
</code></pre>
<h4 id="é€²éš-modelfile-ç¯„ä¾‹"><a class="header" href="#é€²éš-modelfile-ç¯„ä¾‹">é€²éš Modelfile ç¯„ä¾‹</a></h4>
<pre><code class="language-dockerfile"># ç¨‹å¼ç¢¼åŠ©æ‰‹æ¨¡å‹
FROM codellama:7b

PARAMETER temperature 0.3  # é™ä½éš¨æ©Ÿæ€§
PARAMETER num_predict 2000 # æœ€å¤§ç”Ÿæˆé•·åº¦

SYSTEM """
You are an expert programmer. Follow these rules:
1. Always provide working code examples
2. Include comments explaining complex parts
3. Consider edge cases and error handling
4. Suggest best practices and optimizations
5. Use the most appropriate programming patterns
"""

# åŠ å…¥ç¯„ä¾‹å°è©±
MESSAGE user "Write a function to reverse a string"
MESSAGE assistant """Here's a function to reverse a string in Python:

```python
def reverse_string(s):
    \"\"\"
    Reverse a string using Python's slicing feature.
    
    Args:
        s (str): The string to reverse
    
    Returns:
        str: The reversed string
    \"\"\"
    return s[::-1]

# Example usage
print(reverse_string("hello"))  # Output: "olleh"
</code></pre>
<p>"""</p>
<pre><code>
### å¤šæ¨¡å‹ç®¡ç†

#### æ¨¡å‹åˆ‡æ›è…³æœ¬
```python
#!/usr/bin/env python3
"""
Ollama æ¨¡å‹ç®¡ç†å™¨
"""

import subprocess
import json
import sys

class ModelManager:
    def __init__(self):
        self.models = self.get_installed_models()
    
    def get_installed_models(self):
        """å–å¾—å·²å®‰è£çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            result = subprocess.run(
                ['ollama', 'list'], 
                capture_output=True, 
                text=True
            )
            # è§£æè¼¸å‡º
            lines = result.stdout.strip().split('\n')[1:]  # è·³éæ¨™é¡Œ
            models = []
            for line in lines:
                if line:
                    parts = line.split()
                    models.append({
                        'name': parts[0],
                        'size': parts[1] if len(parts) &gt; 1 else 'N/A'
                    })
            return models
        except Exception as e:
            print(f"éŒ¯èª¤: {e}")
            return []
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        print("\nå·²å®‰è£çš„æ¨¡å‹:")
        print("-" * 40)
        for i, model in enumerate(self.models, 1):
            print(f"{i}. {model['name']} ({model['size']})")
    
    def run_model(self, model_name):
        """åŸ·è¡ŒæŒ‡å®šæ¨¡å‹"""
        print(f"\nå•Ÿå‹•æ¨¡å‹: {model_name}")
        subprocess.run(['ollama', 'run', model_name])
    
    def pull_model(self, model_name):
        """ä¸‹è¼‰æ–°æ¨¡å‹"""
        print(f"\nä¸‹è¼‰æ¨¡å‹: {model_name}")
        subprocess.run(['ollama', 'pull', model_name])
    
    def delete_model(self, model_name):
        """åˆªé™¤æ¨¡å‹"""
        confirm = input(f"ç¢ºå®šè¦åˆªé™¤ {model_name}? (y/n): ")
        if confirm.lower() == 'y':
            subprocess.run(['ollama', 'rm', model_name])
            print(f"å·²åˆªé™¤ {model_name}")

def main():
    manager = ModelManager()
    
    while True:
        print("\n" + "="*40)
        print("Ollama æ¨¡å‹ç®¡ç†å™¨")
        print("="*40)
        print("1. åˆ—å‡ºæ¨¡å‹")
        print("2. åŸ·è¡Œæ¨¡å‹")
        print("3. ä¸‹è¼‰æ–°æ¨¡å‹")
        print("4. åˆªé™¤æ¨¡å‹")
        print("5. é›¢é–‹")
        
        choice = input("\né¸æ“‡æ“ä½œ (1-5): ")
        
        if choice == '1':
            manager.list_models()
        
        elif choice == '2':
            manager.list_models()
            model_idx = input("\né¸æ“‡æ¨¡å‹ç·¨è™Ÿ: ")
            try:
                idx = int(model_idx) - 1
                if 0 &lt;= idx &lt; len(manager.models):
                    manager.run_model(manager.models[idx]['name'])
            except (ValueError, IndexError):
                print("ç„¡æ•ˆçš„é¸æ“‡")
        
        elif choice == '3':
            model_name = input("è¼¸å…¥æ¨¡å‹åç¨± (å¦‚ llama3.2:3b): ")
            manager.pull_model(model_name)
            manager.models = manager.get_installed_models()
        
        elif choice == '4':
            manager.list_models()
            model_idx = input("\né¸æ“‡è¦åˆªé™¤çš„æ¨¡å‹ç·¨è™Ÿ: ")
            try:
                idx = int(model_idx) - 1
                if 0 &lt;= idx &lt; len(manager.models):
                    manager.delete_model(manager.models[idx]['name'])
                    manager.models = manager.get_installed_models()
            except (ValueError, IndexError):
                print("ç„¡æ•ˆçš„é¸æ“‡")
        
        elif choice == '5':
            print("å†è¦‹ï¼")
            break
        
        else:
            print("ç„¡æ•ˆçš„é¸æ“‡")

if __name__ == "__main__":
    main()
</code></pre>
<h2 id="api-åƒè€ƒ"><a class="header" href="#api-åƒè€ƒ">API åƒè€ƒ</a></h2>
<h3 id="æ ¸å¿ƒ-api-ç«¯é»"><a class="header" href="#æ ¸å¿ƒ-api-ç«¯é»">æ ¸å¿ƒ API ç«¯é»</a></h3>
<h4 id="1-ç”Ÿæˆæ–‡å­—-apigenerate"><a class="header" href="#1-ç”Ÿæˆæ–‡å­—-apigenerate">1. ç”Ÿæˆæ–‡å­— <code>/api/generate</code></a></h4>
<pre><code class="language-bash"># è«‹æ±‚
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Why is the sky blue?",
    "stream": false,
    "options": {
      "temperature": 0.7,
      "top_p": 0.9,
      "top_k": 40
    }
  }'

# å›æ‡‰
{
  "model": "llama3.2:3b",
  "created_at": "2024-01-01T00:00:00.000Z",
  "response": "The sky appears blue because...",
  "done": true,
  "context": [1, 2, 3],
  "total_duration": 5000000000,
  "load_duration": 1000000000,
  "prompt_eval_duration": 1000000000,
  "eval_duration": 3000000000,
  "eval_count": 100
}
</code></pre>
<h4 id="2-èŠå¤©ä»‹é¢-apichat"><a class="header" href="#2-èŠå¤©ä»‹é¢-apichat">2. èŠå¤©ä»‹é¢ <code>/api/chat</code></a></h4>
<pre><code class="language-bash"># è«‹æ±‚
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Hello!"},
      {"role": "assistant", "content": "Hi! How can I help you?"},
      {"role": "user", "content": "Tell me a joke"}
    ],
    "stream": false
  }'
</code></pre>
<h4 id="3-æ¨¡å‹ç®¡ç†"><a class="header" href="#3-æ¨¡å‹ç®¡ç†">3. æ¨¡å‹ç®¡ç†</a></h4>
<h5 id="åˆ—å‡ºæ¨¡å‹-apitags"><a class="header" href="#åˆ—å‡ºæ¨¡å‹-apitags">åˆ—å‡ºæ¨¡å‹ <code>/api/tags</code></a></h5>
<pre><code class="language-bash">curl http://localhost:11434/api/tags

# å›æ‡‰
{
  "models": [
    {
      "name": "llama3.2:3b",
      "modified_at": "2024-01-01T00:00:00.000Z",
      "size": 3825819519,
      "digest": "sha256:xxx"
    }
  ]
}
</code></pre>
<h5 id="é¡¯ç¤ºæ¨¡å‹è³‡è¨Š-apishow"><a class="header" href="#é¡¯ç¤ºæ¨¡å‹è³‡è¨Š-apishow">é¡¯ç¤ºæ¨¡å‹è³‡è¨Š <code>/api/show</code></a></h5>
<pre><code class="language-bash">curl -X POST http://localhost:11434/api/show \
  -d '{"name": "llama3.2:3b"}'
</code></pre>
<h5 id="è¤‡è£½æ¨¡å‹-apicopy"><a class="header" href="#è¤‡è£½æ¨¡å‹-apicopy">è¤‡è£½æ¨¡å‹ <code>/api/copy</code></a></h5>
<pre><code class="language-bash">curl -X POST http://localhost:11434/api/copy \
  -d '{
    "source": "llama3.2:3b",
    "destination": "my-model"
  }'
</code></pre>
<h5 id="åˆªé™¤æ¨¡å‹-apidelete"><a class="header" href="#åˆªé™¤æ¨¡å‹-apidelete">åˆªé™¤æ¨¡å‹ <code>/api/delete</code></a></h5>
<pre><code class="language-bash">curl -X DELETE http://localhost:11434/api/delete \
  -d '{"name": "llama3.2:3b"}'
</code></pre>
<h5 id="æ‹‰å–æ¨¡å‹-apipull"><a class="header" href="#æ‹‰å–æ¨¡å‹-apipull">æ‹‰å–æ¨¡å‹ <code>/api/pull</code></a></h5>
<pre><code class="language-bash">curl -X POST http://localhost:11434/api/pull \
  -d '{"name": "llama3.2:3b"}'
</code></pre>
<h5 id="æ¨é€æ¨¡å‹-apipush"><a class="header" href="#æ¨é€æ¨¡å‹-apipush">æ¨é€æ¨¡å‹ <code>/api/push</code></a></h5>
<pre><code class="language-bash">curl -X POST http://localhost:11434/api/push \
  -d '{"name": "username/my-model"}'
</code></pre>
<h4 id="4-åµŒå…¥å‘é‡-apiembeddings"><a class="header" href="#4-åµŒå…¥å‘é‡-apiembeddings">4. åµŒå…¥å‘é‡ <code>/api/embeddings</code></a></h4>
<pre><code class="language-bash">curl -X POST http://localhost:11434/api/embeddings \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Hello world"
  }'

# å›æ‡‰
{
  "embedding": [0.1, 0.2, 0.3, ...]
}
</code></pre>
<h3 id="åƒæ•¸èªªæ˜"><a class="header" href="#åƒæ•¸èªªæ˜">åƒæ•¸èªªæ˜</a></h3>
<h4 id="ç”Ÿæˆåƒæ•¸-options"><a class="header" href="#ç”Ÿæˆåƒæ•¸-options">ç”Ÿæˆåƒæ•¸ (options)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>åƒæ•¸</th><th>é¡å‹</th><th>é è¨­å€¼</th><th>èªªæ˜</th></tr></thead><tbody>
<tr><td>temperature</td><td>float</td><td>0.8</td><td>æ§åˆ¶éš¨æ©Ÿæ€§ (0-2)</td></tr>
<tr><td>top_k</td><td>int</td><td>40</td><td>é™åˆ¶è©å½™é¸æ“‡æ•¸é‡</td></tr>
<tr><td>top_p</td><td>float</td><td>0.9</td><td>ç´¯ç©æ©Ÿç‡é–¾å€¼</td></tr>
<tr><td>repeat_penalty</td><td>float</td><td>1.1</td><td>é‡è¤‡æ‡²ç½°</td></tr>
<tr><td>seed</td><td>int</td><td>0</td><td>éš¨æ©Ÿç¨®å­</td></tr>
<tr><td>num_predict</td><td>int</td><td>128</td><td>æœ€å¤§ç”Ÿæˆé•·åº¦</td></tr>
<tr><td>num_ctx</td><td>int</td><td>2048</td><td>ä¸Šä¸‹æ–‡è¦–çª—å¤§å°</td></tr>
<tr><td>stop</td><td>[]string</td><td>[]</td><td>åœæ­¢åºåˆ—</td></tr>
</tbody></table>
</div>
<h3 id="sdk-æ•´åˆ"><a class="header" href="#sdk-æ•´åˆ">SDK æ•´åˆ</a></h3>
<h4 id="python-ollama-python"><a class="header" href="#python-ollama-python">Python (ollama-python)</a></h4>
<pre><code class="language-bash">pip install ollama
</code></pre>
<pre><code class="language-python">import ollama

# ç”Ÿæˆ
response = ollama.generate(model='llama3.2:3b', prompt='Why is the sky blue?')
print(response['response'])

# èŠå¤©
messages = [
    {'role': 'user', 'content': 'Why is the sky blue?'}
]
response = ollama.chat(model='llama3.2:3b', messages=messages)
print(response['message']['content'])

# ä¸²æµ
for chunk in ollama.generate(model='llama3.2:3b', prompt='Tell me a story', stream=True):
    print(chunk['response'], end='', flush=True)
</code></pre>
<h4 id="javascripttypescript"><a class="header" href="#javascripttypescript">JavaScript/TypeScript</a></h4>
<pre><code class="language-bash">npm install ollama
</code></pre>
<pre><code class="language-javascript">import ollama from 'ollama'

// ç”Ÿæˆ
const response = await ollama.generate({
  model: 'llama3.2:3b',
  prompt: 'Why is the sky blue?'
})
console.log(response.response)

// èŠå¤©
const message = await ollama.chat({
  model: 'llama3.2:3b',
  messages: [{ role: 'user', content: 'Why is the sky blue?' }],
})
console.log(message.message.content)

// ä¸²æµ
const stream = await ollama.generate({
  model: 'llama3.2:3b',
  prompt: 'Tell me a story',
  stream: true,
})
for await (const chunk of stream) {
  process.stdout.write(chunk.response)
}
</code></pre>
<h2 id="æœ€ä½³å¯¦è¸"><a class="header" href="#æœ€ä½³å¯¦è¸">æœ€ä½³å¯¦è¸</a></h2>
<h3 id="1-å®‰å…¨æ€§è¨­å®š"><a class="header" href="#1-å®‰å…¨æ€§è¨­å®š">1. å®‰å…¨æ€§è¨­å®š</a></h3>
<pre><code class="language-bash"># é™åˆ¶æœ¬åœ°å­˜å–
OLLAMA_HOST=127.0.0.1:11434 ollama serve

# ä½¿ç”¨ nginx åå‘ä»£ç†
server {
    listen 443 ssl;
    server_name your-domain.com;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location /api/ {
        proxy_pass http://localhost:11434/api/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
</code></pre>
<h3 id="2-ç›£æ§å’Œæ—¥èªŒ"><a class="header" href="#2-ç›£æ§å’Œæ—¥èªŒ">2. ç›£æ§å’Œæ—¥èªŒ</a></h3>
<pre><code class="language-bash"># å³æ™‚ç›£æ§
watch -n 1 'nvidia-smi; echo ""; ollama list'

# æ—¥èªŒåˆ†æ
journalctl -u ollama --since "1 hour ago" | grep ERROR

# æ•ˆèƒ½ç›£æ§è…³æœ¬
#!/bin/bash
while true; do
    echo "$(date): $(ollama list | wc -l) models loaded"
    nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv,noheader
    sleep 5
done
</code></pre>
<h3 id="3-å‚™ä»½å’Œé‚„åŸ"><a class="header" href="#3-å‚™ä»½å’Œé‚„åŸ">3. å‚™ä»½å’Œé‚„åŸ</a></h3>
<pre><code class="language-bash"># å‚™ä»½æ¨¡å‹
tar -czf ollama-models-backup.tar.gz ~/.ollama/models

# é‚„åŸæ¨¡å‹
tar -xzf ollama-models-backup.tar.gz -C ~/

# å‚™ä»½è¨­å®š
cp -r ~/.ollama ollama-config-backup
</code></pre>
<h2 id="è³‡æºé€£çµ"><a class="header" href="#è³‡æºé€£çµ">è³‡æºé€£çµ</a></h2>
<h3 id="å®˜æ–¹è³‡æº"><a class="header" href="#å®˜æ–¹è³‡æº">å®˜æ–¹è³‡æº</a></h3>
<ul>
<li><a href="https://ollama.com">Ollama å®˜æ–¹ç¶²ç«™</a></li>
<li><a href="https://github.com/ollama/ollama">Ollama GitHub</a></li>
<li><a href="https://ollama.com/library">æ¨¡å‹åº«</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/api.md">API æ–‡ä»¶</a></li>
<li><a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Modelfile æ–‡ä»¶</a></li>
</ul>
<h3 id="ç¤¾ç¾¤è³‡æº"><a class="header" href="#ç¤¾ç¾¤è³‡æº">ç¤¾ç¾¤è³‡æº</a></h3>
<ul>
<li><a href="https://discord.gg/ollama">Ollama Discord</a></li>
<li><a href="https://www.reddit.com/r/LocalLLaMA/">Reddit r/LocalLLaMA</a></li>
<li><a href="https://huggingface.co/models">Hugging Face Models</a></li>
</ul>
<h3 id="ç›¸é—œå·¥å…·"><a class="header" href="#ç›¸é—œå·¥å…·">ç›¸é—œå·¥å…·</a></h3>
<ul>
<li><a href="https://github.com/open-webui/open-webui">Open WebUI</a></li>
<li><a href="https://continue.dev/">Continue (VS Code)</a></li>
<li><a href="https://langchain.com/">LangChain</a></li>
<li><a href="https://www.llamaindex.ai/">LlamaIndex</a></li>
</ul>
<h3 id="å­¸ç¿’è³‡æº"><a class="header" href="#å­¸ç¿’è³‡æº">å­¸ç¿’è³‡æº</a></h3>
<ul>
<li><a href="https://www.youtube.com/results?search_query=ollama+tutorial">Ollama æ•™å­¸å½±ç‰‡</a></li>
<li><a href="https://www.deeplearning.ai/">LLM èª²ç¨‹</a></li>
<li><a href="https://www.promptingguide.ai/">Prompt Engineering Guide</a></li>
</ul>
<hr />
<h2 id="æ›´æ–°æ—¥èªŒ"><a class="header" href="#æ›´æ–°æ—¥èªŒ">æ›´æ–°æ—¥èªŒ</a></h2>
<ul>
<li><strong>2024.01</strong>: åˆå§‹ç‰ˆæœ¬</li>
<li><strong>2024.02</strong>: æ–°å¢ Web UI è¨­å®š</li>
<li><strong>2024.03</strong>: æ–°å¢é€²éšè¨­å®šå’Œ API åƒè€ƒ</li>
<li><strong>2024.04</strong>: æ–°å¢æ•…éšœæ’é™¤å’Œæœ€ä½³å¯¦è¸</li>
</ul>
<hr />
<p>ğŸ’¡ <strong>å°æç¤º</strong>:</p>
<ul>
<li>é–‹å§‹ä½¿ç”¨æ™‚å…ˆå˜—è©¦è¼ƒå°çš„æ¨¡å‹ï¼ˆå¦‚ TinyLlamaï¼‰</li>
<li>å®šæœŸæ›´æ–° Ollama ä»¥ç²å¾—æœ€æ–°åŠŸèƒ½å’Œæ•ˆèƒ½æ”¹é€²</li>
<li>åŠ å…¥ç¤¾ç¾¤ç²å¾—æ”¯æ´å’Œåˆ†äº«ç¶“é©—</li>
</ul>
<p>ğŸ“ <strong>æˆæ¬Š</strong>: MIT License</p>
<p>ğŸ¤ <strong>è²¢ç»</strong>: æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ml/opensource-llm-tuning-guide.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../ml/ollama_guide_markdown.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ml/opensource-llm-tuning-guide.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../ml/ollama_guide_markdown.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
