<!DOCTYPE HTML>
<html lang="zh" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>LLM 推理指南 - Jason Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Jason Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/shihyu/jason_note" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="llm-推理框架完整指南---rtx-3060-優化版"><a class="header" href="#llm-推理框架完整指南---rtx-3060-優化版">LLM 推理框架完整指南 - RTX 3060 優化版</a></h1>
<h2 id="一llamacpp-與其他框架比較"><a class="header" href="#一llamacpp-與其他框架比較">一、llama.cpp 與其他框架比較</a></h2>
<h3 id="llamacpp-主要特點"><a class="header" href="#llamacpp-主要特點">llama.cpp 主要特點</a></h3>
<h4 id="優勢"><a class="header" href="#優勢">優勢</a></h4>
<ul>
<li><strong>極致的效率優化</strong>：純 C/C++ 實現，無需 Python 依賴</li>
<li><strong>低記憶體需求</strong>：支援 4-bit、5-bit、8-bit 量化</li>
<li><strong>跨平台支援</strong>：Windows、Linux、macOS、Android、iOS</li>
<li><strong>CPU 優化</strong>：特別適合在沒有 GPU 的環境運行</li>
<li><strong>輕量級</strong>：編譯後的執行檔很小，部署簡單</li>
</ul>
<h4 id="限制"><a class="header" href="#限制">限制</a></h4>
<ul>
<li>主要針對 Llama 系列模型優化</li>
<li>功能相對單一，專注於推理</li>
</ul>
<h3 id="與其他主流框架比較"><a class="header" href="#與其他主流框架比較">與其他主流框架比較</a></h3>
<h4 id="1-vllm"><a class="header" href="#1-vllm">1. vLLM</a></h4>
<ul>
<li><strong>優勢</strong>：PagedAttention 技術、高吞吐量、生產環境優化</li>
<li><strong>劣勢</strong>：需要 GPU、Python 依賴較重</li>
<li><strong>適用場景</strong>：大規模服務部署、需要高並發的 API 服務</li>
</ul>
<h4 id="2-tensorrt-llm-nvidia"><a class="header" href="#2-tensorrt-llm-nvidia">2. TensorRT-LLM (NVIDIA)</a></h4>
<ul>
<li><strong>優勢</strong>：NVIDIA GPU 上效能最佳、支援多 GPU 並行</li>
<li><strong>劣勢</strong>：僅限 NVIDIA GPU、設置複雜</li>
<li><strong>適用場景</strong>：企業級部署、需要極致 GPU 性能</li>
</ul>
<h4 id="3-ollama"><a class="header" href="#3-ollama">3. Ollama</a></h4>
<ul>
<li><strong>優勢</strong>：使用體驗極佳、一鍵安裝、內建模型管理</li>
<li><strong>劣勢</strong>：效能不如專門優化的框架</li>
<li><strong>適用場景</strong>：個人使用、快速原型開發</li>
</ul>
<h4 id="4-text-generation-inference-huggingface"><a class="header" href="#4-text-generation-inference-huggingface">4. Text Generation Inference (HuggingFace)</a></h4>
<ul>
<li><strong>優勢</strong>：與 HuggingFace 生態整合、支援多種模型</li>
<li><strong>劣勢</strong>：資源消耗較大</li>
<li><strong>適用場景</strong>：研究環境、需要靈活切換模型</li>
</ul>
<h4 id="5-exllamav2"><a class="header" href="#5-exllamav2">5. ExLlamaV2</a></h4>
<ul>
<li><strong>優勢</strong>：極致的量化優化、GPTQ 支援優秀</li>
<li><strong>劣勢</strong>：僅支援 Llama 架構、需要 GPU</li>
<li><strong>適用場景</strong>：消費級 GPU 運行大模型</li>
</ul>
<h4 id="6-mlc-llm"><a class="header" href="#6-mlc-llm">6. MLC-LLM</a></h4>
<ul>
<li><strong>優勢</strong>：跨平台（包括瀏覽器 WebGPU）、編譯優化</li>
<li><strong>劣勢</strong>：設置較複雜、社群相對較小</li>
<li><strong>適用場景</strong>：邊緣設備、瀏覽器部署</li>
</ul>
<h3 id="效能對比13b-模型參考數據"><a class="header" href="#效能對比13b-模型參考數據">效能對比（13B 模型參考數據）</a></h3>
<ul>
<li><strong>llama.cpp (CPU)</strong>：15-30 tokens/秒</li>
<li><strong>vLLM (GPU)</strong>：100-200 tokens/秒</li>
<li><strong>TensorRT-LLM</strong>：150-300 tokens/秒</li>
<li><strong>Ollama</strong>：20-100 tokens/秒（依硬體而定）</li>
<li><strong>ExLlamaV2</strong>：80-150 tokens/秒</li>
</ul>
<h3 id="選擇建議"><a class="header" href="#選擇建議">選擇建議</a></h3>
<p><strong>選擇 llama.cpp</strong> 如果您：</p>
<ul>
<li>沒有 GPU 或只有消費級顯卡</li>
<li>需要在邊緣設備或嵌入式系統運行</li>
<li>重視低資源消耗和部署簡單性</li>
<li>主要使用 Llama 系列模型</li>
</ul>
<p><strong>選擇其他框架</strong> 如果您：</p>
<ul>
<li>有專業 GPU 且需要最高效能 → TensorRT-LLM</li>
<li>需要高並發 API 服務 → vLLM</li>
<li>想要最簡單的使用體驗 → Ollama</li>
<li>需要支援多種模型架構 → HuggingFace TGI</li>
</ul>
<hr />
<h2 id="二超越-llamacpp-的選項"><a class="header" href="#二超越-llamacpp-的選項">二、超越 llama.cpp 的選項</a></h2>
<h3 id="cpu-環境下的競爭者"><a class="header" href="#cpu-環境下的競爭者">CPU 環境下的競爭者</a></h3>
<h4 id="1-llamafile-mozilla"><a class="header" href="#1-llamafile-mozilla">1. llamafile (Mozilla)</a></h4>
<ul>
<li><strong>優勢</strong>：基於 llama.cpp 但更進一步優化</li>
<li><strong>特色</strong>：單一執行檔包含模型和推理引擎</li>
<li><strong>效能</strong>：與 llama.cpp 相當或略優</li>
<li><strong>便利性</strong>：勝過 llama.cpp</li>
</ul>
<h4 id="2-candle-rust"><a class="header" href="#2-candle-rust">2. candle (Rust)</a></h4>
<ul>
<li><strong>優勢</strong>：Rust 實現，記憶體安全性更好</li>
<li><strong>效能</strong>：某些情況下與 llama.cpp 相當</li>
<li><strong>生態系統</strong>：Rust 生態整合更好</li>
<li><strong>成熟度</strong>：仍在快速發展</li>
</ul>
<h4 id="3-intel-neural-compressor--openvino"><a class="header" href="#3-intel-neural-compressor--openvino">3. Intel Neural Compressor / OpenVINO</a></h4>
<ul>
<li><strong>優勢</strong>：在 Intel CPU 上可能有 20-40% 效能提升</li>
<li><strong>限制</strong>：主要針對 Intel 硬體優化</li>
<li><strong>使用場景</strong>：Intel 平台專屬優化</li>
</ul>
<h3 id="cpu-效能比較llama2-13b-4-bit"><a class="header" href="#cpu-效能比較llama2-13b-4-bit">CPU 效能比較（Llama2-13B 4-bit）</a></h3>
<pre><code>llama.cpp:           25-30 tokens/秒
llamafile:           25-32 tokens/秒  
candle:              20-28 tokens/秒
OpenVINO (Intel):    35-40 tokens/秒 (Intel CPU)
ONNX Runtime:        22-28 tokens/秒
</code></pre>
<h3 id="特殊硬體平台的最佳選擇"><a class="header" href="#特殊硬體平台的最佳選擇">特殊硬體平台的最佳選擇</a></h3>
<h4 id="apple-silicon-m1m2m3"><a class="header" href="#apple-silicon-m1m2m3">Apple Silicon (M1/M2/M3)</a></h4>
<ul>
<li><strong>MLX</strong> (Apple 官方)：可能有 30-50% 效能優勢</li>
<li>充分利用 Apple 統一記憶體架構</li>
</ul>
<h4 id="android-手機"><a class="header" href="#android-手機">Android 手機</a></h4>
<ul>
<li><strong>MNN</strong> (阿里巴巴)：移動端優化更好</li>
<li><strong>NCNN</strong> (騰訊)：在某些 Android 設備上更快</li>
</ul>
<h3 id="結論"><a class="header" href="#結論">結論</a></h3>
<p>綜合考慮效能、穩定性、易用性，llama.cpp 仍是 CPU 推理的最佳選擇，但特定硬體平台可能有更優選擇。</p>
<hr />
<h2 id="三rtx-3060-12gb-gpu-最佳方案"><a class="header" href="#三rtx-3060-12gb-gpu-最佳方案">三、RTX 3060 12GB GPU 最佳方案</a></h2>
<h3 id="推薦順序"><a class="header" href="#推薦順序">推薦順序</a></h3>
<h4 id="-最推薦exllamav2"><a class="header" href="#-最推薦exllamav2">🏆 最推薦：ExLlamaV2</a></h4>
<pre><code class="language-bash"># 安裝
pip install exllamav2

# 效能預期
# 7B 模型：約 80-120 tokens/秒
# 13B 模型：約 40-60 tokens/秒
</code></pre>
<p><strong>優勢</strong>：</p>
<ul>
<li>專為消費級 NVIDIA GPU 優化</li>
<li>支援優秀的 GPTQ 量化</li>
<li>12GB 可跑到 30B 模型（4-bit）</li>
<li>記憶體使用效率極高</li>
</ul>
<h4 id="-次推薦ollama最簡單"><a class="header" href="#-次推薦ollama最簡單">🥈 次推薦：Ollama（最簡單）</a></h4>
<pre><code class="language-bash"># 一行安裝
curl -fsSL https://ollama.ai/install.sh | sh

# 運行模型
ollama run llama3:8b
</code></pre>
<p><strong>優勢</strong>：</p>
<ul>
<li>使用超級簡單</li>
<li>自動偵測並使用 GPU</li>
<li>內建模型管理</li>
</ul>
<h4 id="-text-generation-webui-oobabooga"><a class="header" href="#-text-generation-webui-oobabooga">🥉 Text Generation WebUI (oobabooga)</a></h4>
<pre><code class="language-bash"># 圖形化介面，整合多種後端
git clone https://github.com/oobabooga/text-generation-webui
</code></pre>
<p><strong>優勢</strong>：</p>
<ul>
<li>支援多種載入方式</li>
<li>友善的網頁介面</li>
<li>適合測試比較</li>
</ul>
<h3 id="rtx-3060-12gb-效能比較"><a class="header" href="#rtx-3060-12gb-效能比較">RTX 3060 12GB 效能比較</a></h3>
<div class="table-wrapper"><table><thead><tr><th>框架</th><th>Tokens/秒</th><th>VRAM 使用</th><th>設置難度</th></tr></thead><tbody>
<tr><td><strong>ExLlamaV2</strong></td><td>45-60</td><td>~7GB</td><td>中等</td></tr>
<tr><td><strong>vLLM</strong></td><td>40-55</td><td>~9GB</td><td>較難</td></tr>
<tr><td><strong>Ollama</strong></td><td>35-50</td><td>~8GB</td><td>極簡單</td></tr>
<tr><td><strong>llama.cpp (CUDA)</strong></td><td>30-40</td><td>~7.5GB</td><td>簡單</td></tr>
<tr><td><strong>Transformers</strong></td><td>25-35</td><td>~10GB</td><td>簡單</td></tr>
</tbody></table>
</div>
<h3 id="可運行的模型大小"><a class="header" href="#可運行的模型大小">可運行的模型大小</a></h3>
<ul>
<li><strong>70B 模型</strong>：2-bit 量化（品質損失較大）</li>
<li><strong>30B 模型</strong>：4-bit 量化（品質不錯）</li>
<li><strong>13B 模型</strong>：8-bit 量化（品質極佳）</li>
<li><strong>7B 模型</strong>：FP16 全精度（最高品質）</li>
</ul>
<h3 id="具體安裝指南"><a class="header" href="#具體安裝指南">具體安裝指南</a></h3>
<h4 id="exllamav2-安裝最佳效能"><a class="header" href="#exllamav2-安裝最佳效能">ExLlamaV2 安裝（最佳效能）</a></h4>
<pre><code class="language-python"># 安裝
pip install exllamav2
pip install flash-attn  # 額外加速

# 使用範例
from exllamav2 import ExLlamaV2, ExLlamaV2Config
from exllamav2.generator import ExLlamaV2Generator

# 載入 GPTQ 量化模型
model_dir = "TheBloke/Llama-2-13B-GPTQ"
</code></pre>
<h4 id="ollama-安裝最簡單"><a class="header" href="#ollama-安裝最簡單">Ollama 安裝（最簡單）</a></h4>
<pre><code class="language-bash"># Windows：下載安裝程式
# Linux/Mac：
curl -fsSL https://ollama.ai/install.sh | sh

# 運行各種模型
ollama run llama3.2:3b      # 3B 模型
ollama run llama3:8b        # 8B 模型  
ollama run qwen2.5:14b      # 14B 模型
</code></pre>
<h4 id="llamacpp-gpu-版本設置"><a class="header" href="#llamacpp-gpu-版本設置">llama.cpp GPU 版本設置</a></h4>
<pre><code class="language-bash"># 編譯 CUDA 版本
cmake -B build -DLLAMA_CUDA=ON
cmake --build build --config Release

# 運行時指定 GPU 層數
./main -m model.gguf -ngl 35  # 35 層放 GPU
</code></pre>
<hr />
<h2 id="四理解-token-與效能"><a class="header" href="#四理解-token-與效能">四、理解 Token 與效能</a></h2>
<h3 id="什麼是-token"><a class="header" href="#什麼是-token">什麼是 Token？</a></h3>
<p>Token 是語言模型處理文字的基本單位，介於字母和單字之間：</p>
<h4 id="範例"><a class="header" href="#範例">範例</a></h4>
<pre><code>英文："Hello, how are you?" → 6 個 tokens
["Hello", ",", " how", " are", " you", "?"]

中文："你好嗎" → 3-5 個 tokens（依模型而定）
["你", "好", "嗎"] 或 ["你好", "嗎"]
</code></pre>
<h4 id="token-的一般規律"><a class="header" href="#token-的一般規律">Token 的一般規律</a></h4>
<ul>
<li><strong>英文</strong>：1 個 token ≈ 0.75 個單字（約 4 個字母）</li>
<li><strong>中文</strong>：1 個 token ≈ 0.5-1 個漢字</li>
<li><strong>程式碼</strong>：變數名、符號通常各算一個 token</li>
</ul>
<h3 id="不同模型的-token-標準差異"><a class="header" href="#不同模型的-token-標準差異">不同模型的 Token 標準差異</a></h3>
<h4 id="實例我愛人工智慧"><a class="header" href="#實例我愛人工智慧">實例："我愛人工智慧"</a></h4>
<pre><code>GPT 系列：    5 tokens ["我", "愛", "人", "工", "智慧"]
LLaMA 系列：   6-7 tokens（對中文切分更細）
ChatGLM：     3-4 tokens ["我", "愛", "人工智慧"]
Qwen：       4 tokens（中文優化更好）
</code></pre>
<h3 id="模型大小與速度關係"><a class="header" href="#模型大小與速度關係">模型大小與速度關係</a></h3>
<h4 id="7b-模型80-120-tokens秒"><a class="header" href="#7b-模型80-120-tokens秒">7B 模型：80-120 tokens/秒</a></h4>
<ul>
<li>70 億參數</li>
<li>每個 token 需要經過 70 億次計算</li>
<li>VRAM 佔用：約 4-6GB（4-bit 量化）</li>
</ul>
<h4 id="13b-模型40-60-tokens秒"><a class="header" href="#13b-模型40-60-tokens秒">13B 模型：40-60 tokens/秒</a></h4>
<ul>
<li>130 億參數（幾乎是 7B 的兩倍）</li>
<li>計算量加倍，速度約減半</li>
<li>VRAM 佔用：約 7-9GB（4-bit 量化）</li>
</ul>
<h3 id="tokens秒的實際體感"><a class="header" href="#tokens秒的實際體感">Tokens/秒的實際體感</a></h3>
<pre><code>10 tokens/秒：  明顯卡頓，像打字機
30 tokens/秒：  流暢，像正常閱讀速度  
60 tokens/秒：  很快，像快速瀏覽
100+ tokens/秒： 極快，幾乎即時
</code></pre>
<h4 id="實際輸出速度"><a class="header" href="#實際輸出速度">實際輸出速度</a></h4>
<ul>
<li><strong>中文輸出</strong>：30 tokens/秒 ≈ 每秒 15-30 個字</li>
<li><strong>英文輸出</strong>：30 tokens/秒 ≈ 每秒 20-25 個單字</li>
<li><strong>一般對話</strong>：30-40 tokens/秒 就很流暢了</li>
</ul>
<h3 id="token-計算工具"><a class="header" href="#token-計算工具">Token 計算工具</a></h3>
<pre><code class="language-python"># 使用 tiktoken（OpenAI）
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
tokens = enc.encode("Hello, world!")
print(f"Token 數：{len(tokens)}")  # 輸出：3

# 使用 transformers（HuggingFace）
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")
tokens = tokenizer.encode("Hello, world!")
print(f"Token 數：{len(tokens)}")
</code></pre>
<h3 id="為什麼要關心-token"><a class="header" href="#為什麼要關心-token">為什麼要關心 Token？</a></h3>
<ol>
<li>
<p><strong>API 計費基準</strong></p>
<ul>
<li>GPT-4：約 $0.03/1K tokens</li>
<li>Claude：約 $0.025/1K tokens</li>
</ul>
</li>
<li>
<p><strong>上下文長度限制</strong></p>
<ul>
<li>GPT-4：128K tokens 限制</li>
<li>Llama-2：4K tokens 限制</li>
<li>Claude：200K tokens 限制</li>
</ul>
</li>
<li>
<p><strong>效能評估標準</strong></p>
<ul>
<li>同樣 30 tokens/秒，中文可能比英文慢</li>
</ul>
</li>
</ol>
<h3 id="rtx-3060-實際使用體驗"><a class="header" href="#rtx-3060-實際使用體驗">RTX 3060 實際使用體驗</a></h3>
<h4 id="7b-模型--100-tokens秒"><a class="header" href="#7b-模型--100-tokens秒">7B 模型 @ 100 tokens/秒</a></h4>
<ul>
<li>英文：每秒約 75 個單字（極快）</li>
<li>中文：每秒約 50-100 個字（很快）</li>
<li>適合：即時對話、程式碼生成</li>
</ul>
<h4 id="13b-模型--50-tokens秒"><a class="header" href="#13b-模型--50-tokens秒">13B 模型 @ 50 tokens/秒</a></h4>
<ul>
<li>英文：每秒約 35-40 個單字（流暢）</li>
<li>中文：每秒約 25-50 個字（流暢）</li>
<li>適合：深度對話、專業寫作</li>
</ul>
<hr />
<h2 id="五實用建議總結"><a class="header" href="#五實用建議總結">五、實用建議總結</a></h2>
<h3 id="快速開始指南"><a class="header" href="#快速開始指南">快速開始指南</a></h3>
<ol>
<li><strong>新手入門</strong>：先試 <strong>Ollama</strong>，5 分鐘就能跑起來</li>
<li><strong>追求效能</strong>：用 <strong>ExLlamaV2</strong>，充分發揮 3060 實力</li>
<li><strong>需要介面</strong>：用 <strong>Text Generation WebUI</strong></li>
<li><strong>已熟悉 llama.cpp</strong>：編譯 CUDA 版本繼續使用</li>
</ol>
<h3 id="模型選擇建議"><a class="header" href="#模型選擇建議">模型選擇建議</a></h3>
<ul>
<li><strong>日常對話</strong>：7B 模型足夠，速度快</li>
<li><strong>專業任務</strong>：13B 模型，品質更好</li>
<li><strong>中文使用</strong>：優先選擇 Qwen、ChatGLM（tokenizer 對中文更友善）</li>
</ul>
<h3 id="效能測試方法"><a class="header" href="#效能測試方法">效能測試方法</a></h3>
<pre><code class="language-bash"># 大多數工具會顯示 tokens/秒
llama.cpp: 會顯示 "tok/s"
ollama: 運行時顯示速度統計
exllama: 提供詳細的效能指標
</code></pre>
<h3 id="rtx-3060-12gb-最佳實踐"><a class="header" href="#rtx-3060-12gb-最佳實踐">RTX 3060 12GB 最佳實踐</a></h3>
<ol>
<li>使用 4-bit 量化以支援更大模型</li>
<li>優先選擇 GPU 優化框架（ExLlamaV2、vLLM）</li>
<li>合理配置 VRAM 使用，預留 1-2GB 給系統</li>
<li>定期更新驅動程式和 CUDA 版本</li>
</ol>
<hr />
<h2 id="附錄常用資源連結"><a class="header" href="#附錄常用資源連結">附錄：常用資源連結</a></h2>
<h3 id="模型下載"><a class="header" href="#模型下載">模型下載</a></h3>
<ul>
<li><a href="https://huggingface.co/models">HuggingFace</a></li>
<li><a href="https://huggingface.co/TheBloke">TheBloke GPTQ Models</a></li>
<li><a href="https://ollama.ai/library">Ollama Model Library</a></li>
</ul>
<h3 id="框架官方文檔"><a class="header" href="#框架官方文檔">框架官方文檔</a></h3>
<ul>
<li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></li>
<li><a href="https://github.com/turboderp/exllamav2">ExLlamaV2</a></li>
<li><a href="https://docs.vllm.ai/">vLLM</a></li>
<li><a href="https://ollama.ai/">Ollama</a></li>
</ul>
<h3 id="社群資源"><a class="header" href="#社群資源">社群資源</a></h3>
<ul>
<li><a href="https://reddit.com/r/LocalLLaMA">LocalLLaMA Reddit</a></li>
<li><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM 效能排行榜</a></li>
</ul>
<hr />
<p><em>最後更新：2025年1月</em></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ml/opensource-llm-tuning-guide.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../ml/pytorch.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ml/opensource-llm-tuning-guide.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../ml/pytorch.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
