<!DOCTYPE HTML>
<html lang="zh" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AI è­˜åˆ¥æŒ‡å— - Jason&#x27;s Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Jason&#x27;s Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/shihyu/jason_note" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="ai-èªéŸ³èˆ‡å½±åƒè¾¨è­˜æŠ€è¡“æŒ‡å—"><a class="header" href="#ai-èªéŸ³èˆ‡å½±åƒè¾¨è­˜æŠ€è¡“æŒ‡å—">AI èªéŸ³èˆ‡å½±åƒè¾¨è­˜æŠ€è¡“æŒ‡å—</a></h1>
<h2 id="-ç›®éŒ„"><a class="header" href="#-ç›®éŒ„">ğŸ“‹ ç›®éŒ„</a></h2>
<ul>
<li><a href="#%E8%AA%9E%E9%9F%B3%E8%BE%A8%E8%AD%98%E6%8A%80%E8%A1%93">èªéŸ³è¾¨è­˜æŠ€è¡“</a></li>
<li><a href="#%E5%BD%B1%E5%83%8F%E8%BE%A8%E8%AD%98%E6%8A%80%E8%A1%93">å½±åƒè¾¨è­˜æŠ€è¡“</a></li>
<li><a href="#%E5%A4%9A%E6%A8%A1%E6%85%8B%E6%87%89%E7%94%A8">å¤šæ¨¡æ…‹æ‡‰ç”¨</a></li>
<li><a href="#%E5%AE%89%E8%A3%9D%E6%8C%87%E5%8D%97">å®‰è£æŒ‡å—</a></li>
<li><a href="#%E5%AF%A6%E9%9A%9B%E6%87%89%E7%94%A8%E6%A1%88%E4%BE%8B">å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹</a></li>
</ul>
<hr />
<h2 id="-èªéŸ³è¾¨è­˜æŠ€è¡“"><a class="header" href="#-èªéŸ³è¾¨è­˜æŠ€è¡“">ğŸ¤ èªéŸ³è¾¨è­˜æŠ€è¡“</a></h2>
<h3 id="1-openai-whisperæ¨è–¦"><a class="header" href="#1-openai-whisperæ¨è–¦">1. OpenAI Whisperï¼ˆæ¨è–¦ï¼‰</a></h3>
<p><strong>ç‰¹é»ï¼š</strong></p>
<ul>
<li>é›¢ç·šé‹è¡Œï¼Œä¿è­·éš±ç§</li>
<li>æ”¯æ´ 99 ç¨®èªè¨€</li>
<li>æº–ç¢ºåº¦æ¥µé«˜</li>
<li>å…è²»é–‹æº</li>
</ul>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install openai-whisper
</code></pre>
<p><strong>åŸºæœ¬ä½¿ç”¨ï¼š</strong></p>
<pre><code class="language-python">import whisper

# è¼‰å…¥æ¨¡å‹ (tiny, base, small, medium, large)
model = whisper.load_model("base")

# è¾¨è­˜éŸ³æª”
result = model.transcribe("audio.mp3")
print(result["text"])

# æ”¯æ´ä¸­æ–‡
result = model.transcribe("chinese_audio.mp3", language="zh")
print(result["text"])

# å–å¾—æ™‚é–“æˆ³è¨˜
result = model.transcribe("audio.mp3", verbose=True)
for segment in result["segments"]:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] {segment['text']}")
</code></pre>
<h3 id="2-google-speech-to-text"><a class="header" href="#2-google-speech-to-text">2. Google Speech-to-Text</a></h3>
<p><strong>ç‰¹é»ï¼š</strong></p>
<ul>
<li>é›²ç«¯æœå‹™ï¼Œæº–ç¢ºåº¦é«˜</li>
<li>æ”¯æ´å³æ™‚ä¸²æµ</li>
<li>è‡ªå‹•æ¨™é»ç¬¦è™Ÿ</li>
</ul>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install google-cloud-speech
</code></pre>
<p><strong>ä½¿ç”¨ç¯„ä¾‹ï¼š</strong></p>
<pre><code class="language-python">from google.cloud import speech
import io

def transcribe_file(speech_file):
    """è½‰éŒ„æœ¬åœ°éŸ³æª”"""
    client = speech.SpeechClient()

    with io.open(speech_file, "rb") as audio_file:
        content = audio_file.read()

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="zh-TW",  # ç¹é«”ä¸­æ–‡
        enable_automatic_punctuation=True,
    )

    response = client.recognize(config=config, audio=audio)
    
    for result in response.results:
        print(f"è½‰éŒ„çµæœ: {result.alternatives[0].transcript}")
        print(f"ä¿¡å¿ƒåˆ†æ•¸: {result.alternatives[0].confidence}")
</code></pre>
<h3 id="3-å³æ™‚èªéŸ³è¾¨è­˜"><a class="header" href="#3-å³æ™‚èªéŸ³è¾¨è­˜">3. å³æ™‚èªéŸ³è¾¨è­˜</a></h3>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install SpeechRecognition pyaudio
</code></pre>
<p><strong>å³æ™‚éº¥å…‹é¢¨è¾¨è­˜ï¼š</strong></p>
<pre><code class="language-python">import speech_recognition as sr

def live_speech_recognition():
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    
    print("èª¿æ•´ç’°å¢ƒå™ªéŸ³...")
    with mic as source:
        recognizer.adjust_for_ambient_noise(source, duration=1)
    
    print("é–‹å§‹èªªè©±...")
    
    while True:
        try:
            with mic as source:
                # è¨­å®šè¶…æ™‚æ™‚é–“
                audio = recognizer.listen(source, timeout=1, phrase_time_limit=5)
                
            # ä½¿ç”¨ Google APIï¼ˆå…è²»ï¼‰
            text = recognizer.recognize_google(audio, language="zh-TW")
            print(f"ä½ èªª: {text}")
            
            # ä¹Ÿå¯ä»¥ä½¿ç”¨ Whisper
            # text = recognizer.recognize_whisper(audio, language="chinese")
            
        except sr.UnknownValueError:
            pass  # ç„¡æ³•è¾¨è­˜
        except sr.RequestError as e:
            print(f"éŒ¯èª¤: {e}")
        except KeyboardInterrupt:
            print("\nåœæ­¢è¾¨è­˜")
            break

if __name__ == "__main__":
    live_speech_recognition()
</code></pre>
<hr />
<h2 id="-å½±åƒè¾¨è­˜æŠ€è¡“"><a class="header" href="#-å½±åƒè¾¨è­˜æŠ€è¡“">ğŸ“· å½±åƒè¾¨è­˜æŠ€è¡“</a></h2>
<h3 id="1-yolo-v8ç‰©ä»¶åµæ¸¬"><a class="header" href="#1-yolo-v8ç‰©ä»¶åµæ¸¬">1. YOLO v8ï¼ˆç‰©ä»¶åµæ¸¬ï¼‰</a></h3>
<p><strong>ç‰¹é»ï¼š</strong></p>
<ul>
<li>å³æ™‚åµæ¸¬</li>
<li>é«˜æº–ç¢ºåº¦</li>
<li>æ”¯æ´å½±ç‰‡ä¸²æµ</li>
</ul>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install ultralytics
</code></pre>
<p><strong>ä½¿ç”¨ç¯„ä¾‹ï¼š</strong></p>
<pre><code class="language-python">from ultralytics import YOLO
import cv2

# è¼‰å…¥é è¨“ç·´æ¨¡å‹
model = YOLO('yolov8n.pt')  # n=nano, s=small, m=medium, l=large, x=extra large

# åœ–ç‰‡åµæ¸¬
def detect_image(image_path):
    results = model(image_path)
    
    for r in results:
        boxes = r.boxes
        for box in boxes:
            # å–å¾—åº§æ¨™
            x1, y1, x2, y2 = box.xyxy[0]
            # å–å¾—é¡åˆ¥å’Œä¿¡å¿ƒåˆ†æ•¸
            conf = box.conf[0]
            cls = box.cls[0]
            print(f"åµæ¸¬åˆ°: {model.names[int(cls)]} (ä¿¡å¿ƒåº¦: {conf:.2f})")
    
    # å„²å­˜çµæœ
    results[0].save(filename='result.jpg')

# å½±ç‰‡å³æ™‚åµæ¸¬
def detect_video(video_path):
    cap = cv2.VideoCapture(video_path)  # æˆ–ä½¿ç”¨ 0 ç‚ºæ”å½±æ©Ÿ
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
            
        results = model(frame)
        annotated_frame = results[0].plot()
        
        cv2.imshow('YOLOv8 åµæ¸¬', annotated_frame)
        
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
</code></pre>
<h3 id="2-transformers-å½±åƒåˆ†é¡"><a class="header" href="#2-transformers-å½±åƒåˆ†é¡">2. Transformers å½±åƒåˆ†é¡</a></h3>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install transformers torch pillow
</code></pre>
<p><strong>ä½¿ç”¨ç¯„ä¾‹ï¼š</strong></p>
<pre><code class="language-python">from transformers import pipeline
from PIL import Image

# å»ºç«‹åˆ†é¡å™¨
classifier = pipeline("image-classification", 
                     model="google/vit-base-patch16-224")

def classify_image(image_path):
    image = Image.open(image_path)
    results = classifier(image)
    
    print("å½±åƒåˆ†é¡çµæœ:")
    for item in results[:5]:  # é¡¯ç¤ºå‰5å€‹çµæœ
        print(f"  {item['label']}: {item['score']:.3f}")
    
    return results

# ç‰©ä»¶åµæ¸¬
detector = pipeline("object-detection", 
                   model="facebook/detr-resnet-50")

def detect_objects(image_path):
    image = Image.open(image_path)
    results = detector(image)
    
    print("åµæ¸¬åˆ°çš„ç‰©ä»¶:")
    for item in results:
        print(f"  {item['label']}: {item['score']:.2f}")
        print(f"    ä½ç½®: {item['box']}")
    
    return results
</code></pre>
<h3 id="3-è‡‰éƒ¨è¾¨è­˜"><a class="header" href="#3-è‡‰éƒ¨è¾¨è­˜">3. è‡‰éƒ¨è¾¨è­˜</a></h3>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install face-recognition opencv-python
</code></pre>
<p><strong>ä½¿ç”¨ç¯„ä¾‹ï¼š</strong></p>
<pre><code class="language-python">import face_recognition
import cv2
import numpy as np

def face_detection_and_recognition():
    # è¼‰å…¥å·²çŸ¥äººè‡‰
    known_image = face_recognition.load_image_file("person1.jpg")
    known_encoding = face_recognition.face_encodings(known_image)[0]
    
    known_face_encodings = [known_encoding]
    known_face_names = ["Person 1"]
    
    # é–‹å•Ÿæ”å½±æ©Ÿ
    video_capture = cv2.VideoCapture(0)
    
    while True:
        ret, frame = video_capture.read()
        
        # è½‰æ› BGR (OpenCV) åˆ° RGB (face_recognition)
        rgb_frame = frame[:, :, ::-1]
        
        # æ‰¾å‡ºæ‰€æœ‰è‡‰éƒ¨ä½ç½®å’Œç·¨ç¢¼
        face_locations = face_recognition.face_locations(rgb_frame)
        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
        
        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
            # æ¯”å°è‡‰éƒ¨
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
            name = "Unknown"
            
            # è¨ˆç®—è·é›¢æ‰¾å‡ºæœ€ä½³åŒ¹é…
            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
            best_match_index = np.argmin(face_distances)
            if matches[best_match_index]:
                name = known_face_names[best_match_index]
            
            # ç•«æ¡†å’Œæ¨™ç±¤
            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)
            cv2.putText(frame, name, (left, top - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)
        
        cv2.imshow('Video', frame)
        
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
    
    video_capture.release()
    cv2.destroyAllWindows()
</code></pre>
<hr />
<h2 id="-å¤šæ¨¡æ…‹æ‡‰ç”¨"><a class="header" href="#-å¤šæ¨¡æ…‹æ‡‰ç”¨">ğŸ”„ å¤šæ¨¡æ…‹æ‡‰ç”¨</a></h2>
<h3 id="1-clip---åœ–æ–‡åŒ¹é…"><a class="header" href="#1-clip---åœ–æ–‡åŒ¹é…">1. CLIP - åœ–æ–‡åŒ¹é…</a></h3>
<p><strong>å®‰è£ï¼š</strong></p>
<pre><code class="language-bash">pip install transformers torch pillow
</code></pre>
<p><strong>ä½¿ç”¨ç¯„ä¾‹ï¼š</strong></p>
<pre><code class="language-python">from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def image_text_similarity(image_path, text_descriptions):
    """è¨ˆç®—åœ–ç‰‡èˆ‡æ–‡å­—æè¿°çš„ç›¸ä¼¼åº¦"""
    image = Image.open(image_path)
    
    # è™•ç†è¼¸å…¥
    inputs = processor(
        text=text_descriptions, 
        images=image, 
        return_tensors="pt", 
        padding=True
    )
    
    # è¨ˆç®—ç›¸ä¼¼åº¦
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)
    
    # é¡¯ç¤ºçµæœ
    for i, (desc, prob) in enumerate(zip(text_descriptions, probs[0])):
        print(f"{desc}: {prob:.2%}")
    
    # å›å‚³æœ€å¯èƒ½çš„æè¿°
    max_idx = probs.argmax()
    return text_descriptions[max_idx]

# ä½¿ç”¨ç¯„ä¾‹
descriptions = [
    "ä¸€éš»è²“åœ¨ç¡è¦º",
    "ä¸€éš»ç‹—åœ¨ç©çƒ",
    "ä¸€å€‹äººåœ¨è·‘æ­¥",
    "ä¸€è¼›è»Šåœ¨è·¯ä¸Š"
]

best_match = image_text_similarity("test_image.jpg", descriptions)
print(f"\næœ€ä½³åŒ¹é…: {best_match}")
</code></pre>
<h3 id="2-å½±ç‰‡ç†è§£çµåˆèªéŸ³å’Œè¦–è¦º"><a class="header" href="#2-å½±ç‰‡ç†è§£çµåˆèªéŸ³å’Œè¦–è¦º">2. å½±ç‰‡ç†è§£ï¼ˆçµåˆèªéŸ³å’Œè¦–è¦ºï¼‰</a></h3>
<pre><code class="language-python">import whisper
import cv2
from ultralytics import YOLO
import numpy as np

class VideoAnalyzer:
    def __init__(self):
        self.whisper_model = whisper.load_model("base")
        self.yolo_model = YOLO('yolov8n.pt')
    
    def analyze_video(self, video_path):
        """å®Œæ•´åˆ†æå½±ç‰‡å…§å®¹"""
        # æå–éŸ³è¨Šä¸¦è½‰éŒ„
        audio_text = self.transcribe_audio(video_path)
        
        # åˆ†æè¦–è¦ºå…§å®¹
        visual_summary = self.analyze_visual(video_path)
        
        return {
            "audio_transcript": audio_text,
            "visual_summary": visual_summary
        }
    
    def transcribe_audio(self, video_path):
        """æå–ä¸¦è½‰éŒ„éŸ³è¨Š"""
        # ä½¿ç”¨ ffmpeg æå–éŸ³è¨Šï¼ˆéœ€è¦å…ˆå®‰è£ ffmpegï¼‰
        import subprocess
        audio_path = "temp_audio.wav"
        subprocess.run([
            "ffmpeg", "-i", video_path, 
            "-vn", "-acodec", "pcm_s16le", 
            "-ar", "16000", "-ac", "1", 
            audio_path, "-y"
        ])
        
        result = self.whisper_model.transcribe(audio_path)
        return result["text"]
    
    def analyze_visual(self, video_path, sample_rate=30):
        """åˆ†æè¦–è¦ºå…§å®¹"""
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        detected_objects = {}
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            # æ¯ sample_rate å¹€åˆ†æä¸€æ¬¡
            if frame_count % sample_rate == 0:
                results = self.yolo_model(frame)
                
                for r in results:
                    for box in r.boxes:
                        cls = int(box.cls[0])
                        label = self.yolo_model.names[cls]
                        
                        if label not in detected_objects:
                            detected_objects[label] = 0
                        detected_objects[label] += 1
            
            frame_count += 1
        
        cap.release()
        return detected_objects

# ä½¿ç”¨ç¯„ä¾‹
analyzer = VideoAnalyzer()
results = analyzer.analyze_video("sample_video.mp4")
print("èªéŸ³å…§å®¹:", results["audio_transcript"])
print("è¦–è¦ºå…§å®¹:", results["visual_summary"])
</code></pre>
<hr />
<h2 id="-å®‰è£æŒ‡å—"><a class="header" href="#-å®‰è£æŒ‡å—">ğŸ“¦ å®‰è£æŒ‡å—</a></h2>
<h3 id="åŸºç¤ç’°å¢ƒè¨­å®š"><a class="header" href="#åŸºç¤ç’°å¢ƒè¨­å®š">åŸºç¤ç’°å¢ƒè¨­å®š</a></h3>
<pre><code class="language-bash"># å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv ai_recognition_env
source ai_recognition_env/bin/activate  # Linux/Mac
# æˆ–
ai_recognition_env\Scripts\activate  # Windows

# å‡ç´š pip
pip install --upgrade pip
</code></pre>
<h3 id="å®Œæ•´å®‰è£å¥—ä»¶"><a class="header" href="#å®Œæ•´å®‰è£å¥—ä»¶">å®Œæ•´å®‰è£å¥—ä»¶</a></h3>
<pre><code class="language-bash"># èªéŸ³è¾¨è­˜å¥—ä»¶
pip install openai-whisper
pip install SpeechRecognition
pip install pyaudio  # å¯èƒ½éœ€è¦é¡å¤–å®‰è£ portaudio

# å½±åƒè¾¨è­˜å¥—ä»¶
pip install ultralytics  # YOLO
pip install transformers  # Hugging Face models
pip install torch torchvision  # PyTorch
pip install opencv-python  # OpenCV
pip install face-recognition  # è‡‰éƒ¨è¾¨è­˜

# å·¥å…·å¥—ä»¶
pip install pillow  # åœ–ç‰‡è™•ç†
pip install numpy  # æ•¸å€¼é‹ç®—
pip install matplotlib  # è¦–è¦ºåŒ–
</code></pre>
<h3 id="docker-å®¹å™¨è¨­å®š"><a class="header" href="#docker-å®¹å™¨è¨­å®š">Docker å®¹å™¨è¨­å®š</a></h3>
<pre><code class="language-dockerfile">FROM python:3.9-slim

WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update &amp;&amp; apt-get install -y \
    ffmpeg \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    wget \
    &amp;&amp; rm -rf /var/lib/apt/lists/*

# å®‰è£ Python å¥—ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
</code></pre>
<hr />
<h2 id="-å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹"><a class="header" href="#-å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹">ğŸ’¡ å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹</a></h2>
<h3 id="1-æ™ºæ…§æœƒè­°ç³»çµ±"><a class="header" href="#1-æ™ºæ…§æœƒè­°ç³»çµ±">1. æ™ºæ…§æœƒè­°ç³»çµ±</a></h3>
<p><strong>åŠŸèƒ½ï¼š</strong></p>
<ul>
<li>å³æ™‚èªéŸ³è½‰æ–‡å­—ç´€éŒ„</li>
<li>ç™¼è¨€è€…è­˜åˆ¥</li>
<li>é‡é»æ‘˜è¦ç”Ÿæˆ</li>
</ul>
<pre><code class="language-python">class SmartMeetingSystem:
    def __init__(self):
        self.whisper_model = whisper.load_model("medium")
        self.speakers = {}
    
    def process_meeting(self, audio_file):
        # è½‰éŒ„æœƒè­°å…§å®¹
        result = self.whisper_model.transcribe(
            audio_file,
            language="zh",
            verbose=True
        )
        
        # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜çš„é€å­—ç¨¿
        transcript = []
        for segment in result["segments"]:
            transcript.append({
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"],
                "speaker": self.identify_speaker(segment)  # éœ€å¯¦ä½œ
            })
        
        return transcript
</code></pre>
<h3 id="2-æ™ºæ…§å®‰é˜²ç³»çµ±"><a class="header" href="#2-æ™ºæ…§å®‰é˜²ç³»çµ±">2. æ™ºæ…§å®‰é˜²ç³»çµ±</a></h3>
<p><strong>åŠŸèƒ½ï¼š</strong></p>
<ul>
<li>äººè‡‰è­˜åˆ¥é–€ç¦</li>
<li>ç•°å¸¸è¡Œç‚ºåµæ¸¬</li>
<li>å³æ™‚è­¦å ±é€šçŸ¥</li>
</ul>
<pre><code class="language-python">class SecuritySystem:
    def __init__(self):
        self.yolo_model = YOLO('yolov8x.pt')
        self.known_faces = self.load_known_faces()
        self.alert_actions = ['fighting', 'falling', 'running']
    
    def monitor_camera(self, camera_id):
        cap = cv2.VideoCapture(camera_id)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                continue
            
            # ç‰©ä»¶å’Œè¡Œç‚ºåµæ¸¬
            results = self.yolo_model(frame)
            
            # æª¢æŸ¥ç•°å¸¸è¡Œç‚º
            for r in results:
                for box in r.boxes:
                    label = self.yolo_model.names[int(box.cls[0])]
                    if label in self.alert_actions:
                        self.send_alert(f"åµæ¸¬åˆ°ç•°å¸¸è¡Œç‚º: {label}")
            
            # äººè‡‰è­˜åˆ¥
            faces = self.detect_faces(frame)
            for face in faces:
                if not self.is_authorized(face):
                    self.send_alert("åµæ¸¬åˆ°æœªæˆæ¬Šäººå“¡")
</code></pre>
<h3 id="3-ç„¡éšœç¤™è¼”åŠ©å·¥å…·"><a class="header" href="#3-ç„¡éšœç¤™è¼”åŠ©å·¥å…·">3. ç„¡éšœç¤™è¼”åŠ©å·¥å…·</a></h3>
<p><strong>åŠŸèƒ½ï¼š</strong></p>
<ul>
<li>ç‚ºè¦–éšœè€…æè¿°ç’°å¢ƒ</li>
<li>æ–‡å­—è½‰èªéŸ³</li>
<li>æ‰‹èªç¿»è­¯</li>
</ul>
<pre><code class="language-python">class AccessibilityAssistant:
    def __init__(self):
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        self.yolo_model = YOLO('yolov8n.pt')
    
    def describe_scene(self, image_path):
        """ç‚ºè¦–éšœè€…æè¿°å ´æ™¯"""
        image = Image.open(image_path)
        
        # åµæ¸¬ç‰©ä»¶
        results = self.yolo_model(image)
        objects = []
        for r in results:
            for box in r.boxes:
                label = self.yolo_model.names[int(box.cls[0])]
                objects.append(label)
        
        # ç”Ÿæˆå ´æ™¯æè¿°
        description = f"å ´æ™¯ä¸­åŒ…å«: {', '.join(set(objects))}"
        
        # ä½¿ç”¨ CLIP ç²å–æ›´è©³ç´°çš„æè¿°
        scene_types = [
            "å®¤å…§å ´æ™¯", "å®¤å¤–å ´æ™¯", "è¡—é“", "å…¬åœ’", 
            "è¾¦å…¬å®¤", "å®¶åº­ç’°å¢ƒ", "å•†åº—"
        ]
        
        inputs = self.clip_processor(
            text=scene_types, 
            images=image, 
            return_tensors="pt", 
            padding=True
        )
        
        outputs = self.clip_model(**inputs)
        probs = outputs.logits_per_image.softmax(dim=1)
        best_scene = scene_types[probs.argmax()]
        
        description += f"ï¼Œé€™ä¼¼ä¹æ˜¯ä¸€å€‹{best_scene}"
        
        return description
</code></pre>
<h3 id="4-å…§å®¹å‰µä½œåŠ©æ‰‹"><a class="header" href="#4-å…§å®¹å‰µä½œåŠ©æ‰‹">4. å…§å®¹å‰µä½œåŠ©æ‰‹</a></h3>
<p><strong>åŠŸèƒ½ï¼š</strong></p>
<ul>
<li>è‡ªå‹•ç”Ÿæˆå½±ç‰‡å­—å¹•</li>
<li>å…§å®¹æ¨™ç±¤å»ºè­°</li>
<li>ç²¾å½©ç‰‡æ®µæ“·å–</li>
</ul>
<pre><code class="language-python">class ContentCreatorAssistant:
    def __init__(self):
        self.whisper_model = whisper.load_model("base")
        self.yolo_model = YOLO('yolov8n.pt')
    
    def generate_subtitles(self, video_path, output_srt):
        """ç”Ÿæˆ SRT å­—å¹•æª”"""
        result = self.whisper_model.transcribe(video_path)
        
        with open(output_srt, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(result["segments"], 1):
                # SRT æ ¼å¼
                f.write(f"{i}\n")
                f.write(f"{self.format_time(segment['start'])} --&gt; {self.format_time(segment['end'])}\n")
                f.write(f"{segment['text'].strip()}\n\n")
    
    def format_time(self, seconds):
        """è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{seconds:06.3f}".replace('.', ',')
    
    def suggest_tags(self, video_path, num_frames=10):
        """å»ºè­°å½±ç‰‡æ¨™ç±¤"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_interval = total_frames // num_frames
        
        all_objects = {}
        
        for i in range(0, total_frames, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                continue
            
            results = self.yolo_model(frame)
            for r in results:
                for box in r.boxes:
                    label = self.yolo_model.names[int(box.cls[0])]
                    all_objects[label] = all_objects.get(label, 0) + 1
        
        cap.release()
        
        # æ’åºä¸¦å›å‚³æœ€å¸¸å‡ºç¾çš„æ¨™ç±¤
        sorted_tags = sorted(all_objects.items(), key=lambda x: x[1], reverse=True)
        return [tag for tag, _ in sorted_tags[:10]]
</code></pre>
<hr />
<h2 id="-åƒè€ƒè³‡æº"><a class="header" href="#-åƒè€ƒè³‡æº">ğŸ“š åƒè€ƒè³‡æº</a></h2>
<h3 id="å®˜æ–¹æ–‡æª”"><a class="header" href="#å®˜æ–¹æ–‡æª”">å®˜æ–¹æ–‡æª”</a></h3>
<ul>
<li><a href="https://github.com/openai/whisper">OpenAI Whisper</a></li>
<li><a href="https://docs.ultralytics.com/">Ultralytics YOLOv8</a></li>
<li><a href="https://huggingface.co/docs/transformers">Hugging Face Transformers</a></li>
<li><a href="https://cloud.google.com/speech-to-text/docs">Google Cloud Speech-to-Text</a></li>
</ul>
<h3 id="æ•™å­¸è³‡æº"><a class="header" href="#æ•™å­¸è³‡æº">æ•™å­¸è³‡æº</a></h3>
<ul>
<li><a href="https://pytorch.org/tutorials/">PyTorch å®˜æ–¹æ•™å­¸</a></li>
<li><a href="https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html">OpenCV Python æ•™å­¸</a></li>
<li><a href="https://www.tensorflow.org/js/demos">TensorFlow.js ç¯„ä¾‹</a></li>
</ul>
<h3 id="æ¨¡å‹åº«"><a class="header" href="#æ¨¡å‹åº«">æ¨¡å‹åº«</a></h3>
<ul>
<li><a href="https://huggingface.co/models">Hugging Face Model Hub</a></li>
<li><a href="https://github.com/ultralytics/ultralytics">Ultralytics Model Zoo</a></li>
<li><a href="https://tfhub.dev/">TensorFlow Hub</a></li>
</ul>
<h3 id="è³‡æ–™é›†"><a class="header" href="#è³‡æ–™é›†">è³‡æ–™é›†</a></h3>
<ul>
<li><a href="https://cocodataset.org/">COCO Dataset</a></li>
<li><a href="https://www.image-net.org/">ImageNet</a></li>
<li><a href="https://commonvoice.mozilla.org/">Common Voice</a></li>
</ul>
<hr />
<h2 id="-ä¸‹ä¸€æ­¥å»ºè­°"><a class="header" href="#-ä¸‹ä¸€æ­¥å»ºè­°">ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­°</a></h2>
<ol>
<li>
<p><strong>å…¥é–€ç·´ç¿’ï¼š</strong></p>
<ul>
<li>å¾ Whisper èªéŸ³è½‰æ–‡å­—é–‹å§‹</li>
<li>å˜—è©¦ YOLOv8 ç‰©ä»¶åµæ¸¬</li>
<li>çµåˆå…©è€…åšç°¡å–®æ‡‰ç”¨</li>
</ul>
</li>
<li>
<p><strong>é€²éšå°ˆæ¡ˆï¼š</strong></p>
<ul>
<li>å»ºç«‹å³æ™‚ç¿»è­¯ç³»çµ±</li>
<li>é–‹ç™¼æ™ºæ…§ç›£æ§æ‡‰ç”¨</li>
<li>è£½ä½œç„¡éšœç¤™è¼”åŠ©å·¥å…·</li>
</ul>
</li>
<li>
<p><strong>æ•ˆèƒ½å„ªåŒ–ï¼š</strong></p>
<ul>
<li>å­¸ç¿’æ¨¡å‹é‡åŒ–æŠ€è¡“</li>
<li>ä½¿ç”¨ GPU åŠ é€Ÿ</li>
<li>éƒ¨ç½²åˆ°é‚Šç·£è£ç½®</li>
</ul>
</li>
<li>
<p><strong>æŒçºŒå­¸ç¿’ï¼š</strong></p>
<ul>
<li>é—œæ³¨æœ€æ–°è«–æ–‡å’ŒæŠ€è¡“</li>
<li>åƒèˆ‡é–‹æºå°ˆæ¡ˆ</li>
<li>åŠ å…¥ AI ç¤¾ç¾¤è¨è«–</li>
</ul>
</li>
</ol>
<hr />
<h2 id="-æˆæ¬Šèˆ‡æ³¨æ„äº‹é …"><a class="header" href="#-æˆæ¬Šèˆ‡æ³¨æ„äº‹é …">ğŸ“ æˆæ¬Šèˆ‡æ³¨æ„äº‹é …</a></h2>
<ul>
<li>ä½¿ç”¨é–‹æºæ¨¡å‹æ™‚æ³¨æ„æˆæ¬Šæ¢æ¬¾</li>
<li>è™•ç†å€‹äººè³‡æ–™æ™‚éµå®ˆéš±ç§æ³•è¦</li>
<li>å•†æ¥­ä½¿ç”¨å‰ç¢ºèªæ¨¡å‹æˆæ¬Š</li>
<li>æ³¨æ„ API ä½¿ç”¨é™åˆ¶å’Œè²»ç”¨</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ml/pytorch_setup.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../ml/data/Face-Recognition/index.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ml/pytorch_setup.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../ml/data/Face-Recognition/index.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
