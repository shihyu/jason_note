<!DOCTYPE HTML>
<html lang="zh" class="rust sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>從0手搓 Transformer 代碼拆解 - Jason&#x27;s Notes</title>


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="../highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="../tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "../";
            const default_light_theme = "rust";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "../searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="../toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="../toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Jason&#x27;s Notes</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/shihyu/jason_note" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="從0手搓transformer代碼拆解篇"><a class="header" href="#從0手搓transformer代碼拆解篇">從0手搓Transformer（代碼拆解篇）</a></h1>
<h2 id="transformer-模型代碼拆解"><a class="header" href="#transformer-模型代碼拆解"><strong>Transformer 模型代碼拆解</strong></a></h2>
<p>Transformer 模型代碼拆解</p>
<ol>
<li>Positional Encoding（位置編碼）</li>
<li>Multi‑Head Attention（多頭注意力）</li>
<li><a href="">Feed Forward Network</a>（前饋網絡）</li>
<li>Transformer Encoder Layer（Transformer 編碼器層）</li>
<li>Transformer Encoder（Transformer 編碼器）</li>
<li>Transformer <a href="">Decoder Layer</a>（Transformer 解碼器層）</li>
<li>Transformer Decoder（Transformer 解碼器）</li>
<li>Transformer Model（Transformer 模型）</li>
<li>mask function（掩碼函數）</li>
<li>Example usage（示例用法）</li>
</ol>
<hr />
<h2 id="transformer-from-scratch"><a class="header" href="#transformer-from-scratch"><strong>Transformer-from-Scratch</strong></a></h2>
<blockquote>
<p>論文<a href="">《Attention Is All You Need》</a>的純 PyTorch 復現<br />
A clean PyTorch re‑implementation of the Transformer architecture described in <em>Attention Is All You Need</em></p>
</blockquote>
<h2 id="簡介"><a class="header" href="#簡介"><strong>簡介</strong></a></h2>
<p>本項目定位為 <strong>AI論文復現 / 從零實現 Transformer</strong>。 代碼遵循原論文的模塊劃分，包含位置編碼、多頭注意力、前饋網絡、編碼器‑解碼器等全部組件，並附帶詳細的中文拆解文檔與英文註釋，方便學習與二次開發。</p>
<h2 id="特性"><a class="header" href="#特性"><strong>特性</strong></a></h2>
<ul>
<li><strong>純 PyTorch</strong>：無第三方高階框架依賴，便於閱讀與修改</li>
<li><strong>模塊化</strong>：各子模塊拆分清晰，可單獨測試</li>
<li><strong>批量優先 (batch‑first)</strong>：符合 PyTorch 常用數據佈局</li>
<li><strong>可復現</strong>：默認超參數即能在 CPU / 單卡 GPU 上跑通示例</li>
<li><strong>完整註釋</strong>：中英雙語文檔 + 代碼行級英文註釋</li>
</ul>
<h2 id="文件結構"><a class="header" href="#文件結構"><strong>文件結構</strong></a></h2>
<pre><code class="language-text"> .
 ├── transformer/                # 核心代碼
 │   ├── PostionalEncoding.py    # 正弦位置編碼
 │   ├── MHA.py                  # 多頭注意力
 │   ├── FFN.py                  # 前饋網絡
 │   ├── Encoder.py              # 編碼器
 │   ├── Decoder.py              # 解碼器
 │   ├── create_mask.py          # 掩碼生成函數
 │   ├── model.py                # Transformer模型
 │   ├── test.py                 # 測試腳本
 │   └── Transformer.ipynb       # 完整實現
 ├── docs/
 │   ├── transformer_arxiv.pdf   # 原論文
 │   ├── Deep-Analysis.md        # 深度解析
 │   └── code‑dasm.md            # 代碼拆解文檔
 ├── examples/
 │   └── exam.py                 # （歡迎來補充）
 ├── LICENCE.md
 └── README.md
</code></pre>
<h2 id="環境依賴"><a class="header" href="#環境依賴"><strong>環境依賴</strong></a></h2>
<ul>
<li>Python ≥ 3.9</li>
<li>PyTorch ≥ 2.0</li>
<li>tqdm（可選，用於進度條顯示）</li>
</ul>
<pre><code class="language-bash"> pip install torch tqdm
</code></pre>
<h2 id="快速開始"><a class="header" href="#快速開始"><strong>快速開始</strong></a></h2>
<pre><code class="language-python"> import torch
 from transformer.model import Transformer
 from transformer.mask import create_padding_mask
 ​
 # 假設詞表大小各 10 k，序列長度 src=10 / tgt=12
 model = Transformer(src_vocab_size=10000,
                     tgt_vocab_size=10000,
                     d_model=512,
                     n_heads=8,
                     d_ff=2048,
                     num_layers=6)
 ​
 src = torch.randint(0, 10000, (32, 10))   # (batch_size, src_len)
 tgt = torch.randint(0, 10000, (32, 12))   # (batch_size, tgt_len)
 ​
 src_mask, tgt_mask = create_padding_mask(src, tgt)
 ​
 logits = model(src, tgt, src_mask, tgt_mask)  # (32, 12, 10000)
 print(logits.shape)      # should be torch.Size([32, 12, 10000])
</code></pre>
<h2 id="訓練示例待補充"><a class="header" href="#訓練示例待補充"><strong>訓練示例(待補充)</strong></a></h2>
<p>在 <code>examples/exam.py</code> 中提供了一個最小訓練腳本，演示如何使用交叉熵損失與 Adam 優化器對xx數據集進行訓練。</p>
<hr />
<h2 id="1-positional-encoding位置編碼"><a class="header" href="#1-positional-encoding位置編碼"><strong>1. Positional Encoding（位置編碼）</strong></a></h2>
<p><img src="images/v2-9d68ce1e30ccd80b4d492671fa87d19a_1440w.jpg" alt="" /></p>
<p>位置編碼</p>
<p>Transformer 模型使用 <strong>位置編碼</strong> 為序列中的每個位置添加位置信息。由於 Transformer 完全依賴注意力機制，缺乏對序列順序的內在建模能力，需要在輸入的詞嵌入中加入位置編碼以讓模型識別不同的位置。這裡實現的 <code>PositionalEncoding</code> 類採用正弦和餘弦函數生成固定的位置編碼，與原始論文中的方法一致。</p>
<p>在構造函數中，首先創建一個 <code>position</code> 張量（包含從 0 到 <code>max_len-1</code> 的位置索引），並計算縮放因子 <code>div_term</code>（相當於 10000^(-2i/d_model)）。然後初始化一個 <code>pe</code> 張量用於存儲位置編碼，其中偶數維度使用 <code>torch.sin(position * div_term)</code>，奇數維度使用 <code>torch.cos(position * div_term)</code> 填充。計算得到的 <code>pe</code> 通過 <code>register_buffer</code> 註冊為模型緩衝區（非可訓練參數），這樣在模型訓練過程中不會被更新。<strong>forward</strong> 方法中，將輸入張量 <code>x</code> 與相應長度的 <code>pe</code> 片段相加，將位置信息融入輸入表示後返回結果。</p>
<pre><code class="language-python"> class PositionalEncoding(nn.Module):
     def __init__(self, d_model, max_len=5000):
         super().__init__()
         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)
         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
 ​
         pe = torch.zeros(1, max_len, d_model)   # (1, max_len, d_model)
         pe[0, :, 0::2] = torch.sin(position * div_term)    # even index
         pe[0, :, 1::2] = torch.cos(position * div_term)    # odd index
         self.register_buffer('pe', pe)
 ​
     def forward(self, x):
         # x: (batch_size, seq_len, d_model)
         x = x + self.pe[:, :x.size(1), :]     # add positional encoding to input tensor
         return x
</code></pre>
<h2 id="multi-head-attention"><a class="header" href="#multi-head-attention"><a href="">Multi-Head Attention</a></a></h2>
<p><img src="images/v2-b817a1750e87d5f3a791201bcb7c59ae_1440w.jpg" alt="" /></p>
<p>多頭注意力</p>
<p><img src="images/v2-eac03c4f0e2719e378143ebfff5ffe67_1440w.jpg" alt="" /></p>
<p>縮放點乘注意力</p>
<p><strong>多頭注意力機制</strong>允許模型在不同的子空間中對序列的不同位置進行關注，從而綜合不同的位置關係信息。<code>MultiHeadAttention</code> 類實現了多頭自注意力的計算，包括將輸入投影成多個頭、計算縮放點積注意力，以及頭輸出的拼接和線性變換。</p>
<p>結構上，該類初始化時根據 <code>d_model</code> 和 <code>n_heads</code> 計算每個注意力頭的維度 <code>d_k = d_model // n_heads</code>，並確保可以整除。然後定義了四個線性層：<code>W_q</code>, <code>W_k</code>, <code>W_v</code> 將輸入特徵映射為查詢（Q）、鍵（K）、值（V），<code>W_o</code> 用於最後將多頭輸出映射回 <code>d_model</code> 維度。此外還包含一個 Dropout 層用於在注意力權重上使用。該類提供了一個方法 <code>scaled_dot_product_attention</code> 實現<strong>縮放點積注意力</strong>計算，其步驟如下：</p>
<ol>
<li><strong>計算注意力分數</strong>：對每個注意力頭，計算 ​，得到形狀 <code>(batch_size, n_heads, seq_len, seq_len)</code> 的分數矩陣；</li>
<li><strong>應用遮罩</strong>：如果提供了 <code>mask</code>（形狀與分數矩陣兼容，元素為 0 或 1），則將 <code>mask == 0</code> 的位置（需要屏蔽的位置）對應的分數賦值為一個極小值（-1e9），從而在 softmax 後這些位置幾乎不產生權重；</li>
<li><strong>計算注意力權重</strong>：對上述分數矩陣在最後一個維度進行 <code>softmax</code>，得到注意力權重，然後對權重應用 Dropout；</li>
<li><strong>加權求和值</strong>：使用注意力權重矩陣與值向量 <code>V</code> 相乘，得到每個頭的輸出，形狀為 <code>(batch_size, n_heads, seq_len, d_k)</code>；</li>
<li><strong>多頭輸出整合</strong>：將所有注意力頭的輸出在最後一個維度拼接（通過 <code>transpose</code> 和 <code>view</code> 恢復形狀），並通過線性層 <code>W_o</code> 將維度變換回 <code>d_model</code>，輸出最終的注意力結果。</li>
</ol>
<p>上述過程在 <code>forward</code> 方法中具體實現：首先使用 <code>W_q</code>, <code>W_k</code>, <code>W_v</code> 將輸入的 Q, K, V 張量投影為 <code>d_model</code> 維度並 reshape 成 <code>(batch_size, n_heads, seq_len, d_k)</code> 的格式，然後調用 <code>scaled_dot_product_attention</code> 來獲得多頭注意力輸出。最後，將多頭輸出重新排列回 <code>(batch_size, seq_len, d_model)</code> 並通過 <code>W_o</code> 映射，得到與輸入維度相同的輸出。代碼實現如下：</p>
<pre><code class="language-python"> class MultiHeadAttention(nn.Module):
     def __init__(self, d_model, n_heads, dropout=0.1):
         super().__init__()
         self.d_model = d_model
         self.n_heads = n_heads
         self.d_k = d_model // n_heads
 ​
         assert (
             self.d_k * n_heads == d_model
         ), f"d_model {d_model} not divisible by n_heads {n_heads}"
 ​
         self.W_q = nn.Linear(d_model, d_model, bias=False)
         self.W_k = nn.Linear(d_model, d_model, bias=False)
         self.W_v = nn.Linear(d_model, d_model, bias=False)
         self.W_o = nn.Linear(d_model, d_model)
 ​
         self.dropout = nn.Dropout(dropout)
 ​
     def scaled_dot_product_attention(self, Q, K, V, mask=None):
         # Q: (batch_size, n_heads, seq_len, d_k)
         # K: (batch_size, n_heads, seq_len, d_k)
         # V: (batch_size, n_heads, seq_len, d_k)
 ​
         scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch_size, n_heads, seq_len, seq_len)
         if mask is not None:
             scores = scores.masked_fill(mask == 0, -1e9)    # apply mask to scores
         
         attn_weights = F.softmax(scores, dim=-1)    # (batch_size, n_heads, seq_len, seq_len)
         attn_weights = self.dropout(attn_weights)    # apply dropout to attention weights
         output = torch.matmul(attn_weights, V)    # (batch_size, n_heads, seq_len, d_k)
         return output
     
     def forward(self, Q, K, V, mask=None):
         # Q: (batch_size, seq_len, d_model)
         # K: (batch_size, seq_len, d_model)
         # V: (batch_size, seq_len, d_model)
 ​
         batch_size = Q.size(0)
 ​
         # (batch_size, seq_len, d_model) -&gt; (batch_size, n_heads, seq_len, d_k)
         Q = self.W_q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)    # (batch_size, n_heads, seq_len, d_k)
         K = self.W_k(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)    # (batch_size, n_heads, seq_len, d_k)
         V = self.W_v(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)    # (batch_size, n_heads, seq_len, d_k)
 ​
         # scaled dot-product attention
         attn_output = self.scaled_dot_product_attention(Q, K, V, mask)    # (batch_size, n_heads, seq_len, d_k)
 ​
         # (batch_size, n_heads, seq_len, d_k) -&gt; (batch_size, seq_len, d_model)
         attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)    # (batch_size, seq_len, d_model)
         output = self.W_o(attn_output)    # (batch_size, seq_len, d_model)
         return output    # (batch_size, seq_len, d_model)
</code></pre>
<h2 id="3-feed-forward-network前饋網絡"><a class="header" href="#3-feed-forward-network前饋網絡"><strong>3. Feed Forward Network（前饋網絡）</strong></a></h2>
<p><img src="images/v2-8f51897da02a09b978b338e9b593878b_1440w.jpg" alt="" /></p>
<p>前饋網絡層</p>
<p><strong>前饋網絡</strong>（Feed Forward Network，簡稱 FFN）模塊對每個位置的表示獨立地進行非線性變換，是 Transformer 中每個編碼器/解碼器層的第二個子層。<code>FeedForward</code> 類實現了一個兩層的前饋神經網絡：先擴展維度再投影回原維度。</p>
<p>結構上，它包含兩個線性層 <code>linear1</code> 和 <code>linear2</code>，中間配合 ReLU 激活函數（<code>self.activation = nn.ReLU()</code>）和 Dropout 正則化。構造函數接受參數 <code>d_model</code>（輸入和輸出的特徵維度）和較大的隱藏層維度 <code>d_ff</code>，以及 Dropout 概率。forward 方法中，將輸入 x 先通過 <code>linear1</code> 投影到 <code>d_ff</code> 維度，經過 ReLU 非線性激活和 Dropout 後，再通過 <code>linear2</code> 投影回 <code>d_model</code> 維度。這樣每個位置的向量都經過相同的兩層感知機變換，輸出形狀與輸入相同。代碼如下：</p>
<pre><code class="language-python"> class FeedForward(nn.Module):
     def __init__(self, d_model, d_ff, dropout=0.1):
         super().__init__()
         self.linear1 = nn.Linear(d_model, d_ff)
         self.dropout = nn.Dropout(dropout)
         self.linear2 = nn.Linear(d_ff, d_model)
         self.activation = nn.ReLU()
 ​
     def forward(self, x):
         # x: (batch_size, seq_len, d_model)
         x = self.linear1(x)    # (batch_size, seq_len, d_ff)
         x = self.activation(x)    # (batch_size, seq_len, d_ff)
         x = self.dropout(x)    # (batch_size, seq_len, d_ff)
         x = self.linear2(x)    # (batch_size, seq_len, d_model)
         return x    # (batch_size, seq_len, d_model)
</code></pre>
<h2 id="4-transformer-encoder-layertransformer-編碼器層"><a class="header" href="#4-transformer-encoder-layertransformer-編碼器層"><strong>4. Transformer Encoder Layer（Transformer 編碼器層）</strong></a></h2>
<p><img src="images/v2-0070f515ff993bbe56836e935f1c982c_1440w.jpg" alt="" /></p>
<p>編碼器層（一個Block）</p>
<p><strong>編碼器層</strong>（Encoder Layer）是 Transformer 編碼器的基本單元，包含自注意力和前饋網絡兩個子層，各自帶有殘差連接和層歸一化（LayerNorm）。<code>EncoderLayer</code> 類在初始化時構造了這些子模塊：</p>
<ul>
<li><code>self.self_attn</code>：多頭自注意力子層，用於對輸入序列自身進行注意力計算；</li>
<li><code>self.dropout1</code> 和 <code>self.norm1</code>：對應自注意力子層的 Dropout 和 LayerNorm，用於殘差連接後的正則化和歸一化；</li>
<li><code>self.ffn</code>：前饋網絡子層，將經過注意力的表示進行非線性變換；</li>
<li><code>self.dropout2</code> 和 <code>self.norm2</code>：對應前饋子層的 Dropout 和 LayerNorm。</li>
</ul>
<p>在 forward 方法中，輸入 <code>x</code> 首先通過 <code>self_attn</code> 計算自注意力（<code>Q = K = V = x</code>），可選的 <code>mask</code> 用於在注意力計算中屏蔽無效的位置（如填充位）。得到的注意力輸出與原始輸入 <code>x</code> 相加（殘差連接）後，經過 <code>dropout1</code> 再送入 <code>norm1</code> 進行層歸一化。接著，將歸一化後的結果通過前饋網絡 <code>ffn</code> 得到新的特徵表示，再與中間結果相加後經過 <code>dropout2</code> 和 <code>norm2</code>。最終返回編碼器層的輸出，其形狀與輸入相同。代碼如下：</p>
<pre><code class="language-python"> class EncoderLayer(nn.Module):
     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
         super().__init__()
         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
         self.dropout1 = nn.Dropout(dropout)
         self.norm1 = nn.LayerNorm(d_model)
         
         self.ffn = FeedForward(d_model, d_ff, dropout)
         self.dropout2 = nn.Dropout(dropout)
         self.norm2 = nn.LayerNorm(d_model)
         
     def forward(self, x, mask=None):
         # x: (batch_size, seq_len, d_model)
         attn_output = self.self_attn(x, x, x, mask)    # (batch_size, seq_len, d_model)
         x = self.norm1(x + self.dropout1(attn_output))    # add &amp; norm
         
         ffn_output = self.ffn(x)    
         x = self.norm2(x + self.dropout2(ffn_output))    # add &amp; norm
         return x    # (batch_size, seq_len, d_model)
</code></pre>
<h2 id="5-transformer-encodertransformer-編碼器"><a class="header" href="#5-transformer-encodertransformer-編碼器"><strong>5. Transformer Encoder（Transformer 編碼器）</strong></a></h2>
<p><img src="images/v2-d65d922d367f140bff805a6f676bd1ec_1440w.jpg" alt="" /></p>
<p>編碼器（N個Block堆疊）</p>
<p><strong>編碼器</strong>（Encoder）由若干個編碼器層堆疊而成。<code>Encoder</code> 類的初始化接收編碼層數 <code>num_layers</code>，並使用 <code>nn.ModuleList</code> 將 <code>num_layers</code> 個 <code>EncoderLayer</code> 實例存儲在列表 <code>self.layers</code> 中。同時還定義了一個最終的 LayerNorm (<code>self.norm</code>) 對整個編碼器輸出進行歸一化。</p>
<p>forward 方法對輸入 <code>x</code> 依次通過每一層編碼器層進行處理：循環遍歷 <code>self.layers</code> 列表，將當前輸出 <code>x</code> 傳入每個 <code>EncoderLayer</code>。可選的 <code>mask</code> 在每層的自注意力計算中都會用到。當所有層都處理完畢後，再對最終的 <code>x</code> 進行一次 <code>LayerNorm</code> 歸一化，作為編碼器的輸出返回。編碼器將輸入序列編碼成高層表示，為後續解碼提供上下文特徵。代碼實現如下：</p>
<pre><code class="language-python"> class Encoder(nn.Module):
     def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):
         super().__init__()
         self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])
         self.norm = nn.LayerNorm(d_model)
 ​
     def forward(self, x, mask=None):
         # x: (batch_size, seq_len, d_model)
         for layer in self.layers:
             x = layer(x, mask)   # (batch_size, seq_len, d_model)
         x = self.norm(x)    # (batch_size, seq_len, d_model)
         return x    # (batch_size, seq_len, d_model)
</code></pre>
<h2 id="6-transformer-decoder-layertransformer解碼器層"><a class="header" href="#6-transformer-decoder-layertransformer解碼器層"><strong>6. Transformer Decoder Layer（Transformer解碼器層）</strong></a></h2>
<p><img src="images/v2-83cb748c002fc227f0af752a0f34dd18_1440w.jpg" alt="" /></p>
<p>解碼器層（一個Block）</p>
<p><strong>解碼器層</strong>（Decoder Layer）是 Transformer 解碼器的基本單元，包括三部分子層：自注意力、交叉注意力和前饋網絡，各自配備殘差連接和 LayerNorm。<code>DecoderLayer</code> 的初始化構造了這些組件：</p>
<ul>
<li><code>self.self_attn</code>：多頭自注意力，用於解碼器當前輸入（目標序列已生成部分）內部的注意力計算；</li>
<li><code>self.cross_attn</code>：多頭交叉注意力，用於將解碼器的中間表示作為查詢，與編碼器輸出（memory）作為鍵和值進行注意力計算，從編碼器提取相關信息；</li>
<li>對每個注意力子層和前饋子層，分別有對應的 Dropout 和 LayerNorm：<code>dropout1/norm1</code>（自注意力）、<code>dropout2/norm2</code>（交叉注意力）、<code>dropout3/norm3</code>（前饋網絡）。</li>
</ul>
<p>在 forward 方法中，<code>tgt</code> 表示解碼器當前時刻的輸入（目標序列的上下文），<code>src</code> 表示編碼器輸出（即 memory）。首先，對 <code>tgt</code> 執行自注意力 <code>self_attn</code>（<code>Q = K = V = x = tgt</code>），使用 <code>tgt_mask</code> 來屏蔽無效位置和未來信息，然後將輸出與 <code>x</code> 殘差相加並經 <code>norm1</code> 標準化。接下來，執行交叉注意力 <code>cross_attn</code>，其中查詢 Q 是當前解碼器的狀態 <code>x</code>，鍵和值 K=V 使用編碼器輸出 <code>src</code>（即 memory），應用 <code>src_mask</code> 來屏蔽掉源序列中無效的填充位置。將交叉注意力輸出與 <code>x</code> 殘差相加，經過 <code>norm2</code>。最後，通過前饋網絡 <code>ffn</code> 變換 <code>x</code>，再與 <code>x</code> 殘差相加，經 <code>norm3</code> 得到解碼器層的輸出。代碼如下：</p>
<pre><code class="language-python"> class DecoderLayer(nn.Module):
     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
         super().__init__()
         self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
         self.dropout1 = nn.Dropout(dropout)
         self.norm1 = nn.LayerNorm(d_model)
 ​
         self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
         self.dropout2 = nn.Dropout(dropout)
         self.norm2 = nn.LayerNorm(d_model)
 ​
         self.ffn = FeedForward(d_model, d_ff, dropout)
         self.dropout3 = nn.Dropout(dropout)
         self.norm3 = nn.LayerNorm(d_model)
 ​
     def forward(self, tgt, src, tgt_mask=None, src_mask=None):
         # tgt: (batch_size, tgt_seq_len, d_model)
         # memory: (batch_size, src_seq_len, d_model)
         # tgt_mask: (batch_size, 1, 1, tgt_seq_len)
         # src_mask: (batch_size, 1, 1, src_seq_len)
 ​
         x = tgt
         output = self.self_attn(x, x, x, tgt_mask)    # (batch_size, tgt_seq_len, d_model)
         x = self.norm1(x + self.dropout1(output))    # add &amp; norm
 ​
         output = self.cross_attn(x, src, src, src_mask)    # (batch_size, seq_len, d_model)
         x = self.norm2(x + self.dropout2(output))    # add &amp; norm
 ​
         output = self.ffn(x)    # (batch_size, seq_len, d_model)
         x = self.norm3(x + self.dropout3(output))    # add &amp; norm
         return x    # (batch_size, seq_len, d_model)
</code></pre>
<h2 id="7-transformer-decodertransformer-解碼器"><a class="header" href="#7-transformer-decodertransformer-解碼器"><strong>7. Transformer Decoder（Transformer 解碼器）</strong></a></h2>
<p><img src="images/v2-8133f702e05be3b01c7f5c7cf829d026_1440w.jpg" alt="" /></p>
<p>解碼器（N個Block堆疊）</p>
<p><strong>解碼器</strong>（Decoder）由若干解碼器層堆疊組成。<code>Decoder</code> 類的構造函數接受層數 <code>num_layers</code>，並使用 <code>nn.ModuleList</code> 包含 <code>num_layers</code> 個 <code>DecoderLayer</code> 實例。與編碼器不同，解碼器類本身並未定義額外的 LayerNorm（部分實現可能在解碼器最後也加歸一化，這裡未使用）。</p>
<p>forward 方法中，傳入解碼器輸入 <code>x</code>（通常是目標序列的嵌入表示）和編碼器輸出 <code>memory</code>，以及可選的 <code>tgt_mask</code>（目標序列遮罩）和 <code>memory_mask</code>（對編碼器輸出的遮罩）。然後循環地將 <code>x</code> 與 <code>memory</code> 輸入到每一層解碼器層中，更新 <code>x</code>。<code>tgt_mask</code> 和 <code>memory_mask</code> 會在各層的注意力計算中用到，以確保解碼器不能“看到”未來的目標詞，以及不關注編碼器中填充的部分。所有層處理完後，返回解碼器的輸出。代碼如下：</p>
<pre><code class="language-python"> class Decoder(nn.Module):
     def __init__(self, d_model, n_heads, d_ff, num_layers, dropout=0.1):
         super().__init__()
         self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(num_layers)])
 ​
     def forward(self, x, memory, tgt_mask=None, memory_mask=None):
         # x: (batch_size, seq_len, d_model)
         for layer in self.layers:
             x = layer(x, memory, tgt_mask, memory_mask)    # (batch_size, seq_len, d_model)
         return x    # (batch_size, seq_len, d_model)
</code></pre>
<h2 id="8-transformer-modeltransformer-模型"><a class="header" href="#8-transformer-modeltransformer-模型"><strong>8. Transformer Model（Transformer 模型）</strong></a></h2>
<p><img src="images/v2-4a45d4c37ed5eeac87ef4de13642bc37_1440w.jpg" alt="" /></p>
<p>Transformer模型</p>
<p><strong>Transformer 模型</strong> 類將上述組件整合在一起，構建完整的編碼-解碼器結構。構造函數中，<code>Transformer</code> 接受源詞表大小 <code>src_vocab_size</code>、目標詞表大小 <code>tgt_vocab_size</code> 以及模型各項超參數（<code>d_model</code>, <code>n_heads</code>, <code>d_ff</code>, <code>num_layers</code>, <code>dropout</code>）。主要組件包括：</p>
<ul>
<li><code>encoder_embedding</code> 和 <code>decoder_embedding</code>：將源序列和目標序列的詞 ID 映射為 <code>d_model</code> 維的詞向量表示；</li>
<li><code>positional_encoding</code>：位置編碼模塊實例，用於給輸入的詞向量加入位置信息；</li>
<li><code>encoder</code>：編碼器（由若干編碼器層組成）；</li>
<li><code>decoder</code>：解碼器（由若干解碼器層組成）；</li>
<li><code>fc_out</code>：輸出的全連接層，將解碼器的輸出特徵映射為目標詞表大小的向量。</li>
</ul>
<p>在 forward 方法中，模型接收源序列 <code>src</code> 和目標序列 <code>tgt</code> 的詞索引張量，以及對應的 <code>src_mask</code> 和 <code>tgt_mask</code>（由外部的遮罩函數生成）。首先，對 <code>src</code> 和 <code>tgt</code> 分別通過嵌入層並乘以 ​ 進行縮放（這一技巧來自論文，幫助穩定模型表示幅度），然後應用 Dropout。接著，將嵌入後的 <code>src</code> 和 <code>tgt</code> 分別加上位置編碼。處理完嵌入和位置後，將 <code>src</code> 送入編碼器 <code>self.encoder</code>，結合 <code>src_mask</code> 得到編碼器輸出 <code>enc_output</code>；然後將 <code>tgt</code> 和編碼器輸出一起送入解碼器 <code>self.decoder</code>，結合 <code>tgt_mask</code>（以及可選的 <code>memory_mask</code>，此實現中未顯式傳入編碼器的 mask，因此解碼器交叉注意力默認不屏蔽編碼器輸出）得到 <code>dec_output</code>。最後，通過 <code>fc_out</code> 將解碼器輸出轉換為目標詞彙表維度的 logits 並返回。此輸出通常需要配合 softmax 和交叉熵損失用於訓練。代碼如下：</p>
<pre><code class="language-python"> class Transformer(nn.Module):
     def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_heads, d_ff, num_layers, dropout=0.1):
         super().__init__()
         self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)
         self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)
         self.positional_encoding = PositionalEncoding(d_model)
 ​
         self.dropout = nn.Dropout(dropout)
 ​
         self.encoder = Encoder(d_model, n_heads, d_ff, num_layers, dropout)
         self.decoder = Decoder(d_model, n_heads, d_ff, num_layers, dropout)
 ​
         self.fc_out = nn.Linear(d_model, tgt_vocab_size)
 ​
     def forward(self, src, tgt, src_mask=None, tgt_mask=None):
         # src: (batch_size, src_seq_len)
         # tgt: (batch_size, tgt_seq_len)
 ​
         src = self.encoder_embedding(src) * math.sqrt(self.encoder_embedding.embedding_dim)    # (batch_size, src_seq_len, d_model)
         tgt = self.decoder_embedding(tgt) * math.sqrt(self.decoder_embedding.embedding_dim)    # (batch_size, tgt_seq_len, d_model)
 ​
         src = self.dropout(src)    # (batch_size, src_seq_len, d_model)
         tgt = self.dropout(tgt)    # (batch_size, tgt_seq_len, d_model)
 ​
         src = self.positional_encoding(src)    # (batch_size, src_seq_len, d_model)
         tgt = self.positional_encoding(tgt)    # (batch_size, tgt_seq_len, d_model)
 ​
         enc_output = self.encoder(src, src_mask)    # (batch_size, src_seq_len, d_model)
         dec_output = self.decoder(tgt, enc_output, tgt_mask)    # (batch_size, tgt_seq_len, d_model)
 ​
         output = self.fc_out(dec_output)    # (batch_size, tgt_seq_len, tgt_vocab_size)
         return output    # (batch_size, tgt_seq_len, tgt_vocab_size)
</code></pre>
<h2 id="9-mask-function掩碼函數"><a class="header" href="#9-mask-function掩碼函數"><strong>9. mask function（掩碼函數）</strong></a></h2>
<p>在訓練或推理時，需要生成遮罩（mask）來屏蔽序列中的填充部分，以及在解碼器中屏蔽未來的詞。<code>create_padding_mask</code> 函數同時生成用於源序列和目標序列的遮罩張量：</p>
<ul>
<li><strong>源序列填充遮罩（src_mask）</strong>：對輸入 <code>src</code> 張量生成形狀為 <code>(batch_size, 1, 1, src_seq_len)</code> 的布爾張量，位置上為 True 表示對應的 <code>src</code> 單詞不為填充符（<code>pad_idx</code>，默認為 0），為 False 表示填充符位置。注意在後續注意力計算中會將 False（即 0）的位置賦予 -∞ 分數，從而忽略填充。</li>
<li><strong>目標序列填充遮罩（tgt_mask）</strong>：對 <code>tgt</code> 生成形狀為 <code>(batch_size, 1, tgt_seq_len, 1)</code> 的遮罩張量，用法類似 src_mask。</li>
<li><strong>未來信息遮罩（look-ahead mask）</strong>：生成一個下三角矩陣（大小為 <code>tgt_len × tgt_len</code>）的布爾張量，True 表示允許看到自身和之前的位置，False 表示屏蔽未來的位置。通過 <code>tril()</code> 得到下三角，在前面加上兩個維度將其擴展成形狀 <code>(1, 1, tgt_len, tgt_len)</code>。</li>
<li><strong>合併遮罩</strong>：將目標序列的填充遮罩 <code>tgt_mask</code> 與 look-ahead mask 按位與 (<code>&amp;</code>) 合併，得到最終的目標遮罩，形狀為 <code>(batch_size, 1, tgt_len, tgt_len)</code>。</li>
</ul>
<p>該函數返回 <code>src_mask</code> 和 <code>tgt_mask</code> 兩個遮罩張量，供 Transformer 編碼器和解碼器在注意力計算時使用。代碼如下：</p>
<pre><code class="language-python"> def create_padding_mask(src, tgt, pad_idx=0):
     # src: (batch_size, src_seq_len)
     # tgt: (batch_size, tgt_seq_len)
 ​
     src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)    # (batch_size, 1, 1, src_seq_len)
     tgt_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(3)    # (batch_size, 1, tgt_seq_len, 1)
 ​
     # look-ahead mask
     tgt_len = tgt.size(1)
     look_ahead_mask = torch.ones(tgt_len, tgt_len).tril().bool().unsqueeze(0).unsqueeze(0)    # (1, 1, tgt_len, tgt_len)
     tgt_mask = tgt_mask &amp; look_ahead_mask.to(tgt.device)    # (batch_size, 1, tgt_len, tgt_len)
 ​
     return src_mask, tgt_mask    # (batch_size, 1, 1, src_seq_len), (batch_size, 1, tgt_seq_len, tgt_seq_len)
</code></pre>
<h2 id="10-example-usage示例用法"><a class="header" href="#10-example-usage示例用法"><strong>10. Example usage（示例用法）</strong></a></h2>
<p>下面的示例代碼展示瞭如何使用上述 <code>Transformer</code> 模型類。它首先定義模型的超參數（詞彙表大小、<code>d_model</code> 等）並實例化一個 <code>Transformer</code> 模型。然後，生成隨機的源序列 <code>src</code> 和目標序列 <code>tgt</code> 張量（形狀分別為 <code>(32, 10)</code> 和 <code>(32, 12)</code>，假設批大小為 32，源序列長度 10，目標序列長度 12），其中每個元素都是在詞彙表範圍內隨機採樣的整數索引。接著，通過 <code>create_padding_mask(src, tgt)</code> 函數得到對應的 <code>src_mask</code> 和 <code>tgt_mask</code>。最後，將這些張量輸入模型的 forward 方法，得到輸出張量 <code>output</code>，並打印輸出的形狀以驗證正確性（應為 <code>(32, 12, 10000)</code>，對應 <em>批大小 × 目標序列長度 × 目標詞表大小</em>）。</p>
<pre><code class="language-python"> if __name__ == "__main__":
     src_vocab_size = 10000
     tgt_vocab_size = 10000
     d_model = 512
     n_heads = 8
     d_ff = 2048
     num_layers = 6
     dropout = 0.1
 ​
     model = Transformer(src_vocab_size, tgt_vocab_size, d_model, n_heads, d_ff, num_layers, dropout)
 ​
     src = torch.randint(0, src_vocab_size, (32, 10))    # (batch_size, src_seq_len)
     tgt = torch.randint(0, tgt_vocab_size, (32, 12))    # (batch_size, tgt_seq_len)
 ​
     src_mask, tgt_mask = create_padding_mask(src, tgt)    # (batch_size, 1, 1, src_seq_len), (batch_size, 1, tgt_seq_len)
 ​
     output = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)    # (batch_size, tgt_seq_len, tgt_vocab_size)
     print(output.shape)    # should be (32, 12, 10000)
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../ml/pytorch_setup.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../ml/ai-recognition-guide.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../ml/pytorch_setup.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../ml/ai-recognition-guide.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="../ace.js"></script>
        <script src="../mode-rust.js"></script>
        <script src="../editor.js"></script>
        <script src="../theme-dawn.js"></script>
        <script src="../theme-tomorrow_night.js"></script>

        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->
        <script src="../mermaid.min.js"></script>
        <script src="../mermaid-init.js"></script>



    </div>
    </body>
</html>
